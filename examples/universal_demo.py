#!/usr/bin/env python3
"""
Universal Layerwise-Adapter æ¡†æ¶æ¼”ç¤ºç¤ºä¾‹
Demo: ä»Amazonæ¨èç³»ç»Ÿæ‰©å±•åˆ°è®¡ç®—æœºè§†è§‰åˆ†ç±»ä»»åŠ¡

å±•ç¤ºå¦‚ä½•ä½¿ç”¨åŒä¸€ä¸ªæ¡†æ¶åˆ†æä¸åŒæ¨¡æ€çš„æ¨¡å‹ï¼š
1. æ–‡æœ¬åˆ†ç±»æ¨¡å‹ (BERT-like)
2. å›¾åƒåˆ†ç±»æ¨¡å‹ (ResNet-like)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import logging
from typing import Tuple

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¯¼å…¥æˆ‘ä»¬çš„é€šç”¨æ¡†æ¶
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from universal.layerwise_adapter import (
    create_analyzer, AnalysisConfig, UniversalLayerwiseAdapter,
    TaskType, ModalityType, TextModelAdapter, VisionModelAdapter
)

class SimpleTextClassifier(nn.Module):
    """ç®€å•çš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹ (BERT-likeç»“æ„)"""
    
    def __init__(self, vocab_size=10000, embed_dim=128, hidden_dim=256, num_classes=2, num_layers=6):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # å¤šå±‚Transformer-likeç»“æ„
        self.layers = nn.ModuleList([
            nn.TransformerEncoderLayer(embed_dim, nhead=8, dim_feedforward=hidden_dim, batch_first=True)
            for _ in range(num_layers)
        ])
        
        self.pooler = nn.AdaptiveAvgPool1d(1)
        self.classifier = nn.Linear(embed_dim, num_classes)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        # x shape: (batch_size, seq_len)
        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)
        
        # é€šè¿‡å„å±‚
        for layer in self.layers:
            x = layer(x)
            
        # æ± åŒ–å’Œåˆ†ç±»
        x = x.transpose(1, 2)  # (batch_size, embed_dim, seq_len)
        x = self.pooler(x).squeeze(-1)  # (batch_size, embed_dim)
        x = self.dropout(x)
        x = self.classifier(x)  # (batch_size, num_classes)
        
        return x

class SimpleVisionClassifier(nn.Module):
    """ç®€å•çš„å›¾åƒåˆ†ç±»æ¨¡å‹ (ResNet-likeç»“æ„)"""
    
    def __init__(self, input_channels=3, num_classes=10):
        super().__init__()
        
        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # æ®‹å·®å—
        self.layer1 = self._make_layer(64, 64, 2)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.layer4 = self._make_layer(256, 512, 2, stride=2)
        
        # åˆ†ç±»å¤´
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
        
    def _make_layer(self, inplanes, planes, blocks, stride=1):
        layers = []
        layers.append(nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1))
        layers.append(nn.BatchNorm2d(planes))
        layers.append(nn.ReLU(inplace=True))
        
        for _ in range(1, blocks):
            layers.append(nn.Conv2d(planes, planes, kernel_size=3, padding=1))
            layers.append(nn.BatchNorm2d(planes))
            layers.append(nn.ReLU(inplace=True))
            
        return nn.Sequential(*layers)
        
    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        
        return x

def create_sample_text_data(batch_size=32, seq_len=128, vocab_size=10000, num_batches=10) -> DataLoader:
    """åˆ›å»ºç¤ºä¾‹æ–‡æœ¬æ•°æ®"""
    data = []
    labels = []
    
    for _ in range(num_batches):
        batch_data = torch.randint(0, vocab_size, (batch_size, seq_len))
        batch_labels = torch.randint(0, 2, (batch_size,))
        data.append(batch_data)
        labels.append(batch_labels)
    
    data = torch.cat(data, dim=0)
    labels = torch.cat(labels, dim=0)
    
    dataset = TensorDataset(data, labels)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

def create_sample_vision_data(batch_size=32, channels=3, height=224, width=224, num_classes=10, num_batches=10) -> DataLoader:
    """åˆ›å»ºç¤ºä¾‹å›¾åƒæ•°æ®"""
    data = []
    labels = []
    
    for _ in range(num_batches):
        batch_data = torch.randn(batch_size, channels, height, width)
        batch_labels = torch.randint(0, num_classes, (batch_size,))
        data.append(batch_data)
        labels.append(batch_labels)
    
    data = torch.cat(data, dim=0)
    labels = torch.cat(labels, dim=0)
    
    dataset = TensorDataset(data, labels)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

def demo_text_classification():
    """æ¼”ç¤ºæ–‡æœ¬åˆ†ç±»æ¨¡å‹åˆ†æ"""
    print("\n" + "="*60)
    print("ğŸ”¤ æ–‡æœ¬åˆ†ç±»æ¨¡å‹åˆ†ææ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleTextClassifier(vocab_size=10000, num_layers=6)
    print(f"ğŸ“Š æ¨¡å‹ç»“æ„: {sum(p.numel() for p in model.parameters())} å‚æ•°")
    
    # åˆ›å»ºæ•°æ®
    data_loader = create_sample_text_data()
    print(f"ğŸ“ æ•°æ®: {len(data_loader.dataset)} æ ·æœ¬")
    
    # åˆ›å»ºåˆ†æå™¨
    adapter = create_analyzer(
        model_name="simple-text-classifier",
        task_type="classification",
        modality_type="text",
        analysis_methods=['fisher_information', 'gradient_based']
    )
    
    # åŠ è½½æ¨¡å‹ (éœ€è¦è‡ªå®šä¹‰é€‚é…å™¨)
    class CustomTextAdapter(TextModelAdapter):
        def _initialize_layers(self):
            layer_idx = 0
            # åµŒå…¥å±‚
            self.layers.append(Layer(self.model.embedding, layer_idx, "embedding"))
            layer_idx += 1
            
            # Transformerå±‚
            for i, transformer_layer in enumerate(self.model.layers):
                self.layers.append(Layer(transformer_layer, layer_idx, f"transformer_{i}"))
                layer_idx += 1
                
            # åˆ†ç±»å™¨
            self.layers.append(Layer(self.model.classifier, layer_idx, "classifier"))
    
    # æ›¿æ¢é»˜è®¤é€‚é…å™¨
    from universal.layerwise_adapter import Layer
    adapter.model = CustomTextAdapter(model, adapter.config)
    
    print(f"ğŸ”§ æ£€æµ‹åˆ° {len(adapter.model.layers)} å±‚")
    for layer in adapter.model.layers[:3]:  # æ˜¾ç¤ºå‰3å±‚
        print(f"   - {layer}")
    
    # æ‰§è¡Œåˆ†æ
    try:
        results = adapter.analyze_importance(data_loader)
        print(f"âœ… åˆ†æå®Œæˆï¼Œä½¿ç”¨äº† {len(results)} ç§æ–¹æ³•")
        
        # æ˜¾ç¤ºç»“æœ
        for method, scores in results.items():
            print(f"\nğŸ“ˆ {method} ç»“æœ:")
            sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
            for layer_idx, score in sorted_scores[:5]:  # æ˜¾ç¤ºtop5
                layer_name = adapter.model.layers[layer_idx].layer_name
                print(f"   Layer {layer_idx} ({layer_name}): {score:.4f}")
        
        # ç”Ÿæˆå‹ç¼©æ–¹æ¡ˆ
        compression_plan = adapter.generate_compression_plan(target_ratio=2.0)
        print(f"\nğŸ—œï¸  å‹ç¼©æ–¹æ¡ˆ (2.0x):")
        print(f"   åŸå§‹å±‚æ•°: {compression_plan['original_layers']}")
        print(f"   å‹ç¼©åå±‚æ•°: {compression_plan['compressed_layers']}")
        print(f"   ä¿ç•™å±‚: {compression_plan['keep_layer_indices']}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")

def demo_vision_classification():
    """æ¼”ç¤ºè§†è§‰åˆ†ç±»æ¨¡å‹åˆ†æ"""
    print("\n" + "="*60)
    print("ğŸ–¼ï¸  è§†è§‰åˆ†ç±»æ¨¡å‹åˆ†ææ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleVisionClassifier(input_channels=3, num_classes=10)
    print(f"ğŸ“Š æ¨¡å‹ç»“æ„: {sum(p.numel() for p in model.parameters())} å‚æ•°")
    
    # åˆ›å»ºæ•°æ®
    data_loader = create_sample_vision_data()
    print(f"ğŸ“ æ•°æ®: {len(data_loader.dataset)} æ ·æœ¬")
    
    # åˆ›å»ºåˆ†æå™¨
    adapter = create_analyzer(
        model_name="simple-vision-classifier",
        task_type="classification",
        modality_type="vision",
        analysis_methods=['fisher_information', 'gradient_based']
    )
    
    # åŠ è½½æ¨¡å‹ (éœ€è¦è‡ªå®šä¹‰é€‚é…å™¨)  
    class CustomVisionAdapter(VisionModelAdapter):
        def _initialize_layers(self):
            layer_idx = 0
            
            # åˆå§‹å·ç§¯å±‚
            self.layers.append(Layer(self.model.conv1, layer_idx, "conv1"))
            layer_idx += 1
            
            # æ®‹å·®å±‚
            for i, res_layer in enumerate([self.model.layer1, self.model.layer2, 
                                         self.model.layer3, self.model.layer4]):
                self.layers.append(Layer(res_layer, layer_idx, f"layer{i+1}"))
                layer_idx += 1
                
            # åˆ†ç±»å™¨
            self.layers.append(Layer(self.model.fc, layer_idx, "classifier"))
    
    # æ›¿æ¢é»˜è®¤é€‚é…å™¨
    from universal.layerwise_adapter import Layer
    adapter.model = CustomVisionAdapter(model, adapter.config)
    
    print(f"ğŸ”§ æ£€æµ‹åˆ° {len(adapter.model.layers)} å±‚")
    for layer in adapter.model.layers:
        print(f"   - {layer}")
    
    # æ‰§è¡Œåˆ†æ
    try:
        results = adapter.analyze_importance(data_loader)
        print(f"âœ… åˆ†æå®Œæˆï¼Œä½¿ç”¨äº† {len(results)} ç§æ–¹æ³•")
        
        # æ˜¾ç¤ºç»“æœ
        for method, scores in results.items():
            print(f"\nğŸ“ˆ {method} ç»“æœ:")
            sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
            for layer_idx, score in sorted_scores:
                layer_name = adapter.model.layers[layer_idx].layer_name
                print(f"   Layer {layer_idx} ({layer_name}): {score:.4f}")
        
        # ç”Ÿæˆå‹ç¼©æ–¹æ¡ˆ
        compression_plan = adapter.generate_compression_plan(target_ratio=1.5)
        print(f"\nğŸ—œï¸  å‹ç¼©æ–¹æ¡ˆ (1.5x):")
        print(f"   åŸå§‹å±‚æ•°: {compression_plan['original_layers']}")
        print(f"   å‹ç¼©åå±‚æ•°: {compression_plan['compressed_layers']}")
        print(f"   ä¿ç•™å±‚: {compression_plan['keep_layer_indices']}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")

def compare_modalities():
    """å¯¹æ¯”ä¸åŒæ¨¡æ€çš„åˆ†æç»“æœ"""
    print("\n" + "="*60)
    print("âš–ï¸  è·¨æ¨¡æ€åˆ†æç»“æœå¯¹æ¯”")
    print("="*60)
    
    print("ğŸ“‹ å¯¹æ¯”æ€»ç»“:")
    print("   ğŸ”¤ æ–‡æœ¬æ¨¡å‹: æ³¨æ„åŠ›å±‚å’Œåˆ†ç±»å™¨é€šå¸¸æ›´é‡è¦")
    print("   ğŸ–¼ï¸  è§†è§‰æ¨¡å‹: é«˜å±‚ç‰¹å¾æå–å™¨å’Œåˆ†ç±»å™¨æ›´é‡è¦") 
    print("   ğŸ”§ é€šç”¨æ¡†æ¶: ç›¸åŒçš„åˆ†ææ–¹æ³•é€‚ç”¨äºä¸åŒæ¨¡æ€")
    print("   ğŸ“Š å‹ç¼©ç­–ç•¥: æ ¹æ®ä»»åŠ¡ç‰¹æ€§è‡ªåŠ¨è°ƒæ•´")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ Universal Layerwise-Adapter æ¡†æ¶æ¼”ç¤º")
    print("ä»Amazonæ¨èç³»ç»Ÿæ‰©å±•åˆ°é€šç”¨AIæ¨¡å‹åˆ†æ")
    
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
    torch.manual_seed(42)
    np.random.seed(42)
    
    try:
        # æ¼”ç¤ºæ–‡æœ¬åˆ†ç±»
        demo_text_classification()
        
        # æ¼”ç¤ºè§†è§‰åˆ†ç±»  
        demo_vision_classification()
        
        # å¯¹æ¯”åˆ†æ
        compare_modalities()
        
        print("\n" + "="*60)
        print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("ğŸŒŸ é€šç”¨æ¡†æ¶æˆåŠŸåˆ†æäº†ä¸åŒæ¨¡æ€çš„æ¨¡å‹")
        print("ğŸ“ˆ ç›¸åŒçš„åˆ†ææ–¹æ³•äº§ç”Ÿäº†æœ‰æ„ä¹‰çš„å±‚é‡è¦æ€§æ’åº")
        print("ğŸ¯ æ¡†æ¶å…·å¤‡äº†å‘æ›´å¤šé¢†åŸŸæ‰©å±•çš„åŸºç¡€èƒ½åŠ›")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
