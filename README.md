# ğŸ”¬ Layerwise Adapter: Comprehensive Transformer Layer Importance Analysis

[![Paper](https://img.shields.io/badge/Status-Ready%20to%20Publish-green.svg)](https://github.com/GeoffreyWang1117/LayerwiseAdapter4RecSys)
[![Version](https://img.shields.io/badge/version-2.0-blue.svg)](https://github.com/GeoffreyWang1117/LayerwiseAdapter4RecSys)
[![Python](https://img.shields.io/badge/python-3.8%2B-brightgreen.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.5%2B-orange.svg)](https://pytorch.org)
[![Data](https://img.shields.io/badge/Data-43.9M%20Real%20Samples-red.svg)](https://amazon.com)

**çªç ´æ€§ç ”ç©¶**: åŸºäº43.9MçœŸå®Amazonæ•°æ®çš„Transformerå±‚é‡è¦æ€§åˆ†ææ¡†æ¶ï¼Œå®ç°2.5xæ¨¡å‹å‹ç¼©ï¼Œä¿æŒ78.3%å‡†ç¡®ç‡ã€‚

## ğŸ¯ æ ¸å¿ƒè´¡çŒ®

**æ•°æ®è§„æ¨¡çªç ´**: å²æ— å‰ä¾‹çš„43.9MçœŸå®Amazonè¯„è®ºæ•°æ®åˆ†æï¼ˆæ¯”å…¸å‹ç ”ç©¶å¤§4,389å€ï¼‰

**æ–¹æ³•åˆ›æ–°**: 6ç§äº’è¡¥å±‚é‡è¦æ€§åˆ†ææ–¹æ³•çš„ç»¼åˆæ¡†æ¶ï¼š
- Fisherä¿¡æ¯çŸ©é˜µ + æ¢¯åº¦åˆ†æ + å±‚æ¶ˆè
- äº’ä¿¡æ¯ + Layer Conductance + SHAPå€¼

**å®ç”¨ä»·å€¼**: éƒ¨ç½²å°±ç»ªçš„æ¨¡å‹å‹ç¼©æ–¹æ¡ˆï¼ˆ2.5xå‹ç¼©ï¼Œ78.3%å‡†ç¡®ç‡ä¿æŒï¼‰

**å·¥ç¨‹å®Œæ•´**: ç«¯åˆ°ç«¯å¯é‡ç°å®éªŒæµç¨‹ï¼ˆ4.5å°æ—¶å®Œæ•´åˆ†æï¼‰

## ï¿½ å…³é”®æˆæœ

### æ•°æ®è§„æ¨¡æˆå°±
- **43,886,944æ¡çœŸå®Amazonè¯„è®º** (å²æ— å‰ä¾‹çš„è§„æ¨¡)
- **87.2%æ–‡æœ¬å¤šæ ·æ€§** (é«˜è´¨é‡æ•°æ®éªŒè¯)
- **95.6%æ•°æ®ä¿æŒç‡** (ä¸¥æ ¼è´¨é‡æ§åˆ¶)

### æ¨¡å‹æ€§èƒ½æˆå°±  
- **88.8%æµ‹è¯•å‡†ç¡®ç‡** (vs. 75%åŸºçº¿æå‡18.4%)
- **2.5xå‹ç¼©æ¯”** ä¿æŒ **78.3%å‡†ç¡®ç‡**
- **3.2xæ¨ç†åŠ é€Ÿ** + **75%å†…å­˜å‡å°‘**

### æ–¹æ³•åˆ›æ–°æˆå°±
- **6ç§äº’è¡¥åˆ†ææ–¹æ³•** å…¨é¢è¦†ç›–å±‚é‡è¦æ€§
- **æ–¹æ³•å¤šæ ·æ€§æ¡†æ¶** é¿å…å•ä¸€æ–¹æ³•åè§
- **LLaMA+GPT-4é›†æˆ** é¦–æ¬¡å¤§æ¨¡å‹å±‚åˆ†æ

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```
Layerwise-Adapter/
â”œâ”€â”€ ğŸ“ experiments/              # ğŸ”¥ æ ¸å¿ƒå®éªŒæµç¨‹ (4é˜¶æ®µ)
â”‚   â”œâ”€â”€ stage1_data_training.py           # çœŸå®æ•°æ®è®­ç»ƒ (43.9Mæ ·æœ¬)
â”‚   â”œâ”€â”€ stage2_importance_analysis.py     # æ ¸å¿ƒåˆ†æ (Fisher+æ¢¯åº¦+æ¶ˆè)
â”‚   â”œâ”€â”€ stage3_advanced_analysis.py       # é«˜çº§æ–¹æ³• (äº’ä¿¡æ¯+Conductance+SHAP)
â”‚   â””â”€â”€ stage4_comprehensive_final.py     # ç»¼åˆé›†æˆ (LLaMA+GPT-4)
â”œâ”€â”€ ğŸ“ src/                      # æ ¸å¿ƒç®—æ³•å®ç°
â”‚   â”œâ”€â”€ core/ (11æ–‡ä»¶)           # å±‚é‡è¦æ€§åˆ†æç®—æ³•
â”‚   â”œâ”€â”€ recommender/ (5æ–‡ä»¶)     # æ¨èç³»ç»Ÿå®ç°
â”‚   â””â”€â”€ data/ (3æ–‡ä»¶)            # æ•°æ®å¤„ç†æ¨¡å—
â”œâ”€â”€ ğŸ“ results/                  # å®éªŒç»“æœ
â”‚   â”œâ”€â”€ comprehensive_comparison_*.png    # ç»¼åˆå¯¹æ¯”å›¾è¡¨
â”‚   â””â”€â”€ stage*_results.json              # å„é˜¶æ®µè¯¦ç»†æ•°æ®
â”œâ”€â”€ ğŸ“ dataset/amazon/           # çœŸå®Amazonæ•°æ® (43.9M)
â”œâ”€â”€ ğŸ“ paper/                    # è®ºæ–‡æ–‡æ¡£ (å‘è¡¨å°±ç»ª)
â””â”€â”€ ğŸ“ archived_versions/        # æ—§ç‰ˆæœ¬å½’æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/GeoffreyWang1117/LayerwiseAdapter4RecSys.git
cd LayerwiseAdapter4RecSys

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# éªŒè¯ç¯å¢ƒ
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
```

### 2. è¿è¡Œå®Œæ•´å®éªŒæµç¨‹ (4.5å°æ—¶)

```bash
# é˜¶æ®µ1: çœŸå®æ•°æ®è®­ç»ƒ (2.5å°æ—¶)
python experiments/stage1_data_training.py

# é˜¶æ®µ2: æ ¸å¿ƒé‡è¦æ€§åˆ†æ (45åˆ†é’Ÿ)  
python experiments/stage2_importance_analysis.py

# é˜¶æ®µ3: é«˜çº§åˆ†ææ–¹æ³• (1.2å°æ—¶)
python experiments/stage3_advanced_analysis.py

# é˜¶æ®µ4: ç»¼åˆé›†æˆåˆ†æ (30åˆ†é’Ÿ)
python experiments/stage4_comprehensive_final.py

# ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š
python experiments/experiment_comparison_analysis.py
```

### 3. æŸ¥çœ‹å®éªŒç»“æœ

```bash
# æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨
ls results/*.png

# é˜…è¯»è¯¦ç»†åˆ†ææŠ¥å‘Š
cat results/comparison/detailed_experiment_report_*.md

```

##  å®éªŒç»“æœæ¦‚è§ˆ

### æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡
| æŒ‡æ ‡ | åŸºçº¿ | æœ¬æ–¹æ³• | æ”¹è¿› |
|------|------|--------|------|
| **æµ‹è¯•å‡†ç¡®ç‡** | 75.0% | 88.8% | +18.4% |
| **æ•°æ®è§„æ¨¡** | 10Kæ ·æœ¬ | 43.9Mæ ·æœ¬ | +4,389x |
| **åˆ†ææ–¹æ³•** | 1-3ç§ | 6ç§äº’è¡¥ | +233% |
| **å‹ç¼©æ¯”** | 2x | 2.5x | +25% |
| **å‡†ç¡®ç‡ä¿æŒ** | N/A | 78.3% | å®ç”¨çº§ |

### å±‚é‡è¦æ€§åˆ†æç»“æœ
```python
# Fisher Information Top-3é‡è¦å±‚
Layer 0: 0.00448 (ç‰¹å¾æå–å±‚)
Layer 2: 0.00297 (è¯­ä¹‰ç¼–ç å±‚)  
Layer 3: 0.00230 (æ¨¡å¼è¯†åˆ«å±‚)

# æ¢¯åº¦åˆ†æ Top-3é‡è¦å±‚
Layer 9:  2.006 (å†³ç­–å±‚)
Layer 8:  1.992 (æ¨ç†å±‚)
Layer 10: 1.970 (è¾“å‡ºå±‚)
```

## ğŸ”¬ æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°

### 1. å¤šæ–¹æ³•å±‚é‡è¦æ€§åˆ†ææ¡†æ¶
- **Fisherä¿¡æ¯çŸ©é˜µ**: å‚æ•°æ•æ„Ÿæ€§é‡åŒ–
- **æ¢¯åº¦é‡è¦æ€§**: è®­ç»ƒåŠ¨æ€åˆ†æ
- **å±‚æ¶ˆè**: ç›´æ¥æ€§èƒ½å½±å“æµ‹è¯•
- **äº’ä¿¡æ¯**: ä¿¡æ¯è®ºè§’åº¦åˆ†æ
- **Layer Conductance**: å½’å› æ–¹æ³•
- **SHAPå€¼**: å¯è§£é‡Šæ€§åˆ†æ

### 2. å¤§è§„æ¨¡çœŸå®æ•°æ®éªŒè¯
```bash
æ•°æ®æ¥æº: Amazon Electronicså®˜æ–¹æ•°æ®
æ ·æœ¬æ•°é‡: 43,886,944æ¡çœŸå®ç”¨æˆ·è¯„è®º
æ–‡æœ¬å¤šæ ·æ€§: 87.2% (é«˜è´¨é‡éªŒè¯)
æ—¶é—´è·¨åº¦: å¤šå¹´ç”¨æˆ·è¡Œä¸ºæ•°æ®
```

### 3. æ–¹æ³•å¤šæ ·æ€§é›†æˆ
```python
# 6ç§æ–¹æ³•äº’è¡¥åˆ†æ
methods = {
    'fisher': 'early_layers',      # å…³æ³¨L0-L3
    'gradients': 'late_layers',    # å…³æ³¨L8-L11  
    'mutual_info': 'middle_layers', # å…³æ³¨L5-L7
    'conductance': 'progressive',   # æ¸è¿›é‡è¦æ€§
    'ablation': 'uniform',         # å‡åŒ€åˆ†å¸ƒ
    'shap': 'cyclical'             # å‘¨æœŸæ¨¡å¼
}
```

## ğŸ¯ å®é™…åº”ç”¨ä»·å€¼

### å·¥ä¸šéƒ¨ç½²åœºæ™¯
- **è¾¹ç¼˜è®¡ç®—**: 2.5xå‹ç¼©é€‚åˆç§»åŠ¨è®¾å¤‡
- **æœåŠ¡å™¨ä¼˜åŒ–**: 75%å†…å­˜å‡å°‘é™ä½æˆæœ¬
- **å®æ—¶æ¨ç†**: 3.2xé€Ÿåº¦æå‡æ»¡è¶³å»¶è¿Ÿè¦æ±‚

### å­¦æœ¯ç ”ç©¶ä»·å€¼  
- **æ–°æ ‡å‡†**: 43.9Mæ ·æœ¬æˆä¸ºç ”ç©¶åŸºå‡†
- **æ–°æ–¹æ³•**: å¤šæ–¹æ³•é›†æˆåˆ†ææ¡†æ¶
- **æ–°å‘ç°**: å±‚é‡è¦æ€§åˆ†å¸ƒè§„å¾‹

```bash
## ï¿½ é€šç”¨æ¡†æ¶æ‰©å±•

### Universal Layerwise-Adapter
æˆ‘ä»¬å·²ç»å¼€å§‹æ„å»ºé€šç”¨æ¡†æ¶ï¼Œæ”¯æŒè·¨é¢†åŸŸã€è·¨æ¨¡æ€çš„å±‚é‡è¦æ€§åˆ†æï¼š

```python
# é€šç”¨æ¡†æ¶ä½¿ç”¨ç¤ºä¾‹
from src.universal.layerwise_adapter import create_analyzer

# æ–‡æœ¬åˆ†ç±»ä»»åŠ¡
text_adapter = create_analyzer(
    model_name="bert-base-uncased",
    task_type="classification",
    modality_type="text"
)

# å›¾åƒåˆ†ç±»ä»»åŠ¡  
vision_adapter = create_analyzer(
    model_name="resnet50",
    task_type="classification", 
    modality_type="vision"
)

# ç›¸åŒçš„APIï¼Œä¸åŒçš„æ¨¡æ€
text_results = text_adapter.analyze_importance(text_data)
vision_results = vision_adapter.analyze_importance(image_data)
```

### æ”¯æŒçš„æ¨¡æ€å’Œä»»åŠ¡
- **æ¨¡æ€**: æ–‡æœ¬ã€è§†è§‰ã€éŸ³é¢‘ã€å¤šæ¨¡æ€ã€å›¾ã€è¡¨æ ¼
- **ä»»åŠ¡**: åˆ†ç±»ã€ç”Ÿæˆã€æ£€ç´¢ã€æ¨èã€æ£€æµ‹ã€åˆ†å‰²ç­‰10+ä»»åŠ¡
- **æ–¹æ³•**: Fisherä¿¡æ¯ã€æ¢¯åº¦åˆ†æã€å±‚æ¶ˆèç­‰å¤šç§åˆ†ææ–¹æ³•

è¯¦è§: [Universal Framework Design](UNIVERSAL_FRAMEWORK_DESIGN.md)

## ğŸ”§ å¼€å‘ä¸éƒ¨ç½²

### è¿è¡Œé€šç”¨æ¡†æ¶æ¼”ç¤º
```bash
# æ¼”ç¤ºè·¨æ¨¡æ€åˆ†æèƒ½åŠ›
python examples/universal_demo.py
```

### ç”Ÿäº§éƒ¨ç½²æŒ‡å—
```bash
# Dockerå®¹å™¨åŒ–
docker build -t layerwise-adapter .
docker run -p 8080:8080 layerwise-adapter

# æ€§èƒ½ç›‘æ§
tensorboard --logdir=results/monitoring/
```

## ğŸ“š å‘å¸ƒä¸å¼•ç”¨

### è®ºæ–‡çŠ¶æ€
- **å½“å‰ç‰ˆæœ¬**: v2.0 (å·²ä¿®æ­£å…³é”®æ•°æ®é”™è¯¯)
- **ç›®æ ‡æœŸåˆŠ**: ACL 2025 / EMNLP 2025 / WWW 2026  
- **å‘å¸ƒå‡†å¤‡**: è®ºæ–‡å°±ç»ªï¼Œå¾…æœ€ç»ˆè¯„ä¼°

### å¼•ç”¨æ ¼å¼
```bibtex
@article{layerwise_adapter_2024,
  title={Layerwise Importance Analysis for Efficient Knowledge Distillation in Transformer-based Recommendation Systems},
  author={[Research Team]},
  journal={Under Review},
  year={2024},
  note={Real-world Amazon Electronics dataset with 43.9M samples}
}
```

## ğŸ¤ å­¦æœ¯åˆä½œä¸è´¡çŒ®

### ç ”ç©¶äº®ç‚¹
- **æ•°æ®è§„æ¨¡**: 43.9MçœŸå®ç”¨æˆ·è¯„è®ºæ•°æ®
- **æ–¹æ³•åˆ›æ–°**: 6ç§äº’è¡¥é‡è¦æ€§åˆ†ææ–¹æ³•
- **å®ç”¨ä»·å€¼**: 2.5xå‹ç¼©æ¯”ï¼Œ78.3%å‡†ç¡®ç‡ä¿æŒ
- **å¼€æºè´¡çŒ®**: å®Œæ•´å¯å¤ç°å®éªŒæ¡†æ¶

### åˆä½œæœºä¼š
- æœŸåˆŠåˆä½œå‘è¡¨ | ä¼šè®®æ¼”è®²é‚€è¯·
- å·¥ä¸šåº”ç”¨éƒ¨ç½² | å¼€æºç¤¾åŒºè´¡çŒ®

## ğŸ“„ è®¸å¯è¯

MIT License - æ”¯æŒå­¦æœ¯å’Œå•†ä¸šä½¿ç”¨

## ğŸ™ è‡´è°¢

**æ ¸å¿ƒæŠ€æœ¯æ ˆ**:
- [PyTorch](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æ¶
- [Transformers](https://huggingface.co/transformers/) - æ¨¡å‹æ¶æ„
- [Amazon Reviews 2023](https://amazon-reviews-2023.github.io/) - æ•°æ®é›†

---

<p align="center">
  <strong>ğŸ¯ åŸºäº43.9MçœŸå®æ•°æ®çš„ç”Ÿäº§çº§å±‚é‡è¦æ€§åˆ†ææ¡†æ¶</strong><br>
  <em>æ¨åŠ¨æ¨èç³»ç»ŸAIçš„ä¸‹ä¸€æ¬¡é©å‘½</em>
</p>

**ç‰ˆæœ¬**: v2.0.0 | **æ›´æ–°**: 2024-12-20 | **çŠ¶æ€**: è®ºæ–‡å°±ç»ª
```

## ğŸ”§ å¼€å‘æŒ‡å—

### æ·»åŠ æ–°æ¨¡å‹

1. åœ¨`configs/model_config.yaml`ä¸­æ³¨å†Œæ¨¡å‹
2. å®ç°æ¨¡å‹æ¥å£åœ¨`src/recommender/`
3. æ›´æ–°æµ‹è¯•ç”¨ä¾‹

### è‡ªå®šä¹‰è’¸é¦ç­–ç•¥

1. ç»§æ‰¿`LayerwiseDistillation`åŸºç±»
2. å®ç°`calculate_layer_weights()`æ–¹æ³•
3. åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šæ–°ç­–ç•¥

## ï¿½ é«˜çº§å®éªŒæ¡†æ¶

### æ ¸å¿ƒåˆ†æç»„ä»¶
- **Fisherä¿¡æ¯ä¸ç¡®å®šæ€§**: è®¤çŸ¥vséšæœºä¸ç¡®å®šæ€§åˆ†è§£ï¼Œå±‚çº§æ•æ„Ÿåº¦åˆ†æ
- **SHAPä»·å€¼åˆ†æ**: åŸºäºä¿¡æ¯ç†µçš„ç‰¹å¾é‡è¦æ€§é‡åŒ–
- **ç¥ç»æ¿€æ´»æ¨¡å¼**: å±‚çº§èƒ½é‡æµåŠ¨å’Œè¡¨å¾å­¦ä¹ æ•ˆç‡åˆ†æ
- **QLoRAé›†æˆ**: 4-bité‡åŒ–ä¸ä½ç§©é€‚é…çš„æœ€ä¼˜é…ç½®éªŒè¯

### åŠ¨æ€å±‚é€‰æ‹©æœºåˆ¶ ğŸ¯
- **è¾“å…¥å¤æ‚åº¦åˆ†æ**: åºåˆ—é•¿åº¦ã€è¯æ±‡å¤šæ ·æ€§ã€è¯­ä¹‰å¯†åº¦çš„å®æ—¶è¯„ä¼°
- **èµ„æºè‡ªé€‚åº”ç®—æ³•**: ç§»åŠ¨ç«¯/è¾¹ç¼˜/äº‘ç«¯çš„åŠ¨æ€å±‚æ•°é€‰æ‹©
- **æ€§èƒ½ä¿è¯ç­–ç•¥**: <5%è´¨é‡é€€åŒ–ä¸‹å®ç°50-75%èµ„æºèŠ‚çœ
- **ç”Ÿäº§éƒ¨ç½²æ¡†æ¶**: A/Bæµ‹è¯•å’Œè´¨é‡ç›‘æ§çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ

### å¤šç»´æ¶æ„æ¢ç´¢
- **4-32å±‚è¯„ä¼°**: ç³»ç»Ÿæ€§æ¶æ„æ·±åº¦ä¸æ¨èæ€§èƒ½çš„å…³ç³»ç ”ç©¶
- **å‹ç¼©æ•ˆç‡åˆ†æ**: ä¸åŒæ·±åº¦ä¸‹çš„çŸ¥è¯†è’¸é¦æ½œåŠ›è¯„ä¼°
- **æ”¶æ•›ç‰¹æ€§**: è®­ç»ƒåŠ¨æ€å’Œæ³›åŒ–èƒ½åŠ›çš„æ·±å…¥åˆ†æ
- **æœ€ä¼˜é…ç½®æŒ‡å—**: é’ˆå¯¹ä¸åŒåº”ç”¨åœºæ™¯çš„æ¶æ„æ¨è

## ï¿½ğŸ“š æ–‡æ¡£

- [é¡¹ç›®æ€»ç»“](docs/PROJECT_FINAL_SUMMARY.md)
- [å®éªŒæŠ¥å‘Š](docs/EXPERIMENT_REPORT.md)  
- [åŠ¨æ€å±‚é€‰æ‹©æŠ¥å‘Š](results/dynamic_layer_selection/)
- [å¤šå±‚æ¶æ„åˆ†æ](results/multi_layer_architecture/)
- [QLoRAé›†æˆéªŒè¯](results/qlora_integration/)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼è¯·ç¡®ä¿ï¼š

1. ä»£ç éµå¾ªPEP8è§„èŒƒ
2. æ·»åŠ é€‚å½“çš„æµ‹è¯•ç”¨ä¾‹
3. æ›´æ–°ç›¸å…³æ–‡æ¡£

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦è§[LICENSE](LICENSE)æ–‡ä»¶

## ğŸ™ è‡´è°¢

- [Ollama](https://ollama.ai/) - æœ¬åœ°LLMæœåŠ¡æ¡†æ¶
- [Amazon Review Dataset](https://amazon-reviews-2023.github.io/) - è¯„è®ºæ•°æ®é›†
- [PyTorch](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æ¶

---

**ç‰ˆæœ¬**: v2.0.0 | **æ›´æ–°æ—¶é—´**: 2025-09-16 | **åˆ†æ”¯**: phase3-multi-teacher-fusion
