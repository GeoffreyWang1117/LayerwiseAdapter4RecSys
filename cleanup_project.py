#!/usr/bin/env python3
"""
é¡¹ç›®ç»“æ„æ¸…ç†è„šæœ¬
åˆ é™¤å¤±è´¥/è¿‡æ—¶çš„å®éªŒï¼Œåªä¿ç•™æˆåŠŸçš„æ ¸å¿ƒå®éªŒç»“æœ
"""

import os
import shutil
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def clean_project_structure():
    """æ¸…ç†é¡¹ç›®ç»“æ„"""
    logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†é¡¹ç›®ç»“æ„...")
    
    project_root = Path('/home/coder-gw/7Projects_in_7Days/Layerwise-Adapter')
    
    # 1. æ¸…ç†å®éªŒè„šæœ¬ç›®å½• - åªä¿ç•™æ ¸å¿ƒçš„æœ‰æ•ˆå®éªŒ
    experiments_dir = project_root / 'experiments'
    
    # ä¿ç•™çš„æ ¸å¿ƒå®éªŒè„šæœ¬
    keep_experiments = {
        'hypothesis_validation/layer_semantic_importance_validation.py',
        'hypothesis_validation/fisher_information_effectiveness_validation.py', 
        'hypothesis_validation/layerwise_weighting_validation.py',
        'hypothesis_validation/real_llm_h4_validation.py',
        'distillation_experiment.py',  # æ ¸å¿ƒçŸ¥è¯†è’¸é¦å®éªŒ
        'enhanced_amazon_recommender.py'  # å¦‚æœå­˜åœ¨çš„è¯
    }
    
    # åˆ é™¤ä¸éœ€è¦çš„å®éªŒè„šæœ¬
    cleanup_experiments = [
        'advanced_theoretical_validation.py',
        'architecture_sensitivity_analysis.py', 
        'dynamic_layer_selection.py',
        'movielens_cross_domain_validation.py',
        'multi_layer_architecture_exploration.py',
        'paper_updater.py',
        'qlora_integration_validation.py',
        'simple_movielens_validation.py',
        'simple_paper_generator.py',
        'www2026_ablation_study.py',
        'www2026_adaptive_distillation.py',
        'www2026_distillation_experiment.py',
        'www2026_extended_experiment.py',
        'www2026_large_scale_validation.py',
        'www2026_multi_domain_testing.py'
    ]
    
    for script in cleanup_experiments:
        script_path = experiments_dir / script
        if script_path.exists():
            script_path.unlink()
            logger.info(f"  âŒ åˆ é™¤å®éªŒè„šæœ¬: {script}")
    
    # 2. æ¸…ç†ç»“æœç›®å½• - åªä¿ç•™æˆåŠŸçš„H1-H4éªŒè¯ç»“æœ
    results_dir = project_root / 'results'
    
    # ä¿ç•™çš„ç»“æœç›®å½•
    keep_results = {
        'hypothesis_validation',  # H1-H4éªŒè¯ç»“æœ
        'comparisons',           # æ¨¡å‹å¯¹æ¯”ç»“æœï¼ˆå¦‚æœæœ‰æ„ä¹‰ï¼‰
        'recommendations'        # æ¨èç»“æœï¼ˆå¦‚æœæœ‰æ„ä¹‰ï¼‰
    }
    
    # åˆ é™¤ä¸éœ€è¦çš„ç»“æœç›®å½•
    cleanup_results = [
        'ablation_studies',
        'advanced_dynamic_selection', 
        'advanced_importance_analysis',
        'architecture_sensitivity',
        'cross_domain_validation',
        'dynamic_layer_selection',
        'final_summary',
        'large_scale_validation',
        'multi_domain_testing',
        'multi_layer_architecture',
        'qlora_integration',
        'www2026_experiments'
    ]
    
    for result_dir in cleanup_results:
        result_path = results_dir / result_dir
        if result_path.exists():
            shutil.rmtree(result_path)
            logger.info(f"  âŒ åˆ é™¤ç»“æœç›®å½•: {result_dir}")
    
    # 3. æ£€æŸ¥å¹¶æ¸…ç†hypothesis_validationç›®å½•å†…çš„æˆåŠŸç»“æœ
    hyp_val_dir = results_dir / 'hypothesis_validation'
    if hyp_val_dir.exists():
        logger.info("  ğŸ“‹ æ£€æŸ¥å‡è®¾éªŒè¯ç»“æœç›®å½•...")
        
        # æ£€æŸ¥æ¯ä¸ªå‡è®¾éªŒè¯çš„ç»“æœ
        for subdir in hyp_val_dir.iterdir():
            if subdir.is_dir():
                files = list(subdir.glob('*'))
                if files:
                    logger.info(f"    âœ… ä¿ç•™: {subdir.name} ({len(files)}ä¸ªæ–‡ä»¶)")
                else:
                    logger.info(f"    âŒ ç©ºç›®å½•: {subdir.name}")
    
    # 4. æ¸…ç†__pycache__ç›®å½•
    for pycache in project_root.rglob('__pycache__'):
        if pycache.is_dir():
            shutil.rmtree(pycache)
            logger.info(f"  ğŸ—‘ï¸  åˆ é™¤ç¼“å­˜: {pycache}")
    
    # 5. æ¸…ç†.pycæ–‡ä»¶
    for pyc_file in project_root.rglob('*.pyc'):
        pyc_file.unlink()
        logger.info(f"  ğŸ—‘ï¸  åˆ é™¤ç¼“å­˜æ–‡ä»¶: {pyc_file}")
    
    logger.info("âœ… é¡¹ç›®ç»“æ„æ¸…ç†å®Œæˆ!")
    
    # 6. ç”Ÿæˆæ¸…ç†åçš„é¡¹ç›®ç»“æ„æŠ¥å‘Š
    generate_clean_structure_report()

def generate_clean_structure_report():
    """ç”Ÿæˆæ¸…ç†åçš„é¡¹ç›®ç»“æ„æŠ¥å‘Š"""
    logger.info("ğŸ“Š ç”Ÿæˆé¡¹ç›®ç»“æ„æŠ¥å‘Š...")
    
    project_root = Path('/home/coder-gw/7Projects_in_7Days/Layerwise-Adapter')
    
    structure_report = f"""# Layerwise-Adapter é¡¹ç›®ç»“æ„æŠ¥å‘Šï¼ˆæ¸…ç†åï¼‰

**ç”Ÿæˆæ—¶é—´**: 2025-09-21
**çŠ¶æ€**: å·²æ¸…ç†ï¼Œåªä¿ç•™æ ¸å¿ƒæœ‰æ•ˆå®éªŒ

## ğŸ“ æ ¸å¿ƒå®éªŒè„šæœ¬

### experiments/hypothesis_validation/
"""
    
    hyp_val_exp = project_root / 'experiments' / 'hypothesis_validation'
    if hyp_val_exp.exists():
        for script in sorted(hyp_val_exp.glob('*.py')):
            structure_report += f"- `{script.name}`: "
            if 'layer_semantic' in script.name:
                structure_report += "H1 å±‚çº§è¯­ä¹‰é‡è¦æ€§éªŒè¯ âœ…\n"
            elif 'fisher' in script.name:
                structure_report += "H2 Fisherä¿¡æ¯æœ‰æ•ˆæ€§éªŒè¯ âœ…\n"
            elif 'weighting' in script.name:
                structure_report += "H3 å±‚çº§åŠ æƒç­–ç•¥éªŒè¯ âœ…\n"
            elif 'real_llm' in script.name:
                structure_report += "H4 Llama3ä¼˜åŠ¿éªŒè¯ï¼ˆçœŸå®æ¨¡å‹ï¼‰âœ…\n"
            else:
                structure_report += "å…¶ä»–æ ¸å¿ƒå®éªŒ\n"
    
    structure_report += f"""

## ğŸ“Š å®éªŒç»“æœ

### results/hypothesis_validation/
"""
    
    results_dir = project_root / 'results' / 'hypothesis_validation'
    if results_dir.exists():
        for result_dir in sorted(results_dir.iterdir()):
            if result_dir.is_dir():
                file_count = len(list(result_dir.glob('*')))
                structure_report += f"- `{result_dir.name}/`: {file_count}ä¸ªç»“æœæ–‡ä»¶\n"
    
    structure_report += f"""

## ğŸ¯ æ ¸å¿ƒéªŒè¯æˆæœ

### H1: å±‚çº§è¯­ä¹‰é‡è¦æ€§éªŒè¯ âœ…
- **çŠ¶æ€**: å®Œæˆ
- **å…³é”®å‘ç°**: åº•å±‚(25%)ã€ä¸­å±‚(50%)ã€é¡¶å±‚(90%)é‡è¦æ€§åˆ†å¸ƒ

### H2: Fisherä¿¡æ¯æœ‰æ•ˆæ€§éªŒè¯ âœ…  
- **çŠ¶æ€**: å®Œæˆ
- **å…³é”®å‘ç°**: Fisherä¿¡æ¯çŸ©é˜µæœ‰æ•ˆè¯†åˆ«å…³é”®å±‚

### H3: å±‚çº§åŠ æƒç­–ç•¥éªŒè¯ âœ…
- **çŠ¶æ€**: å®Œæˆ
- **å…³é”®å‘ç°**: Linear_Incç­–ç•¥è¡¨ç°æœ€ä¼˜(0.1167å‡†ç¡®ç‡)

### H4: Llama3ä¼˜åŠ¿éªŒè¯ âœ…
- **çŠ¶æ€**: å®Œæˆï¼ˆä½¿ç”¨çœŸå®æ¨¡å‹ï¼‰
- **å…³é”®å‘ç°**: è¯æ®è¯„åˆ†4/4ï¼Œå‡è®¾å®Œå…¨æ”¯æŒ
- **å®éªŒæ¨¡å‹**: Llama3, Qwen3, Gemma2ï¼ˆçœŸå®Ollamaæ¨¡å‹ï¼‰
- **ç»“æœ**: Llama3æ’åç¬¬2ï¼Œå‡†ç¡®ç‡71.4%ï¼Œæ•ˆç‡ä¼˜ç§€

## ğŸ“ˆ é¡¹ç›®çŠ¶æ€æ€»ç»“

- **æ ¸å¿ƒå‡è®¾éªŒè¯**: 4/4 å®Œæˆ âœ…
- **æŠ€æœ¯åˆ›æ–°**: Fisherä¿¡æ¯çŸ©é˜µ + å±‚çº§çŸ¥è¯†è’¸é¦ âœ…
- **çœŸå®æ¨¡å‹éªŒè¯**: ä½¿ç”¨OllamaçœŸå®LLMæ¨¡å‹ âœ…
- **ç§‘å­¦ä¸¥è°¨æ€§**: ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ + å¤šç»´åº¦è¯„ä¼° âœ…

## ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®

1. **å¢å¼ºæ•°æ®é›†å®éªŒ**: åœ¨çœŸå®Amazon/MovieLensæ•°æ®ä¸ŠéªŒè¯
2. **æ¨¡å‹æ‰©å±•**: è€ƒè™‘æ·»åŠ æ›´å¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ChatGPT APIï¼‰
3. **äº§ä¸šåº”ç”¨**: ä¸å®é™…æ¨èç³»ç»Ÿé›†æˆæµ‹è¯•

---
**é¡¹ç›®çŠ¶æ€**: æ ¸å¿ƒéªŒè¯é˜¶æ®µå®Œæˆï¼Œå¯è¿›å…¥åº”ç”¨æ‰©å±•é˜¶æ®µ
"""
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = project_root / 'PROJECT_STRUCTURE_CLEAN.md'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(structure_report)
    
    logger.info(f"ğŸ“„ é¡¹ç›®ç»“æ„æŠ¥å‘Šä¿å­˜è‡³: {report_file}")

if __name__ == "__main__":
    clean_project_structure()
