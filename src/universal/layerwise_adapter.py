#!/usr/bin/env python3
"""
é€šç”¨Layerwise-Adapteræ¡†æ¶æ ¸å¿ƒå®ç°
Universal Framework for Layer Importance Analysis and Model Compression

Author: Layerwise-Adapter Research Team
Date: 2024-12-20
Version: 1.0.0-alpha
"""

import os
import abc
import torch
import torch.nn as nn
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass
from enum import Enum
import numpy as np
from pathlib import Path
import logging
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TaskType(Enum):
    """æ”¯æŒçš„ä»»åŠ¡ç±»å‹"""
    CLASSIFICATION = "classification"
    GENERATION = "generation"
    RETRIEVAL = "retrieval"
    RECOMMENDATION = "recommendation"
    DETECTION = "detection"
    SEGMENTATION = "segmentation"
    TRANSLATION = "translation"
    SUMMARIZATION = "summarization"
    QA = "question_answering"
    REINFORCEMENT_LEARNING = "reinforcement_learning"

class ModalityType(Enum):
    """æ”¯æŒçš„æ¨¡æ€ç±»å‹"""
    TEXT = "text"
    VISION = "vision"
    AUDIO = "audio"
    MULTIMODAL = "multimodal"
    GRAPH = "graph"
    TABULAR = "tabular"

@dataclass
class AnalysisConfig:
    """åˆ†æé…ç½®"""
    model_name: str
    task_type: TaskType
    modality_type: ModalityType
    batch_size: int = 32
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    random_seed: int = 42
    max_samples: int = 10000
    compression_targets: List[float] = None
    analysis_methods: List[str] = None
    
    def __post_init__(self):
        if self.compression_targets is None:
            self.compression_targets = [1.5, 2.0, 2.5, 3.0]
        if self.analysis_methods is None:
            self.analysis_methods = ['fisher_information', 'gradient_based', 'layer_ablation']

class Layer:
    """é€šç”¨å±‚è¡¨ç¤º"""
    def __init__(self, module: nn.Module, layer_idx: int, layer_name: str):
        self.module = module
        self.layer_idx = layer_idx  
        self.layer_name = layer_name
        self.parameters = sum(p.numel() for p in module.parameters())
        
    def __repr__(self):
        return f"Layer({self.layer_idx}, {self.layer_name}, {self.parameters} params)"

class UniversalModel(abc.ABC):
    """é€šç”¨æ¨¡å‹æŠ½è±¡åŸºç±»"""
    
    def __init__(self, model: nn.Module, config: AnalysisConfig):
        self.model = model
        self.config = config
        self.layers: List[Layer] = []
        self.hooks: List = []
        self.activations: Dict[int, torch.Tensor] = {}
        self._initialize_layers()
        
    @abc.abstractmethod
    def _initialize_layers(self):
        """åˆå§‹åŒ–å±‚åˆ—è¡¨ - å­ç±»å¿…é¡»å®ç°"""
        pass
        
    @abc.abstractmethod
    def get_layer_output(self, x: torch.Tensor, layer_idx: int) -> torch.Tensor:
        """è·å–æŒ‡å®šå±‚çš„è¾“å‡º - å­ç±»å¿…é¡»å®ç°"""
        pass
        
    def forward_with_hooks(self, x: torch.Tensor) -> Dict[int, torch.Tensor]:
        """å¸¦é’©å­çš„å‰å‘ä¼ æ’­ï¼Œè·å–æ‰€æœ‰å±‚çš„æ¿€æ´»"""
        self.activations.clear()
        
        def make_hook(idx):
            def hook(module, input, output):
                self.activations[idx] = output.detach()
            return hook
            
        # æ³¨å†Œé’©å­
        self.hooks = []
        for layer in self.layers:
            hook = layer.module.register_forward_hook(make_hook(layer.layer_idx))
            self.hooks.append(hook)
            
        # å‰å‘ä¼ æ’­
        _ = self.model(x)
        
        # ç§»é™¤é’©å­
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
            
        return self.activations.copy()
        
    def get_layer_parameters(self, layer_idx: int) -> Dict[str, torch.Tensor]:
        """è·å–æŒ‡å®šå±‚çš„å‚æ•°"""
        if layer_idx >= len(self.layers):
            raise IndexError(f"Layer index {layer_idx} out of range")
        
        layer = self.layers[layer_idx]
        return {name: param for name, param in layer.module.named_parameters()}

class ImportanceAnalyzer(abc.ABC):
    """é‡è¦æ€§åˆ†æå™¨æŠ½è±¡åŸºç±»"""
    
    def __init__(self, name: str):
        self.name = name
        
    @abc.abstractmethod
    def analyze(self, model: UniversalModel, data_loader) -> Dict[int, float]:
        """åˆ†æå„å±‚é‡è¦æ€§
        
        Args:
            model: é€šç”¨æ¨¡å‹å®ä¾‹
            data_loader: æ•°æ®åŠ è½½å™¨
            
        Returns:
            Dict[int, float]: å±‚ç´¢å¼•åˆ°é‡è¦æ€§å¾—åˆ†çš„æ˜ å°„
        """
        pass
        
    def normalize_scores(self, scores: Dict[int, float]) -> Dict[int, float]:
        """æ ‡å‡†åŒ–é‡è¦æ€§å¾—åˆ†åˆ°[0,1]åŒºé—´"""
        if not scores:
            return {}
            
        values = list(scores.values())
        min_val, max_val = min(values), max(values)
        
        if max_val == min_val:
            return {k: 1.0 for k in scores.keys()}
            
        return {k: (v - min_val) / (max_val - min_val) for k, v in scores.items()}

class FisherInformationAnalyzer(ImportanceAnalyzer):
    """Fisherä¿¡æ¯åˆ†æå™¨"""
    
    def __init__(self):
        super().__init__("fisher_information")
        
    def analyze(self, model: UniversalModel, data_loader) -> Dict[int, float]:
        """åŸºäºFisherä¿¡æ¯çŸ©é˜µè®¡ç®—å±‚é‡è¦æ€§"""
        logger.info("å¼€å§‹Fisherä¿¡æ¯åˆ†æ...")
        
        model.model.eval()
        layer_fisher_scores = defaultdict(float)
        
        for batch_idx, (data, targets) in enumerate(data_loader):
            if batch_idx >= 10:  # é™åˆ¶è®¡ç®—é‡
                break
                
            data = data.to(model.config.device)
            targets = targets.to(model.config.device)
            
            # å‰å‘ä¼ æ’­
            outputs = model.model(data)
            loss = nn.CrossEntropyLoss()(outputs, targets)
            
            # åå‘ä¼ æ’­
            model.model.zero_grad()
            loss.backward()
            
            # è®¡ç®—æ¯å±‚çš„Fisherä¿¡æ¯
            for layer in model.layers:
                fisher_score = 0.0
                for param in layer.module.parameters():
                    if param.grad is not None:
                        fisher_score += (param.grad ** 2).sum().item()
                layer_fisher_scores[layer.layer_idx] += fisher_score
                
        # å¹³å‡åŒ–
        for layer_idx in layer_fisher_scores:
            layer_fisher_scores[layer_idx] /= min(len(data_loader), 10)
            
        logger.info(f"Fisherä¿¡æ¯åˆ†æå®Œæˆï¼Œå…±åˆ†æ{len(layer_fisher_scores)}å±‚")
        return dict(layer_fisher_scores)

class GradientBasedAnalyzer(ImportanceAnalyzer):
    """åŸºäºæ¢¯åº¦çš„åˆ†æå™¨"""
    
    def __init__(self):
        super().__init__("gradient_based")
        
    def analyze(self, model: UniversalModel, data_loader) -> Dict[int, float]:
        """åŸºäºæ¢¯åº¦å¹…åº¦è®¡ç®—å±‚é‡è¦æ€§"""
        logger.info("å¼€å§‹æ¢¯åº¦åˆ†æ...")
        
        model.model.eval()
        layer_gradient_scores = defaultdict(float)
        
        for batch_idx, (data, targets) in enumerate(data_loader):
            if batch_idx >= 10:  # é™åˆ¶è®¡ç®—é‡
                break
                
            data = data.to(model.config.device)
            targets = targets.to(model.config.device)
            
            # å‰å‘ä¼ æ’­
            outputs = model.model(data)
            loss = nn.CrossEntropyLoss()(outputs, targets)
            
            # åå‘ä¼ æ’­
            model.model.zero_grad()
            loss.backward()
            
            # è®¡ç®—æ¯å±‚çš„æ¢¯åº¦å¹…åº¦
            for layer in model.layers:
                gradient_norm = 0.0
                for param in layer.module.parameters():
                    if param.grad is not None:
                        gradient_norm += param.grad.norm().item()
                layer_gradient_scores[layer.layer_idx] += gradient_norm
                
        # å¹³å‡åŒ–
        for layer_idx in layer_gradient_scores:
            layer_gradient_scores[layer_idx] /= min(len(data_loader), 10)
            
        logger.info(f"æ¢¯åº¦åˆ†æå®Œæˆï¼Œå…±åˆ†æ{len(layer_gradient_scores)}å±‚")
        return dict(layer_gradient_scores)

class LayerAblationAnalyzer(ImportanceAnalyzer):
    """å±‚æ¶ˆèåˆ†æå™¨"""
    
    def __init__(self):
        super().__init__("layer_ablation")
        
    def analyze(self, model: UniversalModel, data_loader) -> Dict[int, float]:
        """é€šè¿‡å±‚æ¶ˆèè®¡ç®—å±‚é‡è¦æ€§"""
        logger.info("å¼€å§‹å±‚æ¶ˆèåˆ†æ...")
        
        # è·å–åŸºçº¿æ€§èƒ½
        baseline_acc = self._evaluate_model(model, data_loader)
        logger.info(f"åŸºçº¿å‡†ç¡®ç‡: {baseline_acc:.4f}")
        
        layer_importance_scores = {}
        
        for layer in model.layers:
            # ä¸´æ—¶ç¦ç”¨è¯¥å±‚
            original_forward = layer.module.forward
            layer.module.forward = lambda x: x  # è·³è¿‡è¯¥å±‚
            
            # è¯„ä¼°æ€§èƒ½ä¸‹é™
            ablated_acc = self._evaluate_model(model, data_loader)
            importance = baseline_acc - ablated_acc
            layer_importance_scores[layer.layer_idx] = max(0, importance)
            
            # æ¢å¤åŸå§‹forwardå‡½æ•°
            layer.module.forward = original_forward
            
            logger.info(f"Layer {layer.layer_idx}: æ¶ˆèåå‡†ç¡®ç‡={ablated_acc:.4f}, é‡è¦æ€§={importance:.4f}")
            
        logger.info(f"å±‚æ¶ˆèåˆ†æå®Œæˆï¼Œå…±åˆ†æ{len(layer_importance_scores)}å±‚")
        return layer_importance_scores
        
    def _evaluate_model(self, model: UniversalModel, data_loader) -> float:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        model.model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch_idx, (data, targets) in enumerate(data_loader):
                if batch_idx >= 5:  # é™åˆ¶è®¡ç®—é‡
                    break
                    
                data = data.to(model.config.device)
                targets = targets.to(model.config.device)
                
                outputs = model.model(data)
                _, predicted = torch.max(outputs.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
                
        return correct / total if total > 0 else 0.0

class AnalysisMethodRegistry:
    """åˆ†ææ–¹æ³•æ³¨å†Œè¡¨"""
    
    def __init__(self):
        self.methods = {
            'fisher_information': FisherInformationAnalyzer,
            'gradient_based': GradientBasedAnalyzer,
            'layer_ablation': LayerAblationAnalyzer,
        }
        
    def get_analyzer(self, method_name: str) -> ImportanceAnalyzer:
        """è·å–åˆ†æå™¨å®ä¾‹"""
        if method_name not in self.methods:
            raise ValueError(f"Unknown analysis method: {method_name}")
        return self.methods[method_name]()
        
    def add_method(self, name: str, analyzer_class):
        """æ·»åŠ æ–°çš„åˆ†ææ–¹æ³•"""
        self.methods[name] = analyzer_class

class UniversalLayerwiseAdapter:
    """é€šç”¨å±‚çº§é€‚é…å™¨ä¸»ç±»"""
    
    def __init__(self, config: AnalysisConfig):
        self.config = config
        self.registry = AnalysisMethodRegistry()
        self.model: Optional[UniversalModel] = None
        self.analysis_results: Dict[str, Dict[int, float]] = {}
        
    def load_model(self, model: nn.Module) -> UniversalModel:
        """åŠ è½½å¹¶é€‚é…æ¨¡å‹"""
        # è¿™é‡Œéœ€è¦æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„é€‚é…å™¨
        # æš‚æ—¶è¿”å›ä¸€ä¸ªé€šç”¨å®ç°
        logger.info(f"åŠ è½½æ¨¡å‹: {self.config.model_name}")
        
        # TODO: å®ç°è‡ªåŠ¨æ¨¡å‹ç±»å‹æ£€æµ‹å’Œé€‚é…å™¨é€‰æ‹©
        if self.config.modality_type == ModalityType.TEXT:
            self.model = TextModelAdapter(model, self.config)
        elif self.config.modality_type == ModalityType.VISION:
            self.model = VisionModelAdapter(model, self.config)
        else:
            raise NotImplementedError(f"Modality {self.config.modality_type} not yet supported")
            
        logger.info(f"æ¨¡å‹é€‚é…å®Œæˆï¼Œå…±æ£€æµ‹åˆ°{len(self.model.layers)}å±‚")
        return self.model
        
    def analyze_importance(self, data_loader) -> Dict[str, Dict[int, float]]:
        """æ‰§è¡Œå±‚é‡è¦æ€§åˆ†æ"""
        if self.model is None:
            raise ValueError("Model not loaded. Call load_model() first.")
            
        logger.info(f"å¼€å§‹é‡è¦æ€§åˆ†æï¼Œä½¿ç”¨æ–¹æ³•: {self.config.analysis_methods}")
        
        for method_name in self.config.analysis_methods:
            analyzer = self.registry.get_analyzer(method_name)
            scores = analyzer.analyze(self.model, data_loader)
            normalized_scores = analyzer.normalize_scores(scores)
            self.analysis_results[method_name] = normalized_scores
            
        logger.info("æ‰€æœ‰åˆ†ææ–¹æ³•æ‰§è¡Œå®Œæˆ")
        return self.analysis_results
        
    def compute_consensus_ranking(self) -> Dict[int, float]:
        """è®¡ç®—å¤šæ–¹æ³•ä¸€è‡´æ€§æ’å"""
        if not self.analysis_results:
            raise ValueError("No analysis results available. Call analyze_importance() first.")
            
        # è®¡ç®—æ‰€æœ‰æ–¹æ³•çš„å¹³å‡é‡è¦æ€§å¾—åˆ†
        consensus_scores = defaultdict(float)
        method_count = len(self.analysis_results)
        
        for method_results in self.analysis_results.values():
            for layer_idx, score in method_results.items():
                consensus_scores[layer_idx] += score / method_count
                
        logger.info(f"ä¸€è‡´æ€§æ’åè®¡ç®—å®Œæˆï¼ŒåŸºäº{method_count}ç§æ–¹æ³•")
        return dict(consensus_scores)
        
    def generate_compression_plan(self, target_ratio: float) -> Dict[str, Any]:
        """ç”Ÿæˆå‹ç¼©æ–¹æ¡ˆ"""
        consensus_scores = self.compute_consensus_ranking()
        
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_layers = sorted(consensus_scores.items(), key=lambda x: x[1], reverse=True)
        
        # è®¡ç®—éœ€è¦ä¿ç•™çš„å±‚æ•°
        total_layers = len(sorted_layers)
        keep_layers = int(total_layers / target_ratio)
        
        compression_plan = {
            'target_ratio': target_ratio,
            'original_layers': total_layers,
            'compressed_layers': keep_layers,
            'removed_layers': total_layers - keep_layers,
            'keep_layer_indices': [idx for idx, _ in sorted_layers[:keep_layers]],
            'remove_layer_indices': [idx for idx, _ in sorted_layers[keep_layers:]],
            'layer_importance_ranking': sorted_layers
        }
        
        logger.info(f"å‹ç¼©æ–¹æ¡ˆç”Ÿæˆå®Œæˆ: {total_layers}å±‚ â†’ {keep_layers}å±‚ (å‹ç¼©æ¯”{target_ratio}x)")
        return compression_plan

# æ¨¡æ€ç‰¹å®šé€‚é…å™¨å®ç°
class TextModelAdapter(UniversalModel):
    """æ–‡æœ¬æ¨¡å‹é€‚é…å™¨"""
    
    def _initialize_layers(self):
        """åˆå§‹åŒ–æ–‡æœ¬æ¨¡å‹çš„å±‚"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„æ–‡æœ¬æ¨¡å‹æ¶æ„æ¥å®ç°
        # æš‚æ—¶æä¾›ä¸€ä¸ªé€šç”¨å®ç°
        layer_idx = 0
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Linear, nn.LSTM, nn.GRU, nn.TransformerEncoderLayer)):
                self.layers.append(Layer(module, layer_idx, name))
                layer_idx += 1
                
    def get_layer_output(self, x: torch.Tensor, layer_idx: int) -> torch.Tensor:
        """è·å–æŒ‡å®šå±‚çš„è¾“å‡º"""
        # TODO: å®ç°è·å–ç‰¹å®šå±‚è¾“å‡ºçš„é€»è¾‘
        raise NotImplementedError("TextModelAdapter.get_layer_output not implemented")

class VisionModelAdapter(UniversalModel):
    """è§†è§‰æ¨¡å‹é€‚é…å™¨"""
    
    def _initialize_layers(self):
        """åˆå§‹åŒ–è§†è§‰æ¨¡å‹çš„å±‚"""
        layer_idx = 0
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d)):
                self.layers.append(Layer(module, layer_idx, name))
                layer_idx += 1
                
    def get_layer_output(self, x: torch.Tensor, layer_idx: int) -> torch.Tensor:
        """è·å–æŒ‡å®šå±‚çš„è¾“å‡º"""
        # TODO: å®ç°è·å–ç‰¹å®šå±‚è¾“å‡ºçš„é€»è¾‘
        raise NotImplementedError("VisionModelAdapter.get_layer_output not implemented")

# ä¾¿åˆ©å‡½æ•°
def create_analyzer(model_name: str, task_type: str, modality_type: str, **kwargs) -> UniversalLayerwiseAdapter:
    """åˆ›å»ºåˆ†æå™¨çš„ä¾¿åˆ©å‡½æ•°"""
    config = AnalysisConfig(
        model_name=model_name,
        task_type=TaskType(task_type),
        modality_type=ModalityType(modality_type),
        **kwargs
    )
    return UniversalLayerwiseAdapter(config)

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    from collections import defaultdict
    
    # åˆ›å»ºé…ç½®
    config = AnalysisConfig(
        model_name="bert-base-uncased",
        task_type=TaskType.CLASSIFICATION,
        modality_type=ModalityType.TEXT,
        analysis_methods=['fisher_information', 'gradient_based']
    )
    
    # åˆ›å»ºåˆ†æå™¨
    adapter = UniversalLayerwiseAdapter(config)
    
    print("ğŸš€ é€šç”¨Layerwise-Adapteræ¡†æ¶åˆå§‹åŒ–å®Œæˆ!")
    print(f"ğŸ“Š æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {[t.value for t in TaskType]}")
    print(f"ğŸ”§ æ”¯æŒçš„æ¨¡æ€ç±»å‹: {[m.value for m in ModalityType]}")
    print(f"âš¡ æ”¯æŒçš„åˆ†ææ–¹æ³•: {list(adapter.registry.methods.keys())}")
