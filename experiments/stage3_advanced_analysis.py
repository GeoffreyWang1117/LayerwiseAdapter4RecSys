#!/usr/bin/env python3
"""
é˜¶æ®µ3ï¼šé«˜çº§å±‚é‡è¦æ€§åˆ†æ
å®ç°SHAPã€äº’ä¿¡æ¯ã€Layer Conductanceã€PIIã€Dropoutä¸ç¡®å®šæ€§ã€æ¿€æ´»ä¿®è¡¥ç­‰é«˜çº§æ–¹æ³•
åŸºäºé˜¶æ®µ1-2çš„ç¨³å®šåŸºç¡€ï¼Œä½¿ç”¨çœŸå®Amazonæ•°æ®
"""

import os
import json
import logging
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, mutual_info_score
from sklearn.feature_selection import mutual_info_classif
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import pickle
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥SHAP (å¦‚æœå¯ç”¨)
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    logging.warning("SHAPæœªå®‰è£…ï¼Œå°†è·³è¿‡SHAPåˆ†æ")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s'
)
logger = logging.getLogger(__name__)

class HonestDataLoader:
    """è¯šå®çš„æ•°æ®åŠ è½½å™¨ - ä¸é˜¶æ®µ1-2å…¼å®¹"""
    def __init__(self, data_path='dataset/amazon/Electronics_reviews.parquet'):
        self.data_path = data_path
        
    def load_real_data(self):
        """åŠ è½½çœŸå®Amazonæ•°æ®"""
        logger.info("ğŸ“Š åŠ è½½çœŸå®Amazon Electronicsæ•°æ®...")
        df = pd.read_parquet(self.data_path)
        logger.info(f"åŸå§‹æ•°æ®: {len(df):,} æ¡è®°å½•")
        
        # éªŒè¯æ•°æ®çœŸå®æ€§
        self._validate_data(df)
        
        # è´¨é‡è¿‡æ»¤
        df = self._filter_quality_data(df)
        
        return df
    
    def _validate_data(self, df):
        """éªŒè¯æ•°æ®çœŸå®æ€§"""
        logger.info("ğŸ” éªŒè¯Amazon Electronicsæ•°æ®çœŸå®æ€§...")
        logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        logger.info(f"åˆ—å: {df.columns.tolist()}")
        
        # æ–‡æœ¬å¤šæ ·æ€§æ£€æŸ¥
        if 'text' in df.columns:
            unique_texts = df['text'].nunique()
            total_texts = len(df)
            diversity = unique_texts / total_texts
            logger.info(f"æ–‡æœ¬å”¯ä¸€æ€§: {unique_texts:,}/{total_texts:,} = {diversity:.3f}")
            
            if diversity < 0.7:
                logger.warning(f"âš ï¸ æ–‡æœ¬å¤šæ ·æ€§è¾ƒä½: {diversity:.3f}")
            else:
                logger.info("âœ… æ–‡æœ¬å¤šæ ·æ€§éªŒè¯é€šè¿‡")
        
        logger.info("âœ… æ•°æ®çœŸå®æ€§éªŒè¯å®Œæˆ")
    
    def _filter_quality_data(self, df):
        """è¿‡æ»¤é«˜è´¨é‡æ•°æ®"""
        initial_count = len(df)
        
        # åŸºæœ¬è¿‡æ»¤
        df = df.dropna(subset=['text', 'rating'])
        df = df[df['text'].str.len() > 10]  # è‡³å°‘10ä¸ªå­—ç¬¦
        df = df[df['rating'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]  # æœ‰æ•ˆè¯„åˆ†
        
        final_count = len(df)
        retention_rate = final_count / initial_count
        logger.info(f"è´¨é‡è¿‡æ»¤: {initial_count:,} -> {final_count:,} ({retention_rate:.1%}ä¿ç•™)")
        
        return df

class StableTransformer(nn.Module):
    """ä¸é˜¶æ®µ1-2å…¼å®¹çš„ç¨³å®šTransformeræ¨¡å‹"""
    def __init__(self, vocab_size, embed_dim=512, num_heads=8, num_layers=12, 
                 hidden_dim=2048, max_seq_len=256, num_classes=2, dropout=0.1):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.num_layers = num_layers
        self.max_seq_len = max_seq_len
        
        # åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.pos_embedding = nn.Parameter(torch.randn(max_seq_len, embed_dim))
        
        # Transformerå±‚
        self.layers = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads, hidden_dim, dropout)
            for _ in range(num_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.layer_norm = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(embed_dim, num_classes)
        
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.02)
    
    def forward(self, input_ids, attention_mask=None, return_layer_outputs=False):
        """å‰å‘ä¼ æ’­"""
        batch_size, seq_len = input_ids.shape
        
        # åµŒå…¥
        x = self.embedding(input_ids)
        x = x + self.pos_embedding[:seq_len]
        x = self.dropout(x)
        
        # æ³¨æ„åŠ›æ©ç 
        if attention_mask is None:
            attention_mask = (input_ids != 0).float()
        
        # å±‚è¾“å‡ºå­˜å‚¨
        layer_outputs = []
        
        # Transformerå±‚
        for i, layer in enumerate(self.layers):
            x = layer(x, attention_mask)
            if return_layer_outputs:
                layer_outputs.append(x.clone())
        
        # æœ€ç»ˆå¤„ç†
        x = self.layer_norm(x)
        
        # æ± åŒ– (ä½¿ç”¨[CLS]ä½ç½®æˆ–å¹³å‡æ± åŒ–)
        if attention_mask is not None:
            mask_expanded = attention_mask.unsqueeze(-1).expand_as(x)
            x = (x * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1)
        else:
            x = x.mean(dim=1)
        
        # åˆ†ç±»
        logits = self.classifier(self.dropout(x))
        
        if return_layer_outputs:
            return logits, layer_outputs
        return logits

class TransformerBlock(nn.Module):
    """Transformerå—"""
    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0.1):
        super().__init__()
        
        self.attention = nn.MultiheadAttention(
            embed_dim, num_heads, dropout=dropout, batch_first=True
        )
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, embed_dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, x, attention_mask=None):
        # Self-attention
        if attention_mask is not None:
            # è½¬æ¢æ©ç æ ¼å¼
            attn_mask = (attention_mask == 0)
        else:
            attn_mask = None
            
        attn_out, _ = self.attention(x, x, x, key_padding_mask=attn_mask)
        x = self.norm1(x + attn_out)
        
        # Feed forward
        ff_out = self.feed_forward(x)
        x = self.norm2(x + ff_out)
        
        return x

class AdvancedImportanceAnalyzer:
    """é«˜çº§é‡è¦æ€§åˆ†æå™¨ - SHAPã€äº’ä¿¡æ¯ã€Layer Conductanceç­‰é«˜çº§æ–¹æ³•"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.model.to(device)
        
        self.importance_scores = {}
        self.analysis_results = {}
        
    def analyze_advanced_importance(self, dataloader, methods=['mutual_info', 'layer_conductance', 'pii', 'dropout_uncertainty', 'activation_patching']):
        """åˆ†æé«˜çº§å±‚é‡è¦æ€§"""
        logger.info("ğŸ”¬ å¼€å§‹é«˜çº§å±‚é‡è¦æ€§åˆ†æ...")
        
        results = {}
        
        if 'shap' in methods and SHAP_AVAILABLE:
            logger.info("ğŸ¯ è®¡ç®—SHAPå€¼...")
            results['shap'] = self._compute_shap_importance(dataloader)
            
        if 'mutual_info' in methods:
            logger.info("ğŸ“Š è®¡ç®—äº’ä¿¡æ¯é‡è¦æ€§...")
            results['mutual_info'] = self._compute_mutual_info_importance(dataloader)
            
        if 'layer_conductance' in methods:
            logger.info("âš¡ è®¡ç®—Layer Conductance...")
            results['layer_conductance'] = self._compute_layer_conductance(dataloader)
            
        if 'pii' in methods:
            logger.info("ğŸ¯ è®¡ç®—å‚æ•°å½±å“æŒ‡æ•°(PII)...")
            results['pii'] = self._compute_parameter_influence_index(dataloader)
            
        if 'dropout_uncertainty' in methods:
            logger.info("ğŸ² è®¡ç®—Dropoutä¸ç¡®å®šæ€§...")
            results['dropout_uncertainty'] = self._compute_dropout_uncertainty(dataloader)
            
        if 'activation_patching' in methods:
            logger.info("ğŸ”§ æ‰§è¡Œæ¿€æ´»ä¿®è¡¥åˆ†æ...")
            results['activation_patching'] = self._compute_activation_patching(dataloader)
            
        self.analysis_results = results
        return results
    
    def _compute_shap_importance(self, dataloader):
        """è®¡ç®—SHAPé‡è¦æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        if not SHAP_AVAILABLE:
            logger.warning("âš ï¸ SHAPä¸å¯ç”¨ï¼Œè·³è¿‡SHAPåˆ†æ")
            return {}
        
        # ç®€åŒ–ç‰ˆSHAPåˆ†æ - ä½¿ç”¨å±‚çº§åˆ«çš„SHAP
        logger.info("ğŸ’¡ ä½¿ç”¨ç®€åŒ–SHAPæ–¹æ³•è¿›è¡Œå±‚é‡è¦æ€§åˆ†æ...")
        
        # è·å–ä¸€å°æ‰¹æ•°æ®è¿›è¡ŒSHAPåˆ†æ
        sample_data, sample_targets = next(iter(dataloader))
        sample_data = sample_data[:16].to(self.device)  # åªå–16ä¸ªæ ·æœ¬
        sample_targets = sample_targets[:16].to(self.device)
        
        # å®šä¹‰å±‚çº§é¢„æµ‹å‡½æ•°
        def layer_prediction_function(layer_masks):
            """åŸºäºå±‚æ©ç çš„é¢„æµ‹å‡½æ•°"""
            batch_predictions = []
            
            for mask in layer_masks:
                # ä¿®æ”¹æ¨¡å‹çš„å±‚æ©ç 
                original_layers = []
                for i, layer in enumerate(self.model.layers):
                    original_layers.append(layer.training)
                    if mask[i] == 0:
                        layer.eval()  # ç¦ç”¨å±‚
                    else:
                        layer.train()  # å¯ç”¨å±‚
                
                # é¢„æµ‹
                with torch.no_grad():
                    outputs = self.model(sample_data)
                    probs = F.softmax(outputs, dim=1)
                    batch_predictions.append(probs.cpu().numpy())
                
                # æ¢å¤å±‚çŠ¶æ€
                for i, layer in enumerate(self.model.layers):
                    layer.train(original_layers[i])
            
            return np.array(batch_predictions)
        
        # åˆ›å»ºå±‚æ©ç åŸºçº¿
        baseline_mask = np.ones((1, self.model.num_layers))
        
        # ç®€åŒ–çš„SHAPå€¼è®¡ç®—
        shap_values = {}
        for layer_idx in range(self.model.num_layers):
            # åˆ›å»ºç§»é™¤è¯¥å±‚çš„æ©ç 
            test_mask = baseline_mask.copy()
            test_mask[0, layer_idx] = 0
            
            # è®¡ç®—åŸºçº¿å’Œæµ‹è¯•é¢„æµ‹
            baseline_pred = layer_prediction_function(baseline_mask)[0]
            test_pred = layer_prediction_function(test_mask)[0]
            
            # SHAPå€¼æ˜¯é¢„æµ‹å·®å¼‚
            shap_value = np.mean(np.abs(baseline_pred - test_pred))
            shap_values[f'layer_{layer_idx}'] = shap_value
        
        logger.info("âœ… ç®€åŒ–SHAPåˆ†æå®Œæˆ")
        return shap_values
    
    def _compute_mutual_info_importance(self, dataloader):
        """è®¡ç®—äº’ä¿¡æ¯é‡è¦æ€§"""
        self.model.eval()
        layer_activations = {f'layer_{i}': [] for i in range(self.model.num_layers)}
        targets_list = []
        
        # æ”¶é›†å±‚æ¿€æ´»
        with torch.no_grad():
            for batch_idx, (data, targets) in enumerate(tqdm(dataloader, desc="æ”¶é›†å±‚æ¿€æ´»")):
                if batch_idx >= 20:  # é™åˆ¶æ•°æ®é‡
                    break
                    
                data, targets = data.to(self.device), targets.to(self.device)
                
                # è·å–å±‚è¾“å‡º
                _, layer_outputs = self.model(data, return_layer_outputs=True)
                
                # å­˜å‚¨æ¿€æ´»ç»Ÿè®¡
                for layer_idx, layer_output in enumerate(layer_outputs):
                    # è®¡ç®—å±‚æ¿€æ´»çš„ç»Ÿè®¡ç‰¹å¾
                    activation_stats = torch.cat([
                        layer_output.mean(dim=(1, 2)),  # å¹³å‡æ¿€æ´»
                        layer_output.std(dim=(1, 2)),   # æ¿€æ´»æ ‡å‡†å·®
                        layer_output.max(dim=2)[0].max(dim=1)[0],  # æœ€å¤§æ¿€æ´»
                    ], dim=0)
                    
                    layer_activations[f'layer_{layer_idx}'].append(activation_stats.cpu().numpy())
                
                targets_list.extend(targets.cpu().numpy())
        
        # è®¡ç®—äº’ä¿¡æ¯
        mutual_info_scores = {}
        for layer_name, activations in layer_activations.items():
            if activations:
                # å°†æ¿€æ´»å±•å¹³
                activation_matrix = np.vstack(activations)
                
                # è®¡ç®—æ¯ä¸ªç‰¹å¾ç»´åº¦ä¸ç›®æ ‡çš„äº’ä¿¡æ¯
                try:
                    mi_scores = mutual_info_classif(activation_matrix, targets_list[:len(activation_matrix)])
                    mutual_info_scores[layer_name] = np.mean(mi_scores)
                except Exception as e:
                    logger.warning(f"äº’ä¿¡æ¯è®¡ç®—å¤±è´¥ {layer_name}: {e}")
                    mutual_info_scores[layer_name] = 0.0
        
        logger.info("âœ… äº’ä¿¡æ¯åˆ†æå®Œæˆ")
        return mutual_info_scores
    
    def _compute_layer_conductance(self, dataloader):
        """è®¡ç®—Layer Conductance"""
        self.model.train()
        conductance_scores = {f'layer_{i}': [] for i in range(self.model.num_layers)}
        
        for batch_idx, (data, targets) in enumerate(tqdm(dataloader, desc="Layer Conductanceè®¡ç®—")):
            if batch_idx >= 30:  # é™åˆ¶è®¡ç®—é‡
                break
                
            data, targets = data.to(self.device), targets.to(self.device)
            
            # æ³¨å†Œé’©å­æ”¶é›†æ¢¯åº¦
            layer_gradients = {}
            hooks = []
            
            def create_hook(layer_name):
                def hook_fn(module, grad_input, grad_output):
                    if grad_output[0] is not None:
                        layer_gradients[layer_name] = grad_output[0].clone()
                return hook_fn
            
            # ä¸ºæ¯å±‚æ³¨å†Œé’©å­
            for i, layer in enumerate(self.model.layers):
                hook = layer.register_backward_hook(create_hook(f'layer_{i}'))
                hooks.append(hook)
            
            # å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
            self.model.zero_grad()
            outputs = self.model(data)
            loss = F.cross_entropy(outputs, targets)
            loss.backward()
            
            # è®¡ç®—conductance (æ¿€æ´»å€¼ Ã— æ¢¯åº¦)
            with torch.no_grad():
                _, layer_outputs = self.model(data, return_layer_outputs=True)
                
                for i, layer_output in enumerate(layer_outputs):
                    layer_name = f'layer_{i}'
                    if layer_name in layer_gradients:
                        # Conductance = |activation Ã— gradient|
                        conductance = torch.abs(layer_output * layer_gradients[layer_name])
                        conductance_score = conductance.mean().item()
                        conductance_scores[layer_name].append(conductance_score)
            
            # æ¸…ç†é’©å­
            for hook in hooks:
                hook.remove()
        
        # è®¡ç®—å¹³å‡conductance
        layer_conductance = {}
        for layer_name, scores in conductance_scores.items():
            if scores:
                layer_conductance[layer_name] = np.mean(scores)
            else:
                layer_conductance[layer_name] = 0.0
        
        logger.info("âœ… Layer Conductanceè®¡ç®—å®Œæˆ")
        return layer_conductance
    
    def _compute_parameter_influence_index(self, dataloader):
        """è®¡ç®—å‚æ•°å½±å“æŒ‡æ•°(PII)"""
        self.model.train()
        pii_scores = {f'layer_{i}': [] for i in range(self.model.num_layers)}
        
        for batch_idx, (data, targets) in enumerate(tqdm(dataloader, desc="PIIè®¡ç®—")):
            if batch_idx >= 25:  # é™åˆ¶è®¡ç®—é‡
                break
                
            data, targets = data.to(self.device), targets.to(self.device)
            
            # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
            self.model.zero_grad()
            outputs = self.model(data)
            loss = F.cross_entropy(outputs, targets)
            loss.backward()
            
            # è®¡ç®—æ¯å±‚çš„PII
            for layer_idx in range(self.model.num_layers):
                layer_pii = 0
                param_count = 0
                
                for name, param in self.model.named_parameters():
                    if f'layers.{layer_idx}' in name and param.grad is not None:
                        # PII = |param Ã— gradient|
                        param_influence = torch.abs(param * param.grad)
                        layer_pii += param_influence.sum().item()
                        param_count += param.numel()
                
                if param_count > 0:
                    pii_scores[f'layer_{layer_idx}'].append(layer_pii / param_count)
        
        # è®¡ç®—å¹³å‡PII
        layer_pii = {}
        for layer_name, scores in pii_scores.items():
            if scores:
                layer_pii[layer_name] = np.mean(scores)
            else:
                layer_pii[layer_name] = 0.0
        
        logger.info("âœ… PIIè®¡ç®—å®Œæˆ")
        return layer_pii
    
    def _compute_dropout_uncertainty(self, dataloader):
        """è®¡ç®—Dropoutä¸ç¡®å®šæ€§"""
        self.model.train()  # ä¿æŒdropoutå¼€å¯
        uncertainty_scores = {f'layer_{i}': [] for i in range(self.model.num_layers)}
        
        # å®šä¹‰ä¸åŒdropoutç‡
        dropout_rates = [0.1, 0.3, 0.5]
        
        for batch_idx, (data, targets) in enumerate(tqdm(dataloader, desc="Dropoutä¸ç¡®å®šæ€§è®¡ç®—")):
            if batch_idx >= 15:  # é™åˆ¶è®¡ç®—é‡
                break
                
            data, targets = data.to(self.device), targets.to(self.device)
            
            # å¯¹æ¯å±‚æµ‹è¯•ä¸åŒdropoutç‡çš„å½±å“
            for layer_idx in range(self.model.num_layers):
                layer_uncertainties = []
                
                # ä¿å­˜åŸå§‹dropoutç‡
                original_dropout = self.model.dropout.p
                
                for dropout_rate in dropout_rates:
                    # è®¾ç½®æ–°çš„dropoutç‡
                    self.model.dropout.p = dropout_rate
                    
                    # å¤šæ¬¡å‰å‘ä¼ æ’­
                    predictions = []
                    for _ in range(5):  # 5æ¬¡é‡‡æ ·
                        with torch.no_grad():
                            outputs = self.model(data)
                            probs = F.softmax(outputs, dim=1)
                            predictions.append(probs.cpu().numpy())
                    
                    # è®¡ç®—é¢„æµ‹ä¸ç¡®å®šæ€§ (æ ‡å‡†å·®)
                    predictions = np.stack(predictions)
                    uncertainty = np.mean(np.std(predictions, axis=0))
                    layer_uncertainties.append(uncertainty)
                
                # æ¢å¤åŸå§‹dropoutç‡
                self.model.dropout.p = original_dropout
                
                # å±‚ä¸ç¡®å®šæ€§æ˜¯ä¸åŒdropoutç‡çš„å¹³å‡ä¸ç¡®å®šæ€§
                uncertainty_scores[f'layer_{layer_idx}'].append(np.mean(layer_uncertainties))
        
        # è®¡ç®—å¹³å‡ä¸ç¡®å®šæ€§
        layer_uncertainty = {}
        for layer_name, scores in uncertainty_scores.items():
            if scores:
                layer_uncertainty[layer_name] = np.mean(scores)
            else:
                layer_uncertainty[layer_name] = 0.0
        
        logger.info("âœ… Dropoutä¸ç¡®å®šæ€§è®¡ç®—å®Œæˆ")
        return layer_uncertainty
    
    def _compute_activation_patching(self, dataloader):
        """è®¡ç®—æ¿€æ´»ä¿®è¡¥é‡è¦æ€§"""
        self.model.eval()
        patching_scores = {f'layer_{i}': [] for i in range(self.model.num_layers)}
        
        # è·å–åŸºå‡†é¢„æµ‹
        sample_data, sample_targets = next(iter(dataloader))
        sample_data = sample_data[:32].to(self.device)  # é™åˆ¶æ ·æœ¬æ•°
        sample_targets = sample_targets[:32].to(self.device)
        
        with torch.no_grad():
            baseline_outputs = self.model(sample_data)
            baseline_probs = F.softmax(baseline_outputs, dim=1)
        
        # å¯¹æ¯å±‚è¿›è¡Œæ¿€æ´»ä¿®è¡¥
        for layer_idx in range(self.model.num_layers):
            logger.info(f"æ¿€æ´»ä¿®è¡¥ç¬¬{layer_idx}å±‚...")
            
            # æ³¨å†Œé’©å­è¿›è¡Œæ¿€æ´»ä¿®è¡¥
            def create_patching_hook(noise_scale=0.1):
                def hook_fn(module, input, output):
                    # æ·»åŠ å™ªå£°åˆ°æ¿€æ´»
                    noise = torch.randn_like(output) * noise_scale
                    return output + noise
                return hook_fn
            
            # æ³¨å†Œé’©å­
            hook = self.model.layers[layer_idx].register_forward_hook(create_patching_hook())
            
            # ä¿®è¡¥åçš„é¢„æµ‹
            with torch.no_grad():
                patched_outputs = self.model(sample_data)
                patched_probs = F.softmax(patched_outputs, dim=1)
            
            # è®¡ç®—é¢„æµ‹å·®å¼‚
            prob_diff = torch.abs(baseline_probs - patched_probs).mean().item()
            patching_scores[f'layer_{layer_idx}'].append(prob_diff)
            
            # ç§»é™¤é’©å­
            hook.remove()
        
        # æ•´ç†ç»“æœ
        layer_patching = {}
        for layer_name, scores in patching_scores.items():
            if scores:
                layer_patching[layer_name] = np.mean(scores)
            else:
                layer_patching[layer_name] = 0.0
        
        logger.info("âœ… æ¿€æ´»ä¿®è¡¥åˆ†æå®Œæˆ")
        return layer_patching
    
    def combine_importance_scores(self, core_results, advanced_results):
        """ç»¼åˆæ ¸å¿ƒå’Œé«˜çº§é‡è¦æ€§åˆ†æ•°"""
        logger.info("ğŸ”— ç»¼åˆé‡è¦æ€§åˆ†æç»“æœ...")
        
        # æ”¶é›†æ‰€æœ‰æ–¹æ³•çš„åˆ†æ•°
        all_methods = {}
        all_methods.update(core_results)
        all_methods.update(advanced_results)
        
        # æ ‡å‡†åŒ–åˆ†æ•°
        normalized_scores = {}
        for method, scores in all_methods.items():
            if scores:
                score_values = list(scores.values())
                if len(score_values) > 0 and max(score_values) > 0:
                    normalized_scores[method] = {
                        layer: score / max(score_values) 
                        for layer, score in scores.items()
                    }
                else:
                    normalized_scores[method] = scores
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        layer_names = [f'layer_{i}' for i in range(self.model.num_layers)]
        comprehensive_scores = {}
        
        for layer_name in layer_names:
            layer_score = 0
            method_count = 0
            
            for method, scores in normalized_scores.items():
                if layer_name in scores:
                    layer_score += scores[layer_name]
                    method_count += 1
            
            if method_count > 0:
                comprehensive_scores[layer_name] = layer_score / method_count
            else:
                comprehensive_scores[layer_name] = 0.0
        
        logger.info("âœ… ç»¼åˆé‡è¦æ€§åˆ†æå®Œæˆ")
        return comprehensive_scores, normalized_scores
    
    def select_optimal_layers(self, comprehensive_scores, target_compression=2.0):
        """é€‰æ‹©æœ€ä¼˜å±‚ç»„åˆ"""
        logger.info(f"ğŸ¯ é€‰æ‹©æœ€ä¼˜å±‚ç»„åˆï¼Œç›®æ ‡å‹ç¼©æ¯”: {target_compression:.1f}x")
        
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_layers = sorted(comprehensive_scores.items(), 
                             key=lambda x: x[1], reverse=True)
        
        # è®¡ç®—ç›®æ ‡å±‚æ•°
        original_layers = self.model.num_layers
        target_layers = max(1, int(original_layers / target_compression))
        
        # é€‰æ‹©topå±‚
        selected_layers = [layer for layer, score in sorted_layers[:target_layers]]
        selected_scores = {layer: score for layer, score in sorted_layers[:target_layers]}
        
        logger.info(f"âœ… é€‰æ‹©{len(selected_layers)}å±‚è¿›è¡Œ{target_compression:.1f}xå‹ç¼©:")
        for layer, score in sorted_layers[:target_layers]:
            logger.info(f"  {layer}: {score:.6f}")
        
        return selected_layers, selected_scores

def load_stage2_results():
    """åŠ è½½é˜¶æ®µ2çš„ç»“æœ"""
    results_path = 'results/stage2_importance_analysis.json'
    if os.path.exists(results_path):
        with open(results_path, 'r') as f:
            return json.load(f)
    return None

def prepare_data_from_previous_stages():
    """åŸºäºå‰é¢é˜¶æ®µå‡†å¤‡æ•°æ®"""
    # é‡æ–°åŠ è½½å’Œå¤„ç†æ•°æ®ä»¥ä¿æŒä¸€è‡´æ€§
    loader = HonestDataLoader()
    df = loader.load_real_data()
    
    # åˆ›å»ºæ­£è´Ÿä¾‹
    positive_samples = df[df['rating'] >= 4].sample(n=min(15000, len(df[df['rating'] >= 4])))
    negative_samples = df[df['rating'] <= 2].sample(n=min(15000, len(df[df['rating'] <= 2])))
    
    # åˆå¹¶æ•°æ®
    final_df = pd.concat([positive_samples, negative_samples]).sample(frac=1).reset_index(drop=True)
    final_df['label'] = (final_df['rating'] >= 4).astype(int)
    
    # ç®€å•tokenization (ä¸å‰é¢é˜¶æ®µä¿æŒä¸€è‡´)
    vocab = set()
    for text in final_df['text']:
        vocab.update(text.lower().split())
    
    vocab = ['<PAD>', '<UNK>'] + sorted(list(vocab))[:10000]
    word_to_idx = {word: idx for idx, word in enumerate(vocab)}
    
    def tokenize_text(text, max_len=256):
        tokens = text.lower().split()[:max_len]
        indices = [word_to_idx.get(token, 1) for token in tokens]  # 1 for <UNK>
        indices.extend([0] * (max_len - len(indices)))  # 0 for <PAD>
        return indices[:max_len]
    
    # Tokenization
    tokenized_texts = [tokenize_text(text) for text in final_df['text']]
    
    # è½¬æ¢ä¸ºtensor
    X = torch.tensor(tokenized_texts, dtype=torch.long)
    y = torch.tensor(final_df['label'].values, dtype=torch.long)
    
    # æ•°æ®åˆ†å‰²
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    test_dataset = TensorDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)
    test_loader = DataLoader(test_dataset, batch_size=32)
    
    return train_loader, val_loader, test_loader, len(vocab)

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹é˜¶æ®µ3ï¼šé«˜çº§å±‚é‡è¦æ€§åˆ†æ")
    
    # æ£€æŸ¥GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½é˜¶æ®µ2ç»“æœ
    stage2_results = load_stage2_results()
    if stage2_results:
        logger.info("âœ… é˜¶æ®µ2ç»“æœå·²åŠ è½½")
    
    # å‡†å¤‡æ•°æ®
    logger.info("ğŸ“Š å‡†å¤‡æ•°æ®...")
    train_loader, val_loader, test_loader, vocab_size = prepare_data_from_previous_stages()
    logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {vocab_size}")
    
    # åˆ›å»ºæ¨¡å‹ (ä¸å‰é¢é˜¶æ®µç›¸åŒçš„æ¶æ„)
    logger.info("ğŸ—ï¸ åˆ›å»ºTransformeræ¨¡å‹...")
    model = StableTransformer(
        vocab_size=vocab_size,
        embed_dim=512,
        num_heads=8,
        num_layers=12,
        hidden_dim=2048,
        max_seq_len=256,
        num_classes=2,
        dropout=0.1
    )
    model.to(device)
    logger.info("âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # åˆ›å»ºé«˜çº§é‡è¦æ€§åˆ†æå™¨
    logger.info("ğŸ”¬ åˆ›å»ºé«˜çº§é‡è¦æ€§åˆ†æå™¨...")
    analyzer = AdvancedImportanceAnalyzer(model, device)
    
    # æ‰§è¡Œé«˜çº§é‡è¦æ€§åˆ†æ
    logger.info("ğŸ¯ æ‰§è¡Œé«˜çº§é‡è¦æ€§åˆ†æ...")
    advanced_results = analyzer.analyze_advanced_importance(
        val_loader, 
        methods=['mutual_info', 'layer_conductance', 'pii', 'dropout_uncertainty', 'activation_patching']
    )
    
    # ç»¼åˆåˆ†æç»“æœ
    if stage2_results and 'importance_analysis' in stage2_results:
        core_results = stage2_results['importance_analysis']
        logger.info("ğŸ”— ç»¼åˆæ ¸å¿ƒå’Œé«˜çº§åˆ†æç»“æœ...")
        comprehensive_scores, normalized_scores = analyzer.combine_importance_scores(
            core_results, advanced_results
        )
    else:
        logger.info("ğŸ“Š ä»…ä½¿ç”¨é«˜çº§åˆ†æç»“æœ...")
        comprehensive_scores = advanced_results.get('mutual_info', {})
        normalized_scores = advanced_results
    
    # é€‰æ‹©æœ€ä¼˜å±‚
    logger.info("ğŸ¯ é€‰æ‹©æœ€ä¼˜å±‚ç»„åˆ...")
    selected_layers_2x, scores_2x = analyzer.select_optimal_layers(
        comprehensive_scores, target_compression=2.0
    )
    selected_layers_3x, scores_3x = analyzer.select_optimal_layers(
        comprehensive_scores, target_compression=3.0
    )
    selected_layers_4x, scores_4x = analyzer.select_optimal_layers(
        comprehensive_scores, target_compression=4.0
    )
    
    # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
    def convert_to_serializable(obj):
        """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºJSONå¯åºåˆ—åŒ–ç±»å‹"""
        if isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(v) for v in obj]
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj
    
    # ä¿å­˜ç»“æœ
    results = {
        'timestamp': datetime.now().isoformat(),
        'device': str(device),
        'model_config': {
            'vocab_size': vocab_size,
            'embed_dim': 512,
            'num_heads': 8,
            'original_layers': 12
        },
        'advanced_analysis': convert_to_serializable(advanced_results),
        'comprehensive_scores': convert_to_serializable(comprehensive_scores),
        'normalized_scores': convert_to_serializable(normalized_scores),
        'optimal_selections': {
            '2x_compression': {
                'selected_layers': selected_layers_2x,
                'scores': convert_to_serializable(scores_2x),
                'target_layers': len(selected_layers_2x)
            },
            '3x_compression': {
                'selected_layers': selected_layers_3x,
                'scores': convert_to_serializable(scores_3x),
                'target_layers': len(selected_layers_3x)
            },
            '4x_compression': {
                'selected_layers': selected_layers_4x,
                'scores': convert_to_serializable(scores_4x),
                'target_layers': len(selected_layers_4x)
            }
        }
    }
    
    # ä¿å­˜ç»“æœ
    os.makedirs('results', exist_ok=True)
    results_path = 'results/stage3_advanced_analysis.json'
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"ğŸ’¾ é˜¶æ®µ3ç»“æœå·²ä¿å­˜: {results_path}")
    
    # å¯è§†åŒ–ç»“æœ
    logger.info("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    create_advanced_visualization(advanced_results, comprehensive_scores, normalized_scores)
    
    logger.info("ğŸ‰ é˜¶æ®µ3å®Œæˆï¼")
    logger.info("ğŸ”œ å‡†å¤‡è¿è¡Œé˜¶æ®µ4: æ¨¡å‹é›†æˆå’Œæœ€ç»ˆè¯„ä¼°")
    
    return results

def create_advanced_visualization(advanced_results, comprehensive_scores, normalized_scores):
    """åˆ›å»ºé«˜çº§åˆ†æå¯è§†åŒ–å›¾è¡¨"""
    plt.style.use('seaborn-v0_8')
    
    # åˆ›å»ºå¤§å›¾è¡¨
    fig = plt.figure(figsize=(20, 16))
    
    # 1. é«˜çº§æ–¹æ³•å¯¹æ¯”çƒ­å›¾
    ax1 = plt.subplot(3, 3, 1)
    methods_data = []
    method_names = []
    
    for method, scores in advanced_results.items():
        if scores:
            method_names.append(method.replace('_', ' ').title())
            layer_scores = [scores.get(f'layer_{i}', 0) for i in range(12)]
            methods_data.append(layer_scores)
    
    if methods_data:
        sns.heatmap(methods_data, xticklabels=[f'L{i}' for i in range(12)], 
                   yticklabels=method_names, annot=True, fmt='.3f', ax=ax1)
        ax1.set_title('Advanced Methods Heatmap', fontweight='bold')
    
    # 2. äº’ä¿¡æ¯é‡è¦æ€§
    if 'mutual_info' in advanced_results:
        ax2 = plt.subplot(3, 3, 2)
        mi_scores = advanced_results['mutual_info']
        layers = list(mi_scores.keys())
        scores = list(mi_scores.values())
        
        bars = ax2.bar(range(len(layers)), scores, alpha=0.7, color='purple')
        ax2.set_title('Mutual Information', fontweight='bold')
        ax2.set_xlabel('Layers')
        ax2.set_ylabel('MI Score')
        ax2.set_xticks(range(len(layers)))
        ax2.set_xticklabels([f'L{i}' for i in range(len(layers))])
    
    # 3. Layer Conductance
    if 'layer_conductance' in advanced_results:
        ax3 = plt.subplot(3, 3, 3)
        lc_scores = advanced_results['layer_conductance']
        layers = list(lc_scores.keys())
        scores = list(lc_scores.values())
        
        ax3.bar(range(len(layers)), scores, alpha=0.7, color='orange')
        ax3.set_title('Layer Conductance', fontweight='bold')
        ax3.set_xlabel('Layers')
        ax3.set_ylabel('Conductance')
        ax3.set_xticks(range(len(layers)))
        ax3.set_xticklabels([f'L{i}' for i in range(len(layers))])
    
    # 4. PIIåˆ†æ•°
    if 'pii' in advanced_results:
        ax4 = plt.subplot(3, 3, 4)
        pii_scores = advanced_results['pii']
        layers = list(pii_scores.keys())
        scores = list(pii_scores.values())
        
        ax4.bar(range(len(layers)), scores, alpha=0.7, color='green')
        ax4.set_title('Parameter Influence Index (PII)', fontweight='bold')
        ax4.set_xlabel('Layers')
        ax4.set_ylabel('PII Score')
        ax4.set_xticks(range(len(layers)))
        ax4.set_xticklabels([f'L{i}' for i in range(len(layers))])
    
    # 5. Dropoutä¸ç¡®å®šæ€§
    if 'dropout_uncertainty' in advanced_results:
        ax5 = plt.subplot(3, 3, 5)
        du_scores = advanced_results['dropout_uncertainty']
        layers = list(du_scores.keys())
        scores = list(du_scores.values())
        
        ax5.bar(range(len(layers)), scores, alpha=0.7, color='red')
        ax5.set_title('Dropout Uncertainty', fontweight='bold')
        ax5.set_xlabel('Layers')
        ax5.set_ylabel('Uncertainty')
        ax5.set_xticks(range(len(layers)))
        ax5.set_xticklabels([f'L{i}' for i in range(len(layers))])
    
    # 6. æ¿€æ´»ä¿®è¡¥
    if 'activation_patching' in advanced_results:
        ax6 = plt.subplot(3, 3, 6)
        ap_scores = advanced_results['activation_patching']
        layers = list(ap_scores.keys())
        scores = list(ap_scores.values())
        
        ax6.bar(range(len(layers)), scores, alpha=0.7, color='cyan')
        ax6.set_title('Activation Patching', fontweight='bold')
        ax6.set_xlabel('Layers')
        ax6.set_ylabel('Patching Effect')
        ax6.set_xticks(range(len(layers)))
        ax6.set_xticklabels([f'L{i}' for i in range(len(layers))])
    
    # 7. ç»¼åˆé‡è¦æ€§åˆ†æ•°
    ax7 = plt.subplot(3, 3, 7)
    if comprehensive_scores:
        layers = list(comprehensive_scores.keys())
        scores = list(comprehensive_scores.values())
        
        bars = ax7.bar(range(len(layers)), scores, alpha=0.8, color='gold')
        ax7.set_title('Comprehensive Importance Scores', fontweight='bold', fontsize=12)
        ax7.set_xlabel('Layers')
        ax7.set_ylabel('Combined Score')
        ax7.set_xticks(range(len(layers)))
        ax7.set_xticklabels([f'L{i}' for i in range(len(layers))])
        
        # é«˜äº®top-6å±‚
        sorted_indices = np.argsort(scores)[::-1][:6]
        for idx in sorted_indices:
            bars[idx].set_color('red')
            bars[idx].set_alpha(0.9)
    
    # 8. æ ‡å‡†åŒ–åˆ†æ•°å¯¹æ¯”
    ax8 = plt.subplot(3, 3, 8)
    if normalized_scores:
        methods = list(normalized_scores.keys())
        layer_indices = range(12)
        
        x = np.arange(len(layer_indices))
        width = 0.12
        
        colors = ['blue', 'green', 'red', 'orange', 'purple', 'cyan', 'yellow', 'pink']
        
        for i, method in enumerate(methods[:6]):  # é™åˆ¶æ–¹æ³•æ•°é‡
            if method in normalized_scores:
                scores = [normalized_scores[method].get(f'layer_{j}', 0) for j in layer_indices]
                ax8.bar(x + i * width, scores, width, label=method.replace('_', ' ').title(), 
                       alpha=0.7, color=colors[i % len(colors)])
        
        ax8.set_title('Normalized Scores Comparison', fontweight='bold')
        ax8.set_xlabel('Layers')
        ax8.set_ylabel('Normalized Score')
        ax8.set_xticks(x + width * 2)
        ax8.set_xticklabels([f'L{i}' for i in layer_indices])
        ax8.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # 9. å±‚é€‰æ‹©ç»“æœ
    ax9 = plt.subplot(3, 3, 9)
    if comprehensive_scores:
        # æ˜¾ç¤ºä¸åŒå‹ç¼©æ¯”çš„å±‚é€‰æ‹©
        compressions = [2.0, 3.0, 4.0]
        layer_counts = []
        
        for compression in compressions:
            target_layers = max(1, int(12 / compression))
            layer_counts.append(target_layers)
        
        bars = ax9.bar(range(len(compressions)), layer_counts, alpha=0.7, color='brown')
        ax9.set_title('Layer Selection for Different Compressions', fontweight='bold')
        ax9.set_xlabel('Compression Ratio')
        ax9.set_ylabel('Selected Layers')
        ax9.set_xticks(range(len(compressions)))
        ax9.set_xticklabels([f'{c:.1f}x' for c in compressions])
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, count) in enumerate(zip(bars, layer_counts)):
            ax9.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    str(count), ha='center')
    
    plt.tight_layout()
    plt.savefig('results/stage3_advanced_visualization.png', dpi=300, bbox_inches='tight')
    logger.info("ğŸ“Š é«˜çº§åˆ†æå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: results/stage3_advanced_visualization.png")
    plt.close()

if __name__ == "__main__":
    main()
