#!/usr/bin/env python3
"""
é˜¶æ®µ2ï¼šæ ¸å¿ƒå±‚é‡è¦æ€§åˆ†æ
åŸºäºé˜¶æ®µ1çš„ç¨³å®šæ¨¡å‹ï¼Œå®ç°Fisherä¿¡æ¯ã€æ¢¯åº¦åˆ†æã€å±‚æ¶ˆèç­‰æ ¸å¿ƒæ–¹æ³•
ä½¿ç”¨çœŸå®Amazonæ•°æ®è¿›è¡Œå±‚é‡è¦æ€§è¯„ä¼°
"""

import os
import json
import logging
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import pickle
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s'
)
logger = logging.getLogger(__name__)

class HonestDataLoader:
    """è¯šå®çš„æ•°æ®åŠ è½½å™¨ - ä¸é˜¶æ®µ1å…¼å®¹"""
    def __init__(self, data_path='dataset/amazon/Electronics_reviews.parquet'):
        self.data_path = data_path
        
    def load_real_data(self):
        """åŠ è½½çœŸå®Amazonæ•°æ®"""
        logger.info("ğŸ“Š åŠ è½½çœŸå®Amazon Electronicsæ•°æ®...")
        df = pd.read_parquet(self.data_path)
        logger.info(f"åŸå§‹æ•°æ®: {len(df):,} æ¡è®°å½•")
        
        # éªŒè¯æ•°æ®çœŸå®æ€§
        self._validate_data(df)
        
        # è´¨é‡è¿‡æ»¤
        df = self._filter_quality_data(df)
        
        return df
    
    def _validate_data(self, df):
        """éªŒè¯æ•°æ®çœŸå®æ€§"""
        logger.info("ğŸ” éªŒè¯Amazon Electronicsæ•°æ®çœŸå®æ€§...")
        logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        logger.info(f"åˆ—å: {df.columns.tolist()}")
        
        # æ–‡æœ¬å¤šæ ·æ€§æ£€æŸ¥
        if 'text' in df.columns:
            unique_texts = df['text'].nunique()
            total_texts = len(df)
            diversity = unique_texts / total_texts
            logger.info(f"æ–‡æœ¬å”¯ä¸€æ€§: {unique_texts:,}/{total_texts:,} = {diversity:.3f}")
            
            if diversity < 0.7:
                logger.warning(f"âš ï¸ æ–‡æœ¬å¤šæ ·æ€§è¾ƒä½: {diversity:.3f}")
            else:
                logger.info("âœ… æ–‡æœ¬å¤šæ ·æ€§éªŒè¯é€šè¿‡")
        
        # ç»Ÿè®¡åˆ†æ
        if 'rating' in df.columns:
            rating_dist = df['rating'].value_counts().sort_index()
            logger.info("è¯„åˆ†åˆ†å¸ƒ:")
            for rating, count in rating_dist.items():
                pct = count / len(df) * 100
                logger.info(f"  {rating}æ˜Ÿ: {count:,} ({pct:.1f}%)")
        
        logger.info("âœ… æ•°æ®çœŸå®æ€§éªŒè¯å®Œæˆ")
    
    def _filter_quality_data(self, df):
        """è¿‡æ»¤é«˜è´¨é‡æ•°æ®"""
        initial_count = len(df)
        
        # åŸºæœ¬è¿‡æ»¤
        df = df.dropna(subset=['text', 'rating'])
        df = df[df['text'].str.len() > 10]  # è‡³å°‘10ä¸ªå­—ç¬¦
        df = df[df['rating'].isin([1.0, 2.0, 3.0, 4.0, 5.0])]  # æœ‰æ•ˆè¯„åˆ†
        
        final_count = len(df)
        retention_rate = final_count / initial_count
        logger.info(f"è´¨é‡è¿‡æ»¤: {initial_count:,} -> {final_count:,} ({retention_rate:.1%}ä¿ç•™)")
        
        return df

class StableTransformer(nn.Module):
    """ä¸é˜¶æ®µ1å…¼å®¹çš„ç¨³å®šTransformeræ¨¡å‹"""
    def __init__(self, vocab_size, embed_dim=512, num_heads=8, num_layers=12, 
                 hidden_dim=2048, max_seq_len=256, num_classes=2, dropout=0.1):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.num_layers = num_layers
        self.max_seq_len = max_seq_len
        
        # åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.pos_embedding = nn.Parameter(torch.randn(max_seq_len, embed_dim))
        
        # Transformerå±‚
        self.layers = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads, hidden_dim, dropout)
            for _ in range(num_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.layer_norm = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(embed_dim, num_classes)
        
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.02)
    
    def forward(self, input_ids, attention_mask=None, return_layer_outputs=False):
        """å‰å‘ä¼ æ’­"""
        batch_size, seq_len = input_ids.shape
        
        # åµŒå…¥
        x = self.embedding(input_ids)
        x = x + self.pos_embedding[:seq_len]
        x = self.dropout(x)
        
        # æ³¨æ„åŠ›æ©ç 
        if attention_mask is None:
            attention_mask = (input_ids != 0).float()
        
        # å±‚è¾“å‡ºå­˜å‚¨
        layer_outputs = []
        
        # Transformerå±‚
        for i, layer in enumerate(self.layers):
            x = layer(x, attention_mask)
            if return_layer_outputs:
                layer_outputs.append(x.clone())
        
        # æœ€ç»ˆå¤„ç†
        x = self.layer_norm(x)
        
        # æ± åŒ– (ä½¿ç”¨[CLS]ä½ç½®æˆ–å¹³å‡æ± åŒ–)
        if attention_mask is not None:
            mask_expanded = attention_mask.unsqueeze(-1).expand_as(x)
            x = (x * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1)
        else:
            x = x.mean(dim=1)
        
        # åˆ†ç±»
        logits = self.classifier(self.dropout(x))
        
        if return_layer_outputs:
            return logits, layer_outputs
        return logits

class TransformerBlock(nn.Module):
    """Transformerå—"""
    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0.1):
        super().__init__()
        
        self.attention = nn.MultiheadAttention(
            embed_dim, num_heads, dropout=dropout, batch_first=True
        )
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, embed_dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, x, attention_mask=None):
        # Self-attention
        if attention_mask is not None:
            # è½¬æ¢æ©ç æ ¼å¼
            attn_mask = (attention_mask == 0)
        else:
            attn_mask = None
            
        attn_out, _ = self.attention(x, x, x, key_padding_mask=attn_mask)
        x = self.norm1(x + attn_out)
        
        # Feed forward
        ff_out = self.feed_forward(x)
        x = self.norm2(x + ff_out)
        
        return x

class CoreImportanceAnalyzer:
    """æ ¸å¿ƒé‡è¦æ€§åˆ†æå™¨ - Fisherä¿¡æ¯ã€æ¢¯åº¦åˆ†æã€å±‚æ¶ˆè"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.model.to(device)
        
        self.importance_scores = {}
        self.analysis_results = {}
        
    def analyze_layer_importance(self, dataloader, methods=['fisher', 'gradient', 'ablation']):
        """åˆ†æå±‚é‡è¦æ€§"""
        logger.info("ğŸ” å¼€å§‹æ ¸å¿ƒå±‚é‡è¦æ€§åˆ†æ...")
        
        results = {}
        
        if 'fisher' in methods:
            logger.info("ğŸ¯ è®¡ç®—Fisherä¿¡æ¯çŸ©é˜µ...")
            results['fisher'] = self._compute_fisher_information(dataloader)
            
        if 'gradient' in methods:
            logger.info("ğŸ“ˆ è®¡ç®—æ¢¯åº¦é‡è¦æ€§...")
            results['gradient'] = self._compute_gradient_importance(dataloader)
            
        if 'ablation' in methods:
            logger.info("âœ‚ï¸ æ‰§è¡Œå±‚æ¶ˆèåˆ†æ...")
            results['ablation'] = self._compute_ablation_importance(dataloader)
            
        self.analysis_results = results
        return results
    
    def _compute_fisher_information(self, dataloader):
        """è®¡ç®—Fisherä¿¡æ¯çŸ©é˜µ"""
        self.model.train()  # éœ€è¦è®­ç»ƒæ¨¡å¼æ¥è®¡ç®—æ¢¯åº¦
        fisher_info = {}
        
        # åˆå§‹åŒ–Fisherä¿¡æ¯å­˜å‚¨
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                fisher_info[name] = torch.zeros_like(param)
        
        total_samples = 0
        
        # ä¸ä½¿ç”¨no_gradï¼Œå› ä¸ºéœ€è¦è®¡ç®—æ¢¯åº¦
        for batch_idx, (data, targets) in enumerate(tqdm(dataloader, desc="Fisherä¿¡æ¯è®¡ç®—")):
            data, targets = data.to(self.device), targets.to(self.device)
            
            # å¯¹æ¯ä¸ªæ ·æœ¬è®¡ç®—Fisherä¿¡æ¯
            for i in range(min(data.size(0), 8)):  # é™åˆ¶æ‰¹æ¬¡å†…æ ·æœ¬æ•°é‡
                self.model.zero_grad()
                
                # å•æ ·æœ¬å‰å‘ä¼ æ’­
                single_data = data[i:i+1]
                single_target = targets[i:i+1]
                
                outputs = self.model(single_data)
                log_probs = F.log_softmax(outputs, dim=1)
                
                # å•æ ·æœ¬æŸå¤±
                sample_log_prob = log_probs[0, single_target[0]]
                sample_log_prob.backward()
                
                # ç´¯ç§¯Fisherä¿¡æ¯ (æ¢¯åº¦çš„å¹³æ–¹)
                for name, param in self.model.named_parameters():
                    if param.grad is not None:
                        fisher_info[name] += param.grad.data ** 2
                
                total_samples += 1
            
            if batch_idx >= 15:  # é™åˆ¶è®¡ç®—é‡
                break
        
        # å½’ä¸€åŒ–Fisherä¿¡æ¯
        for name in fisher_info:
            fisher_info[name] /= total_samples
        
        # æŒ‰å±‚èšåˆFisherä¿¡æ¯
        layer_fisher = {}
        for layer_idx in range(self.model.num_layers):
            layer_fisher[f'layer_{layer_idx}'] = 0
            param_count = 0
            
            for name, fisher_val in fisher_info.items():
                if f'layers.{layer_idx}' in name:
                    layer_fisher[f'layer_{layer_idx}'] += fisher_val.mean().item()
                    param_count += 1
            
            if param_count > 0:
                layer_fisher[f'layer_{layer_idx}'] /= param_count
        
        logger.info("âœ… Fisherä¿¡æ¯è®¡ç®—å®Œæˆ")
        return layer_fisher
    
    def _compute_gradient_importance(self, dataloader):
        """è®¡ç®—æ¢¯åº¦é‡è¦æ€§"""
        self.model.eval()
        gradient_norms = {f'layer_{i}': [] for i in range(self.model.num_layers)}
        
        for batch_idx, (data, targets) in enumerate(tqdm(dataloader, desc="æ¢¯åº¦é‡è¦æ€§è®¡ç®—")):
            data, targets = data.to(self.device), targets.to(self.device)
            
            self.model.zero_grad()
            outputs = self.model(data)
            loss = F.cross_entropy(outputs, targets)
            loss.backward()
            
            # è®¡ç®—æ¯å±‚çš„æ¢¯åº¦èŒƒæ•°
            for layer_idx in range(self.model.num_layers):
                layer_grad_norm = 0
                param_count = 0
                
                for name, param in self.model.named_parameters():
                    if f'layers.{layer_idx}' in name and param.grad is not None:
                        layer_grad_norm += param.grad.norm().item() ** 2
                        param_count += 1
                
                if param_count > 0:
                    layer_grad_norm = np.sqrt(layer_grad_norm / param_count)
                    gradient_norms[f'layer_{layer_idx}'].append(layer_grad_norm)
            
            if batch_idx >= 50:  # é™åˆ¶è®¡ç®—é‡
                break
        
        # è®¡ç®—å¹³å‡æ¢¯åº¦èŒƒæ•°
        layer_gradient_importance = {}
        for layer_name, norms in gradient_norms.items():
            if norms:
                layer_gradient_importance[layer_name] = np.mean(norms)
            else:
                layer_gradient_importance[layer_name] = 0.0
        
        logger.info("âœ… æ¢¯åº¦é‡è¦æ€§è®¡ç®—å®Œæˆ")
        return layer_gradient_importance
    
    def _compute_ablation_importance(self, dataloader):
        """è®¡ç®—å±‚æ¶ˆèé‡è¦æ€§"""
        self.model.eval()
        
        # è·å–åŸºå‡†æ€§èƒ½
        baseline_acc = self._evaluate_model(dataloader)
        logger.info(f"åŸºå‡†å‡†ç¡®ç‡: {baseline_acc:.4f}")
        
        ablation_importance = {}
        
        for layer_idx in range(self.model.num_layers):
            logger.info(f"æ¶ˆèç¬¬{layer_idx}å±‚...")
            
            # ä¿å­˜åŸå§‹æƒé‡
            original_weights = {}
            for name, param in self.model.named_parameters():
                if f'layers.{layer_idx}' in name:
                    original_weights[name] = param.data.clone()
                    param.data.zero_()  # å°†æƒé‡è®¾ä¸º0
            
            # è¯„ä¼°æ¶ˆèåçš„æ€§èƒ½
            ablated_acc = self._evaluate_model(dataloader)
            importance = baseline_acc - ablated_acc
            ablation_importance[f'layer_{layer_idx}'] = importance
            
            logger.info(f"æ¶ˆèåå‡†ç¡®ç‡: {ablated_acc:.4f}, é‡è¦æ€§: {importance:.4f}")
            
            # æ¢å¤åŸå§‹æƒé‡
            for name, original_weight in original_weights.items():
                for param_name, param in self.model.named_parameters():
                    if param_name == name:
                        param.data.copy_(original_weight)
                        break
        
        logger.info("âœ… å±‚æ¶ˆèåˆ†æå®Œæˆ")
        return ablation_importance
    
    def _evaluate_model(self, dataloader):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in dataloader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = self.model(data)
                _, predicted = torch.max(outputs, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
        
        return correct / total
    
    def select_important_layers(self, top_k=6, method='fisher'):
        """é€‰æ‹©é‡è¦å±‚"""
        if method not in self.analysis_results:
            raise ValueError(f"æ–¹æ³• {method} çš„åˆ†æç»“æœä¸å­˜åœ¨")
        
        importance_scores = self.analysis_results[method]
        
        # æ’åºå¹¶é€‰æ‹©top-k
        sorted_layers = sorted(importance_scores.items(), 
                             key=lambda x: x[1], reverse=True)
        
        selected_layers = [layer for layer, score in sorted_layers[:top_k]]
        selected_scores = {layer: score for layer, score in sorted_layers[:top_k]}
        
        logger.info(f"âœ… ä½¿ç”¨{method}æ–¹æ³•é€‰æ‹©top-{top_k}é‡è¦å±‚:")
        for layer, score in sorted_layers[:top_k]:
            logger.info(f"  {layer}: {score:.6f}")
        
        return selected_layers, selected_scores
    
    def create_compressed_model(self, selected_layers, original_vocab_size):
        """åˆ›å»ºå‹ç¼©æ¨¡å‹"""
        logger.info(f"ğŸ—ï¸ åˆ›å»ºå‹ç¼©æ¨¡å‹ï¼Œä¿ç•™{len(selected_layers)}å±‚...")
        
        # åˆ›å»ºæ–°çš„å‹ç¼©æ¨¡å‹
        compressed_model = StableTransformer(
            vocab_size=original_vocab_size,
            embed_dim=self.model.embed_dim,
            num_heads=8,  # ä¿æŒåŸæœ‰é…ç½®
            num_layers=len(selected_layers),
            hidden_dim=2048,
            max_seq_len=self.model.max_seq_len,
            num_classes=2,
            dropout=0.1
        )
        
        # å¤åˆ¶é€‰ä¸­å±‚çš„æƒé‡
        layer_indices = [int(layer.split('_')[1]) for layer in selected_layers]
        layer_indices.sort()
        
        for new_idx, old_idx in enumerate(layer_indices):
            # å¤åˆ¶transformerå±‚æƒé‡
            old_layer = self.model.layers[old_idx]
            new_layer = compressed_model.layers[new_idx]
            
            new_layer.load_state_dict(old_layer.state_dict())
        
        # å¤åˆ¶å…¶ä»–ç»„ä»¶
        compressed_model.embedding.load_state_dict(self.model.embedding.state_dict())
        compressed_model.layer_norm.load_state_dict(self.model.layer_norm.state_dict())
        compressed_model.classifier.load_state_dict(self.model.classifier.state_dict())
        
        # ä½ç½®åµŒå…¥éœ€è¦ç‰¹æ®Šå¤„ç†
        compressed_model.pos_embedding.data = self.model.pos_embedding.data.clone()
        
        logger.info(f"âœ… å‹ç¼©æ¨¡å‹åˆ›å»ºå®Œæˆ: {self.model.num_layers} -> {len(selected_layers)}å±‚")
        
        return compressed_model
    
    def evaluate_compression(self, original_model, compressed_model, test_dataloader):
        """è¯„ä¼°å‹ç¼©æ•ˆæœ"""
        logger.info("ğŸ“Š è¯„ä¼°å‹ç¼©æ•ˆæœ...")
        
        # è¯„ä¼°åŸå§‹æ¨¡å‹
        original_acc = self._evaluate_model_with_loader(original_model, test_dataloader)
        
        # è¯„ä¼°å‹ç¼©æ¨¡å‹
        compressed_acc = self._evaluate_model_with_loader(compressed_model, test_dataloader)
        
        # è®¡ç®—å‹ç¼©æ¯”
        original_params = sum(p.numel() for p in original_model.parameters())
        compressed_params = sum(p.numel() for p in compressed_model.parameters())
        compression_ratio = original_params / compressed_params
        
        results = {
            'original_accuracy': original_acc,
            'compressed_accuracy': compressed_acc,
            'accuracy_retention': compressed_acc / original_acc,
            'original_parameters': original_params,
            'compressed_parameters': compressed_params,
            'compression_ratio': compression_ratio,
            'parameter_reduction': 1 - (compressed_params / original_params)
        }
        
        logger.info("ğŸ¯ å‹ç¼©æ•ˆæœè¯„ä¼°:")
        logger.info(f"  åŸå§‹å‡†ç¡®ç‡: {original_acc:.4f}")
        logger.info(f"  å‹ç¼©å‡†ç¡®ç‡: {compressed_acc:.4f}")
        logger.info(f"  å‡†ç¡®ç‡ä¿æŒ: {results['accuracy_retention']:.2%}")
        logger.info(f"  å‹ç¼©æ¯”: {compression_ratio:.1f}x")
        logger.info(f"  å‚æ•°å‡å°‘: {results['parameter_reduction']:.1%}")
        
        return results
    
    def _evaluate_model_with_loader(self, model, dataloader):
        """ä½¿ç”¨æ•°æ®åŠ è½½å™¨è¯„ä¼°æ¨¡å‹"""
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in dataloader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = model(data)
                _, predicted = torch.max(outputs, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
        
        return correct / total

def load_stage1_results():
    """åŠ è½½é˜¶æ®µ1çš„ç»“æœ"""
    results_path = 'results/stage1_complete_results.json'
    if os.path.exists(results_path):
        with open(results_path, 'r') as f:
            return json.load(f)
    return None

def prepare_data_from_stage1():
    """åŸºäºé˜¶æ®µ1å‡†å¤‡æ•°æ®"""
    # é‡æ–°åŠ è½½å’Œå¤„ç†æ•°æ®ä»¥ä¿æŒä¸€è‡´æ€§
    loader = HonestDataLoader()
    df = loader.load_real_data()
    
    # åˆ›å»ºæ­£è´Ÿä¾‹
    positive_samples = df[df['rating'] >= 4].sample(n=min(15000, len(df[df['rating'] >= 4])))
    negative_samples = df[df['rating'] <= 2].sample(n=min(15000, len(df[df['rating'] <= 2])))
    
    # åˆå¹¶æ•°æ®
    final_df = pd.concat([positive_samples, negative_samples]).sample(frac=1).reset_index(drop=True)
    final_df['label'] = (final_df['rating'] >= 4).astype(int)
    
    # ç®€å•tokenization (ä¸é˜¶æ®µ1ä¿æŒä¸€è‡´)
    vocab = set()
    for text in final_df['text']:
        vocab.update(text.lower().split())
    
    vocab = ['<PAD>', '<UNK>'] + sorted(list(vocab))[:10000]
    word_to_idx = {word: idx for idx, word in enumerate(vocab)}
    
    def tokenize_text(text, max_len=256):
        tokens = text.lower().split()[:max_len]
        indices = [word_to_idx.get(token, 1) for token in tokens]  # 1 for <UNK>
        indices.extend([0] * (max_len - len(indices)))  # 0 for <PAD>
        return indices[:max_len]
    
    # Tokenization
    tokenized_texts = [tokenize_text(text) for text in final_df['text']]
    
    # è½¬æ¢ä¸ºtensor
    X = torch.tensor(tokenized_texts, dtype=torch.long)
    y = torch.tensor(final_df['label'].values, dtype=torch.long)
    
    # æ•°æ®åˆ†å‰²
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    test_dataset = TensorDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)
    test_loader = DataLoader(test_dataset, batch_size=32)
    
    return train_loader, val_loader, test_loader, len(vocab)

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹é˜¶æ®µ2ï¼šæ ¸å¿ƒå±‚é‡è¦æ€§åˆ†æ")
    
    # æ£€æŸ¥GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½é˜¶æ®µ1ç»“æœ
    stage1_results = load_stage1_results()
    if stage1_results:
        logger.info("âœ… é˜¶æ®µ1ç»“æœå·²åŠ è½½")
    
    # å‡†å¤‡æ•°æ®
    logger.info("ğŸ“Š å‡†å¤‡æ•°æ®...")
    train_loader, val_loader, test_loader, vocab_size = prepare_data_from_stage1()
    logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {vocab_size}")
    
    # åˆ›å»ºæ¨¡å‹ (ä¸é˜¶æ®µ1ç›¸åŒçš„æ¶æ„)
    logger.info("ğŸ—ï¸ åˆ›å»ºTransformeræ¨¡å‹...")
    model = StableTransformer(
        vocab_size=vocab_size,
        embed_dim=512,
        num_heads=8,
        num_layers=12,
        hidden_dim=2048,
        max_seq_len=256,
        num_classes=2,
        dropout=0.1
    )
    model.to(device)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®­ç»ƒæ¨¡å‹
    if os.path.exists('best_model_epoch_6.pt'):
        logger.info("ğŸ“‚ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        checkpoint = torch.load('best_model_epoch_6.pt', map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        logger.info("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆ")
    else:
        logger.info("âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    # åˆ›å»ºé‡è¦æ€§åˆ†æå™¨
    logger.info("ğŸ” åˆ›å»ºæ ¸å¿ƒé‡è¦æ€§åˆ†æå™¨...")
    analyzer = CoreImportanceAnalyzer(model, device)
    
    # æ‰§è¡Œæ ¸å¿ƒé‡è¦æ€§åˆ†æ
    logger.info("ğŸ¯ æ‰§è¡Œæ ¸å¿ƒé‡è¦æ€§åˆ†æ...")
    importance_results = analyzer.analyze_layer_importance(
        val_loader, 
        methods=['fisher', 'gradient', 'ablation']
    )
    
    # é€‰æ‹©é‡è¦å±‚
    logger.info("ğŸ¯ é€‰æ‹©é‡è¦å±‚...")
    selected_layers_fisher, fisher_scores = analyzer.select_important_layers(
        top_k=6, method='fisher'
    )
    selected_layers_gradient, gradient_scores = analyzer.select_important_layers(
        top_k=6, method='gradient'
    )
    selected_layers_ablation, ablation_scores = analyzer.select_important_layers(
        top_k=6, method='ablation'
    )
    
    # åˆ›å»ºå‹ç¼©æ¨¡å‹ (ä½¿ç”¨Fisheræ–¹æ³•)
    logger.info("ğŸ—ï¸ åˆ›å»ºå‹ç¼©æ¨¡å‹...")
    compressed_model = analyzer.create_compressed_model(selected_layers_fisher, vocab_size)
    compressed_model.to(device)
    
    # è¯„ä¼°å‹ç¼©æ•ˆæœ
    logger.info("ğŸ“Š è¯„ä¼°å‹ç¼©æ•ˆæœ...")
    compression_results = analyzer.evaluate_compression(
        model, compressed_model, test_loader
    )
    
    # ä¿å­˜ç»“æœ
    results = {
        'timestamp': datetime.now().isoformat(),
        'device': str(device),
        'model_config': {
            'vocab_size': vocab_size,
            'embed_dim': 512,
            'num_heads': 8,
            'original_layers': 12,
            'compressed_layers': len(selected_layers_fisher)
        },
        'importance_analysis': importance_results,
        'layer_selection': {
            'fisher': {
                'selected_layers': selected_layers_fisher,
                'scores': fisher_scores
            },
            'gradient': {
                'selected_layers': selected_layers_gradient,  
                'scores': gradient_scores
            },
            'ablation': {
                'selected_layers': selected_layers_ablation,
                'scores': ablation_scores
            }
        },
        'compression_results': compression_results
    }
    
    # ä¿å­˜ç»“æœ
    os.makedirs('results', exist_ok=True)
    results_path = 'results/stage2_importance_analysis.json'
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"ğŸ’¾ é˜¶æ®µ2ç»“æœå·²ä¿å­˜: {results_path}")
    
    # å¯è§†åŒ–ç»“æœ
    logger.info("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    create_visualization(importance_results, selected_layers_fisher)
    
    logger.info("ğŸ‰ é˜¶æ®µ2å®Œæˆï¼")
    logger.info("ğŸ”œ å‡†å¤‡è¿è¡Œé˜¶æ®µ3: é«˜çº§é‡è¦æ€§åˆ†ææ–¹æ³•")
    
    return results

def create_visualization(importance_results, selected_layers):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    plt.style.use('seaborn-v0_8')
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Fisherä¿¡æ¯é‡è¦æ€§
    if 'fisher' in importance_results:
        fisher_scores = importance_results['fisher']
        layers = list(fisher_scores.keys())
        scores = list(fisher_scores.values())
        
        ax = axes[0, 0]
        bars = ax.bar(range(len(layers)), scores, alpha=0.7)
        ax.set_title('Fisher Information Layer Importance', fontsize=14, fontweight='bold')
        ax.set_xlabel('Transformer Layers')
        ax.set_ylabel('Fisher Information Score')
        ax.set_xticks(range(len(layers)))
        ax.set_xticklabels([f'L{i}' for i in range(len(layers))], rotation=45)
        
        # é«˜äº®é€‰ä¸­çš„å±‚
        for i, layer in enumerate(layers):
            if layer in selected_layers:
                bars[i].set_color('red')
                bars[i].set_alpha(0.9)
    
    # 2. æ¢¯åº¦é‡è¦æ€§
    if 'gradient' in importance_results:
        gradient_scores = importance_results['gradient']
        layers = list(gradient_scores.keys())
        scores = list(gradient_scores.values())
        
        ax = axes[0, 1]
        ax.bar(range(len(layers)), scores, alpha=0.7, color='green')
        ax.set_title('Gradient Norm Layer Importance', fontsize=14, fontweight='bold')
        ax.set_xlabel('Transformer Layers')
        ax.set_ylabel('Gradient Norm Score')
        ax.set_xticks(range(len(layers)))
        ax.set_xticklabels([f'L{i}' for i in range(len(layers))], rotation=45)
    
    # 3. æ¶ˆèé‡è¦æ€§
    if 'ablation' in importance_results:
        ablation_scores = importance_results['ablation']
        layers = list(ablation_scores.keys())
        scores = list(ablation_scores.values())
        
        ax = axes[1, 0]
        ax.bar(range(len(layers)), scores, alpha=0.7, color='orange')
        ax.set_title('Ablation Study Layer Importance', fontsize=14, fontweight='bold')
        ax.set_xlabel('Transformer Layers')
        ax.set_ylabel('Performance Drop (Importance)')
        ax.set_xticks(range(len(layers)))
        ax.set_xticklabels([f'L{i}' for i in range(len(layers))], rotation=45)
    
    # 4. æ–¹æ³•å¯¹æ¯”
    ax = axes[1, 1]
    methods = list(importance_results.keys())
    layer_indices = range(12)  # å‡è®¾12å±‚
    
    x = np.arange(len(layer_indices))
    width = 0.25
    
    for i, method in enumerate(methods):
        scores = [importance_results[method].get(f'layer_{j}', 0) for j in layer_indices]
        ax.bar(x + i * width, scores, width, label=method.title(), alpha=0.7)
    
    ax.set_title('Layer Importance Comparison', fontsize=14, fontweight='bold')
    ax.set_xlabel('Transformer Layers')
    ax.set_ylabel('Normalized Importance Score')
    ax.set_xticks(x + width)
    ax.set_xticklabels([f'L{i}' for i in layer_indices])
    ax.legend()
    
    plt.tight_layout()
    plt.savefig('results/stage2_importance_visualization.png', dpi=300, bbox_inches='tight')
    logger.info("ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: results/stage2_importance_visualization.png")
    plt.close()

if __name__ == "__main__":
    main()
