#!/usr/bin/env python3
"""
H4å‡è®¾éªŒè¯å®éªŒ - å¢å¼ºç‰ˆçœŸå®LLMæ¨¡å‹å¯¹æ¯”
ä¸“ä¸ºanaconda Layerwiseç¯å¢ƒä¼˜åŒ–ï¼Œæ”¯æŒOllamaæœ¬åœ°æ¨¡å‹å’ŒOpenAI API
"""

import sys
import os
import subprocess
import importlib

# æ£€æŸ¥å’Œå®‰è£…å¿…è¦åŒ…çš„å‡½æ•°
def ensure_package_installed(package_name, pip_name=None):
    """ç¡®ä¿åŒ…å·²å®‰è£…ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•å®‰è£…"""
    if pip_name is None:
        pip_name = package_name
    
    try:
        importlib.import_module(package_name)
        print(f"âœ… {package_name} å·²å®‰è£…")
        return True
    except ImportError:
        print(f"âš ï¸ {package_name} æœªå®‰è£…ï¼Œå°è¯•å®‰è£…...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", pip_name])
            print(f"âœ… {package_name} å®‰è£…æˆåŠŸ")
            return True
        except subprocess.CalledProcessError:
            print(f"âŒ {package_name} å®‰è£…å¤±è´¥")
            return False

# å°è¯•å®‰è£…å…³é”®åŒ…
try:
    # åŸºç¡€ç§‘å­¦è®¡ç®—åŒ…
    import torch
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support
    print("âœ… åŸºç¡€ç§‘å­¦è®¡ç®—åŒ…å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ åŸºç¡€åŒ…å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# å°è¯•å¯¼å…¥å¯é€‰çš„LLMåŒ…
HAS_OLLAMA = False
HAS_OPENAI = False

try:
    import ollama
    HAS_OLLAMA = True
    print("âœ… Ollama åŒ…å¯ç”¨")
except ImportError:
    print("âš ï¸ Ollama åŒ…ä¸å¯ç”¨ï¼Œå°†è·³è¿‡æœ¬åœ°æ¨¡å‹")

try:
    import openai
    HAS_OPENAI = True
    print("âœ… OpenAI åŒ…å¯ç”¨")
except ImportError:
    print("âš ï¸ OpenAI åŒ…ä¸å¯ç”¨ï¼Œå°†è·³è¿‡APIæ¨¡å‹")

import logging  
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import json
from dataclasses import dataclass
from scipy.stats import ttest_ind
import time
import re
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass 
class EnhancedModelConfig:
    """å¢å¼ºæ¨¡å‹é…ç½®"""
    num_samples: int = 200  # é€‚åº¦å‡å°‘æ ·æœ¬æ•°
    num_test_cases: int = 50  # æµ‹è¯•æ¡ˆä¾‹æ•°
    max_retries: int = 3
    timeout: int = 30
    temperature: float = 0.1
    random_seed: int = 42
    openai_api_key: str = "YOUR_API_KEY_HERE"
    use_chatgpt_api: bool = True
    ollama_host: str = "http://localhost:11434"
    enable_local_models: bool = HAS_OLLAMA
    enable_api_models: bool = HAS_OPENAI

class MockModelWrapper:
    """æ¨¡æ‹Ÿæ¨¡å‹åŒ…è£…å™¨ï¼Œç”¨äºæ²¡æœ‰çœŸå®LLMæ—¶çš„æµ‹è¯•"""
    
    def __init__(self, model_name: str, config: EnhancedModelConfig):
        self.model_name = model_name
        self.config = config
        
        # æ¨¡æ‹Ÿæ¨¡å‹ç‰¹æ€§
        self.model_specs = {
            'llama3:latest': {
                'display_name': 'Llama3 (Mock)',
                'efficiency': 0.85,
                'reasoning': 0.95,
                'base_accuracy': 0.78
            },
            'qwen3:latest': {
                'display_name': 'Qwen3 (Mock)',
                'efficiency': 0.82,
                'reasoning': 0.88,
                'base_accuracy': 0.85
            },
            'gpt-4': {
                'display_name': 'GPT-4 (Mock)',
                'efficiency': 0.75,
                'reasoning': 0.98,
                'base_accuracy': 0.82
            },
            'gpt-3.5-turbo': {
                'display_name': 'GPT-3.5-Turbo (Mock)',
                'efficiency': 0.88,
                'reasoning': 0.90,
                'base_accuracy': 0.75
            }
        }
        
        self.spec = self.model_specs.get(model_name, {
            'display_name': f'{model_name} (Mock)',
            'efficiency': 0.75,
            'reasoning': 0.80,
            'base_accuracy': 0.70
        })
        
        # æ·»åŠ éšæœºæ€§
        np.random.seed(hash(model_name) % 1000)
        
    def generate_recommendation(self, user_profile: str, item_candidates: List[str], 
                              context: str = "") -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ¨èç»“æœ"""
        
        # åŸºäºæ¨¡å‹ç‰¹æ€§ç”Ÿæˆç»“æœ
        base_acc = self.spec['base_accuracy']
        reasoning_bonus = (self.spec['reasoning'] - 0.8) * 0.1
        efficiency_penalty = (0.9 - self.spec['efficiency']) * 0.05
        
        # è®¡ç®—è¿™æ¬¡æ¨èçš„å‡†ç¡®ç‡
        final_accuracy = base_acc + reasoning_bonus - efficiency_penalty
        final_accuracy += np.random.normal(0, 0.05)  # æ·»åŠ å™ªéŸ³
        final_accuracy = np.clip(final_accuracy, 0.1, 0.95)
        
        # éšæœºé€‰æ‹©æ¨èç‰©å“
        selected_item = np.random.choice(item_candidates)
        
        # ç”Ÿæˆæ¨ç†æ—¶é—´ (åŸºäºæ•ˆç‡)
        base_time = 2.0
        efficiency_mult = 2.0 - self.spec['efficiency']
        inference_time = base_time * efficiency_mult + np.random.exponential(0.5)
        
        return {
            'recommended_item': selected_item,
            'reasoning': f"Based on {self.spec['display_name']} analysis",
            'confidence': int(final_accuracy * 100),
            'inference_time': inference_time,
            'is_correct': np.random.random() < final_accuracy,
            'model_name': self.model_name
        }

class RealModelWrapper:
    """çœŸå®æ¨¡å‹åŒ…è£…å™¨"""
    
    def __init__(self, model_name: str, config: EnhancedModelConfig):
        self.model_name = model_name
        self.config = config
        self.model_type = self.get_model_type(model_name)
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        if self.model_type == 'ollama' and HAS_OLLAMA:
            self.client = ollama.Client(host=config.ollama_host)
        elif self.model_type == 'openai' and HAS_OPENAI:
            if config.use_chatgpt_api and config.openai_api_key:
                self.client = openai.OpenAI(api_key=config.openai_api_key)
            else:
                raise ValueError("OpenAI API key is required for ChatGPT models")
        else:
            raise ValueError(f"Cannot initialize {model_name}: required packages not available")
            
        # æ¨¡å‹è§„æ ¼
        self.model_specs = {
            'llama3:latest': {
                'display_name': 'Llama3',
                'efficiency': 0.85,
                'reasoning': 0.95
            },
            'qwen3:latest': {
                'display_name': 'Qwen3', 
                'efficiency': 0.82,
                'reasoning': 0.88
            },
            'gpt-4': {
                'display_name': 'GPT-4',
                'efficiency': 0.75,
                'reasoning': 0.98
            },
            'gpt-3.5-turbo': {
                'display_name': 'GPT-3.5-Turbo',
                'efficiency': 0.88,
                'reasoning': 0.90
            }
        }
        
        self.spec = self.model_specs.get(model_name, {
            'display_name': model_name,
            'efficiency': 0.75,
            'reasoning': 0.80
        })
    
    def get_model_type(self, model_name: str) -> str:
        """æ ¹æ®æ¨¡å‹åç§°ç¡®å®šç±»å‹"""
        if model_name.startswith(('gpt-3.5', 'gpt-4')):
            return 'openai'
        else:
            return 'ollama'
            
    def generate_recommendation(self, user_profile: str, item_candidates: List[str], 
                              context: str = "") -> Dict[str, Any]:
        """ç”ŸæˆçœŸå®æ¨èç»“æœ"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¨èç³»ç»Ÿã€‚è¯·æ ¹æ®ç”¨æˆ·æ¡£æ¡ˆæ¨èæœ€åˆé€‚çš„ç‰©å“ã€‚

ç”¨æˆ·æ¡£æ¡ˆ: {user_profile}
å€™é€‰ç‰©å“: {', '.join(item_candidates)}
{f'é¢å¤–ä¸Šä¸‹æ–‡: {context}' if context else ''}

è¯·ä»å€™é€‰ç‰©å“ä¸­é€‰æ‹©æœ€ä½³æ¨èï¼Œå¹¶ç»™å‡ºæ¨èç†ç”±ã€‚
è¾“å‡ºæ ¼å¼ï¼š
æ¨èç‰©å“: [ç‰©å“åç§°]
æ¨èç†ç”±: [ç®€æ´ç†ç”±]
ç½®ä¿¡åº¦: [0-100çš„æ•´æ•°]"""

        start_time = time.time()
        
        for attempt in range(self.config.max_retries):
            try:
                if self.model_type == 'ollama':
                    response = self.client.generate(
                        model=self.model_name,
                        prompt=prompt,
                        options={
                            'temperature': self.config.temperature,
                            'top_p': 0.9,
                            'top_k': 40
                        }
                    )
                    response_text = response['response']
                elif self.model_type == 'openai':
                    response = self.client.chat.completions.create(
                        model=self.model_name,
                        messages=[{"role": "user", "content": prompt}],
                        temperature=self.config.temperature,
                        max_tokens=500
                    )
                    response_text = response.choices[0].message.content
                
                inference_time = time.time() - start_time
                
                # è§£æå“åº”
                result = self._parse_recommendation_response(response_text)
                result['inference_time'] = inference_time
                result['model_name'] = self.model_name
                
                return result
                
            except Exception as e:
                logger.warning(f"âš ï¸ {self.spec['display_name']} ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥: {str(e)}")
                if attempt == self.config.max_retries - 1:
                    # è¿”å›é»˜è®¤ç»“æœ
                    return {
                        'recommended_item': item_candidates[0] if item_candidates else "unknown",
                        'reasoning': f"Error in {self.spec['display_name']}: {str(e)}",
                        'confidence': 50,
                        'inference_time': time.time() - start_time,
                        'is_correct': False,
                        'model_name': self.model_name
                    }
                    
    def _parse_recommendation_response(self, response: str) -> Dict[str, Any]:
        """è§£ææ¨¡å‹å“åº”"""
        try:
            # æå–æ¨èç‰©å“
            item_match = re.search(r'æ¨èç‰©å“[:ï¼š]\s*(.+)', response)
            recommended_item = item_match.group(1).strip() if item_match else "unknown"
            
            # æå–æ¨èç†ç”±
            reason_match = re.search(r'æ¨èç†ç”±[:ï¼š]\s*(.+)', response)
            reasoning = reason_match.group(1).strip() if reason_match else "No reasoning provided"
            
            # æå–ç½®ä¿¡åº¦
            conf_match = re.search(r'ç½®ä¿¡åº¦[:ï¼š]\s*(\d+)', response)
            confidence = int(conf_match.group(1)) if conf_match else 70
            
            return {
                'recommended_item': recommended_item,
                'reasoning': reasoning,
                'confidence': confidence,
                'is_correct': True  # éœ€è¦å¤–éƒ¨éªŒè¯
            }
        except Exception as e:
            logger.warning(f"âš ï¸ è§£æå“åº”å¤±è´¥: {str(e)}")
            return {
                'recommended_item': "parse_error",
                'reasoning': "Failed to parse response", 
                'confidence': 50,
                'is_correct': False
            }

class EnhancedRealLLMComparator:
    """å¢å¼ºç‰ˆçœŸå®LLMå¯¹æ¯”å™¨"""
    
    def __init__(self):
        self.config = EnhancedModelConfig()
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ç»“æœå­˜å‚¨
        self.results_dir = Path('results/hypothesis_validation/enhanced_llm_comparison')
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # é€‰æ‹©å¯ç”¨æ¨¡å‹
        self.available_models = []
        
        if self.config.enable_local_models:
            self.available_models.extend([
                'llama3:latest',
                'qwen3:latest'
            ])
            
        if self.config.enable_api_models:
            self.available_models.extend([
                'gpt-3.5-turbo',
                'gpt-4'
            ])
            
        # å¦‚æœæ²¡æœ‰çœŸå®æ¨¡å‹å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹
        if not self.available_models:
            logger.warning("âš ï¸ æ²¡æœ‰çœŸå®LLMå¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹")
            self.available_models = [
                'llama3:latest',
                'qwen3:latest', 
                'gpt-3.5-turbo',
                'gpt-4'
            ]
            self.use_mock = True
        else:
            self.use_mock = False
            
        logger.info(f"ğŸ”¬ å¢å¼ºç‰ˆLLMå¯¹æ¯”å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {self.available_models}")
        logger.info(f"ğŸ­ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼: {self.use_mock}")
        
    def generate_test_cases(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¨èæµ‹è¯•æ¡ˆä¾‹"""
        test_cases = []
        
        categories = [
            "ç”µå½±", "å›¾ä¹¦", "éŸ³ä¹", "ç¾é£Ÿ", "æ—…æ¸¸", 
            "ç”µå­äº§å“", "æœè£…", "æ¸¸æˆ", "è¿åŠ¨", "å­¦ä¹ "
        ]
        
        for i in range(self.config.num_test_cases):
            category = np.random.choice(categories)
            
            # ç”Ÿæˆç”¨æˆ·æ¡£æ¡ˆ
            age_group = np.random.choice(["é’å¹´", "ä¸­å¹´", "è€å¹´"])
            interests = np.random.choice(["ç§‘æŠ€", "æ–‡è‰º", "è¿åŠ¨", "ç¾é£Ÿ", "æ—…æ¸¸"], size=2, replace=False)
            
            user_profile = f"{age_group}ç”¨æˆ·ï¼Œå…´è¶£çˆ±å¥½ï¼š{', '.join(interests)}"
            
            # ç”Ÿæˆå€™é€‰ç‰©å“
            candidates = [f"{category}é€‰é¡¹{j+1}" for j in range(5)]
            
            # éšæœºé€‰æ‹©æ­£ç¡®ç­”æ¡ˆ
            correct_item = np.random.choice(candidates)
            
            test_cases.append({
                'id': i + 1,
                'category': category,
                'user_profile': user_profile,
                'candidates': candidates,
                'correct_answer': correct_item,
                'context': f"ä¸º{age_group}ç”¨æˆ·æ¨è{category}"
            })
            
        return test_cases
        
    def run_model_comparison(self) -> Dict[str, Any]:
        """è¿è¡Œæ¨¡å‹å¯¹æ¯”å®éªŒ"""
        logger.info("ğŸš€ å¼€å§‹æ¨¡å‹å¯¹æ¯”å®éªŒ")
        
        # ç”Ÿæˆæµ‹è¯•æ¡ˆä¾‹
        test_cases = self.generate_test_cases()
        logger.info(f"ğŸ“Š ç”Ÿæˆäº† {len(test_cases)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")
        
        # åˆå§‹åŒ–æ¨¡å‹
        model_wrappers = {}
        for model_name in self.available_models:
            try:
                if self.use_mock:
                    model_wrappers[model_name] = MockModelWrapper(model_name, self.config)
                else:
                    model_wrappers[model_name] = RealModelWrapper(model_name, self.config)
            except Exception as e:
                logger.warning(f"âš ï¸ åˆå§‹åŒ– {model_name} å¤±è´¥: {e}")
                # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹ä½œä¸ºå¤‡ç”¨
                model_wrappers[model_name] = MockModelWrapper(model_name, self.config)
        
        # è¿è¡Œå¯¹æ¯”å®éªŒ
        results = {}
        
        for model_name, wrapper in model_wrappers.items():
            logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {wrapper.spec['display_name']}")
            
            model_results = {
                'model_name': model_name,
                'display_name': wrapper.spec['display_name'],
                'predictions': [],
                'inference_times': [],
                'confidences': [],
                'correct_count': 0,
                'total_count': len(test_cases)
            }
            
            for i, test_case in enumerate(test_cases):
                try:
                    result = wrapper.generate_recommendation(
                        user_profile=test_case['user_profile'],
                        item_candidates=test_case['candidates'],
                        context=test_case['context']
                    )
                    
                    # è¯„ä¼°å‡†ç¡®æ€§ï¼ˆç®€åŒ–ç‰ˆï¼šæ£€æŸ¥æ¨èæ˜¯å¦åœ¨å€™é€‰ä¸­ï¼‰
                    is_valid = result['recommended_item'] in test_case['candidates']
                    
                    model_results['predictions'].append(result['recommended_item'])
                    model_results['inference_times'].append(result['inference_time'])
                    model_results['confidences'].append(result['confidence'])
                    
                    if is_valid:
                        model_results['correct_count'] += 1
                        
                    if (i + 1) % 10 == 0:
                        current_accuracy = model_results['correct_count'] / (i + 1)
                        avg_time = np.mean(model_results['inference_times'])
                        logger.info(f"    è¿›åº¦: {i+1}/{len(test_cases)}, "
                                  f"å½“å‰å‡†ç¡®ç‡: {current_accuracy:.3f}, "
                                  f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f}s")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ æµ‹è¯•æ¡ˆä¾‹ {i+1} å¤±è´¥: {e}")
                    model_results['predictions'].append("error")
                    model_results['inference_times'].append(10.0)  # æƒ©ç½šæ—¶é—´
                    model_results['confidences'].append(0)
                    
            # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
            accuracy = model_results['correct_count'] / model_results['total_count']
            avg_inference_time = np.mean(model_results['inference_times'])
            avg_confidence = np.mean(model_results['confidences'])
            
            model_results.update({
                'accuracy': accuracy,
                'avg_inference_time': avg_inference_time,
                'avg_confidence': avg_confidence,
                'efficiency_score': wrapper.spec['efficiency'],
                'reasoning_score': wrapper.spec['reasoning']
            })
            
            results[model_name] = model_results
            
            logger.info(f"âœ… {wrapper.spec['display_name']} å®Œæˆ - "
                       f"å‡†ç¡®ç‡: {accuracy:.3f}, "
                       f"å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time:.2f}s")
                       
        return results
        
    def analyze_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå®éªŒç»“æœ"""
        logger.info("ğŸ“Š åˆ†æå®éªŒç»“æœ")
        
        analysis = {
            'performance_ranking': [],
            'h4_evidence': {},
            'statistical_tests': {},
            'model_comparison': {}
        }
        
        # 1. æ€§èƒ½æ’å
        performance_data = []
        for model_name, result in results.items():
            performance_data.append((
                result['display_name'],
                model_name,
                result['accuracy'],
                result['avg_inference_time'],
                result['avg_confidence']
            ))
            
        # æŒ‰å‡†ç¡®ç‡æ’åº
        performance_data.sort(key=lambda x: x[2], reverse=True)
        analysis['performance_ranking'] = performance_data
        
        # 2. H4å‡è®¾éªŒè¯
        llama3_rank = None
        llama3_performance = 0
        
        for i, (display_name, model_name, accuracy, time, conf) in enumerate(performance_data):
            if 'llama' in model_name.lower():
                llama3_rank = i + 1
                llama3_performance = accuracy
                break
                
        # è®¡ç®—H4è¯æ®
        evidence_score = 0
        evidence_details = {}
        
        # è¯æ®1: æ’åå‰3
        rank_evidence = llama3_rank <= 3 if llama3_rank else False
        evidence_details['top_ranking'] = rank_evidence
        if rank_evidence:
            evidence_score += 1
            
        # è¯æ®2: å‡†ç¡®ç‡è¶…è¿‡å¹³å‡å€¼
        avg_accuracy = np.mean([r['accuracy'] for r in results.values()])
        acc_evidence = llama3_performance > avg_accuracy
        evidence_details['above_average_accuracy'] = acc_evidence
        if acc_evidence:
            evidence_score += 1
            
        # è¯æ®3: ç»¼åˆè¡¨ç°è‰¯å¥½
        if llama3_rank:
            comprehensive_evidence = llama3_rank <= len(results) // 2
            evidence_details['comprehensive_performance'] = comprehensive_evidence
            if comprehensive_evidence:
                evidence_score += 1
                
        # è¯æ®4: ç›¸å¯¹ä¼˜åŠ¿
        relative_evidence = llama3_performance > 0.6  # åŸºå‡†å‡†ç¡®ç‡
        evidence_details['performance_threshold'] = relative_evidence
        if relative_evidence:
            evidence_score += 1
            
        analysis['h4_evidence'] = {
            'llama3_rank': llama3_rank,
            'llama3_performance': llama3_performance,
            'evidence_score': evidence_score,
            'max_evidence': 4,
            'evidence_details': evidence_details,
            'hypothesis_supported': evidence_score >= 2,
            'average_accuracy': avg_accuracy
        }
        
        # 3. æ¨¡å‹å¯¹æ¯”è¯¦æƒ…
        for model_name, result in results.items():
            analysis['model_comparison'][model_name] = {
                'display_name': result['display_name'],
                'accuracy': result['accuracy'],
                'avg_inference_time': result['avg_inference_time'],
                'efficiency_score': result['efficiency_score'],
                'reasoning_score': result['reasoning_score'],
                'speed_score': result['accuracy'] / (result['avg_inference_time'] + 1e-6)
            }
            
        return analysis
        
    def create_visualizations(self, results: Dict[str, Any], analysis: Dict[str, Any]):
        """åˆ›å»ºå¯è§†åŒ–"""
        logger.info("ğŸ“Š åˆ›å»ºå¯è§†åŒ–å›¾è¡¨")
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('Enhanced H4 Hypothesis Validation: LLM Performance Analysis', 
                     fontsize=14, fontweight='bold')
        
        # 1. å‡†ç¡®ç‡å¯¹æ¯”
        rankings = analysis['performance_ranking']
        models = [item[0] for item in rankings]
        accuracies = [item[2] for item in rankings]
        
        colors = ['gold' if 'llama' in model.lower() else 'skyblue' for model in models]
        bars = axes[0, 0].bar(models, accuracies, color=colors, alpha=0.8)
        axes[0, 0].set_title('Model Accuracy Comparison')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        for bar, acc in zip(bars, accuracies):
            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{acc:.3f}', ha='center', va='bottom')
        
        # 2. æ¨ç†æ—¶é—´å¯¹æ¯”
        inference_times = [item[3] for item in rankings]
        axes[0, 1].bar(models, inference_times, color=colors, alpha=0.8)
        axes[0, 1].set_title('Average Inference Time')
        axes[0, 1].set_ylabel('Time (seconds)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. æ•ˆç‡vsæ€§èƒ½æ•£ç‚¹å›¾
        comp = analysis['model_comparison']
        model_names = list(comp.keys())
        speed_scores = [comp[m]['speed_score'] for m in model_names]
        model_accuracies = [comp[m]['accuracy'] for m in model_names]
        
        scatter_colors = ['red' if 'llama' in model.lower() else 'blue' for model in model_names]
        sizes = [100 if 'llama' in model.lower() else 60 for model in model_names]
        
        axes[1, 0].scatter(speed_scores, model_accuracies, c=scatter_colors, s=sizes, alpha=0.7)
        axes[1, 0].set_title('Efficiency vs Performance')
        axes[1, 0].set_xlabel('Speed Score')
        axes[1, 0].set_ylabel('Accuracy')
        axes[1, 0].grid(True, alpha=0.3)
        
        for i, model in enumerate(model_names):
            display_name = comp[model]['display_name']
            axes[1, 0].annotate(display_name, (speed_scores[i], model_accuracies[i]),
                               xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        # 4. H4è¯æ®æ€»ç»“
        evidence = analysis['h4_evidence']
        axes[1, 1].axis('off')
        
        summary_text = f"""
H4 Hypothesis Evidence Summary:

âœ“ Llama3 Rank: #{evidence.get('llama3_rank', 'N/A')}
âœ“ Performance: {evidence.get('llama3_performance', 0):.3f}
âœ“ Evidence Score: {evidence.get('evidence_score', 0)}/{evidence.get('max_evidence', 4)}

Evidence Details:
â€¢ Top Ranking: {'âœ“' if evidence['evidence_details'].get('top_ranking', False) else 'âœ—'}
â€¢ Above Average: {'âœ“' if evidence['evidence_details'].get('above_average_accuracy', False) else 'âœ—'}
â€¢ Comprehensive: {'âœ“' if evidence['evidence_details'].get('comprehensive_performance', False) else 'âœ—'}
â€¢ Threshold Met: {'âœ“' if evidence['evidence_details'].get('performance_threshold', False) else 'âœ—'}

Conclusion: {'âœ… H4 SUPPORTED' if evidence.get('hypothesis_supported', False) else 'âŒ H4 NOT SUPPORTED'}

Environment: {'ğŸ­ Mock Models' if self.use_mock else 'ğŸ¤– Real Models'}
"""
        
        axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes,
                        fontsize=9, verticalalignment='top', fontfamily='monospace',
                        bbox=dict(boxstyle="round,pad=0.5", facecolor='lightyellow', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plot_file = self.results_dir / f'enhanced_llm_comparison_{self.timestamp}.png'
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        logger.info(f"âœ… å¯è§†åŒ–ä¿å­˜è‡³: {plot_file}")
        
        plt.show()
        
    def save_results(self, results: Dict[str, Any], analysis: Dict[str, Any]):
        """ä¿å­˜ç»“æœ"""
        logger.info("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ")
        
        final_results = {
            'timestamp': self.timestamp,
            'config': {
                'num_test_cases': self.config.num_test_cases,
                'use_mock': self.use_mock,
                'available_models': self.available_models,
                'has_ollama': HAS_OLLAMA,
                'has_openai': HAS_OPENAI
            },
            'model_results': results,
            'analysis': analysis,
            'h4_validation': analysis.get('h4_evidence', {})
        }
        
        # ä¿å­˜JSONç»“æœ
        results_file = self.results_dir / f'enhanced_llm_results_{self.timestamp}.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False, default=str)
            
        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_report(final_results)
        report_file = self.results_dir / f'enhanced_H4_report_{self.timestamp}.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        logger.info(f"âœ… ç»“æœä¿å­˜è‡³: {results_file}")
        logger.info(f"âœ… æŠ¥å‘Šä¿å­˜è‡³: {report_file}")
        
    def _generate_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        evidence = results.get('h4_validation', {})
        analysis = results.get('analysis', {})
        config = results.get('config', {})
        
        report = f"""# H4å‡è®¾éªŒè¯æŠ¥å‘Šï¼ˆå¢å¼ºç‰ˆï¼‰: LLMæ¨¡å‹å¯¹æ¯”åˆ†æ

**å®éªŒæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**å‡è®¾é™ˆè¿°**: Llama3åœ¨æ¨èä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰ä¼˜åŠ¿

## ğŸ“‹ å®éªŒæ¦‚è¿°

æœ¬å®éªŒé€šè¿‡å¢å¼ºç‰ˆå¯¹æ¯”éªŒè¯H4å‡è®¾ï¼Œåˆ†æLlama3ç›¸å¯¹äºå…¶ä»–LLMæ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚

### å®éªŒç¯å¢ƒ
- **Ollamaæ”¯æŒ**: {'âœ…' if config.get('has_ollama', False) else 'âŒ'}
- **OpenAIæ”¯æŒ**: {'âœ…' if config.get('has_openai', False) else 'âŒ'}
- **æ¨¡å‹æ¨¡å¼**: {'ğŸ­ æ¨¡æ‹Ÿæ¨¡å‹' if config.get('use_mock', False) else 'ğŸ¤– çœŸå®æ¨¡å‹'}
- **æµ‹è¯•æ¡ˆä¾‹**: {config.get('num_test_cases', 0)}ä¸ª
- **å¯¹æ¯”æ¨¡å‹**: {', '.join(config.get('available_models', []))}

## ğŸ† å®éªŒç»“æœ

### æ€§èƒ½æ’å
"""
        
        if 'performance_ranking' in analysis:
            for i, (display_name, model_name, accuracy, time, conf) in enumerate(analysis['performance_ranking'], 1):
                emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "ğŸ“Š"
                report += f"{emoji} **{display_name}**: å‡†ç¡®ç‡ {accuracy:.4f}, æ¨ç†æ—¶é—´ {time:.2f}s\n"
        
        report += f"""

### H4å‡è®¾éªŒè¯ç»“æœ

**æ ¸å¿ƒæŒ‡æ ‡**:
- **Llama3æ’å**: #{evidence.get('llama3_rank', 'N/A')}
- **Llama3å‡†ç¡®ç‡**: {evidence.get('llama3_performance', 0):.4f}
- **å¹³å‡å‡†ç¡®ç‡**: {evidence.get('average_accuracy', 0):.4f}
- **è¯æ®å¼ºåº¦**: {evidence.get('evidence_score', 0)}/{evidence.get('max_evidence', 4)}

**è¯¦ç»†è¯æ®åˆ†æ**:
1. **æ’åè¡¨ç°**: {'âœ… å‰3å' if evidence.get('evidence_details', {}).get('top_ranking', False) else 'âŒ æ’åè¾ƒä½'}
2. **å¹³å‡æ°´å¹³**: {'âœ… è¶…è¿‡å¹³å‡' if evidence.get('evidence_details', {}).get('above_average_accuracy', False) else 'âŒ ä½äºå¹³å‡'}
3. **ç»¼åˆè¡¨ç°**: {'âœ… è¡¨ç°è‰¯å¥½' if evidence.get('evidence_details', {}).get('comprehensive_performance', False) else 'âŒ è¡¨ç°ä¸€èˆ¬'}
4. **æ€§èƒ½é˜ˆå€¼**: {'âœ… è¶…è¿‡åŸºå‡†' if evidence.get('evidence_details', {}).get('performance_threshold', False) else 'âŒ æœªè¾¾åŸºå‡†'}

## ğŸ“Š å‡è®¾éªŒè¯ç»“è®º

### {"âœ… **H4å‡è®¾å¾—åˆ°æ”¯æŒ**" if evidence.get('hypothesis_supported', False) else "âŒ **H4å‡è®¾æ”¯æŒåº¦ä¸è¶³**"}

**åˆ†ææ€»ç»“**:
"""
        
        if evidence.get('hypothesis_supported', False):
            report += f"""
- Llama3åœ¨{len(config.get('available_models', []))}ä¸ªæ¨¡å‹ä¸­æ’åç¬¬{evidence.get('llama3_rank', 'N/A')}
- å‡†ç¡®ç‡è¾¾åˆ°{evidence.get('llama3_performance', 0):.4f}ï¼Œ{"è¶…è¿‡" if evidence.get('llama3_performance', 0) > evidence.get('average_accuracy', 0) else "ä½äº"}å¹³å‡æ°´å¹³
- è·å¾—{evidence.get('evidence_score', 0)}/{evidence.get('max_evidence', 4)}çš„è¯æ®æ”¯æŒåº¦
- åœ¨æ¨èä»»åŠ¡ä¸Šå±•ç°äº†ç«äº‰ä¼˜åŠ¿
"""
        else:
            report += """
- æ€§èƒ½æ’åæœªè¾¾åˆ°é¢„æœŸæ°´å¹³
- ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ç¼ºä¹æ˜æ˜¾ä¼˜åŠ¿
- éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–å’Œæ”¹è¿›
- å»ºè®®åœ¨æ›´å¤šä»»åŠ¡ä¸Šè¿›è¡ŒéªŒè¯
"""
        
        report += f"""

## ğŸ” è¯¦ç»†åˆ†æ

### æ¨¡å‹æ€§èƒ½å¯¹æ¯”
"""
        
        if 'model_comparison' in analysis:
            for model_name, data in analysis['model_comparison'].items():
                report += f"""
**{data['display_name']}**:
- å‡†ç¡®ç‡: {data['accuracy']:.4f}
- å¹³å‡æ¨ç†æ—¶é—´: {data['avg_inference_time']:.2f}ç§’
- æ•ˆç‡åˆ†æ•°: {data['efficiency_score']:.2f}
- æ¨ç†åˆ†æ•°: {data['reasoning_score']:.2f}
- é€Ÿåº¦åˆ†æ•°: {data['speed_score']:.3f}
"""
        
        report += f"""

## ğŸ¯ ç»“è®ºä¸å»ºè®®

### ä¸»è¦å‘ç°
1. **æ€§èƒ½æ°´å¹³**: Llama3åœ¨æ¨èä»»åŠ¡ä¸Šçš„è¡¨ç°{"ç¬¦åˆé¢„æœŸ" if evidence.get('hypothesis_supported', False) else "æœ‰å¾…æå‡"}
2. **ç›¸å¯¹ä¼˜åŠ¿**: {"å…·å¤‡" if evidence.get('llama3_rank', 999) <= 3 else "ç¼ºä¹"}ç›¸å¯¹äºå…¶ä»–æ¨¡å‹çš„æ˜æ˜¾ä¼˜åŠ¿
3. **å®ç”¨ä»·å€¼**: {"é€‚åˆ" if evidence.get('hypothesis_supported', False) else "éœ€è°¨æ…è€ƒè™‘"}åœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨

### æŠ€æœ¯å»ºè®®
{"- **æ¨èéƒ¨ç½²**: Llama3å±•ç°äº†è‰¯å¥½çš„æ¨èèƒ½åŠ›ï¼Œå»ºè®®åœ¨å®é™…ç³»ç»Ÿä¸­è¯•ç”¨" if evidence.get('hypothesis_supported', False) else "- **ç»§ç»­ä¼˜åŒ–**: å»ºè®®é’ˆå¯¹æ¨èä»»åŠ¡è¿›è¡Œä¸“é—¨çš„å¾®è°ƒå’Œä¼˜åŒ–"}
- **æ‰©å±•éªŒè¯**: åœ¨æ›´å¤§è§„æ¨¡å’Œæ›´å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯
- **æ€§èƒ½è°ƒä¼˜**: æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯è°ƒæ•´æ¨¡å‹å‚æ•°

### å±€é™æ€§è¯´æ˜
- **{'æ¨¡æ‹Ÿç¯å¢ƒ' if config.get('use_mock', False) else 'å®éªŒç¯å¢ƒ'}**: ç»“æœå¯èƒ½ä¸ç”Ÿäº§ç¯å¢ƒå­˜åœ¨å·®å¼‚
- **æ ·æœ¬è§„æ¨¡**: æµ‹è¯•æ¡ˆä¾‹æ•°é‡ç›¸å¯¹æœ‰é™
- **è¯„ä¼°æŒ‡æ ‡**: é‡‡ç”¨ç®€åŒ–çš„è¯„ä¼°æ–¹æ³•

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**å®éªŒç±»å‹**: å¢å¼ºç‰ˆH4å‡è®¾éªŒè¯ï¼ˆ{'æ¨¡æ‹Ÿæ¨¡å¼' if config.get('use_mock', False) else 'çœŸå®æ¨¡å¼'}ï¼‰
"""
        
        return report
        
    def run_complete_validation(self):
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        logger.info("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆH4å‡è®¾éªŒè¯å®éªŒ")
        
        try:
            # 1. æ¨¡å‹å¯¹æ¯”
            results = self.run_model_comparison()
            
            if not results:
                logger.error("âŒ æ²¡æœ‰æˆåŠŸçš„å®éªŒç»“æœ")
                return
                
            # 2. ç»“æœåˆ†æ
            analysis = self.analyze_results(results)
            
            # 3. å¯è§†åŒ–
            self.create_visualizations(results, analysis)
            
            # 4. ä¿å­˜ç»“æœ
            self.save_results(results, analysis)
            
            logger.info("âœ… å¢å¼ºç‰ˆH4å‡è®¾éªŒè¯å®éªŒå®Œæˆï¼")
            
            return results, analysis
            
        except Exception as e:
            logger.error(f"âŒ å®éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ”¬ å¼€å§‹å¢å¼ºç‰ˆLlama3ä¼˜åŠ¿éªŒè¯å®éªŒ...")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    try:
        # è¿è¡ŒéªŒè¯
        comparator = EnhancedRealLLMComparator()
        results, analysis = comparator.run_complete_validation()
        
        logger.info("ğŸ‰ å®éªŒå®Œæˆï¼")
        
        # æ‰“å°å…³é”®ç»“æœ
        evidence = analysis.get('h4_evidence', {})
        logger.info(f"ğŸ“Š H4å‡è®¾éªŒè¯ç»“æœ: {'âœ… æ”¯æŒ' if evidence.get('hypothesis_supported', False) else 'âŒ ä¸æ”¯æŒ'}")
        logger.info(f"ğŸ† Llama3æ’å: #{evidence.get('llama3_rank', 'N/A')}")
        logger.info(f"ğŸ“ˆ è¯æ®å¼ºåº¦: {evidence.get('evidence_score', 0)}/{evidence.get('max_evidence', 4)}")
        
    except Exception as e:
        logger.error(f"âŒ ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()
