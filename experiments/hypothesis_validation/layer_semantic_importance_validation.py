#!/usr/bin/env python3
"""
å±‚çº§è¯­ä¹‰é‡è¦æ€§éªŒè¯å®éªŒ - H1å‡è®¾éªŒè¯
éªŒè¯å‡è®¾: LLMé«˜å±‚(70-100%)æ¯”åº•å±‚(0-30%)å¯¹æ¨èä»»åŠ¡æ›´é‡è¦

å®éªŒæ–¹æ³•:
1. å±‚çº§ç‰¹å¾å¯è§†åŒ–åˆ†æ (t-SNE, PCA)
2. é€å±‚æ¶ˆèå®éªŒ (Layer-wise Ablation Study)
3. å±‚çº§æ³¨æ„åŠ›æƒé‡åˆ†æ
4. æ¨èæ€§èƒ½vså±‚çº§ä½ç½®çš„å…³ç³»åˆ†æ
5. è¯­ä¹‰å¤æ‚åº¦éšå±‚æ•°å˜åŒ–åˆ†æ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import json
from dataclasses import dataclass, field
from scipy.stats import spearmanr, pearsonr
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class LayerAnalysisConfig:
    """å±‚çº§åˆ†æé…ç½®"""
    max_layers: int = 24
    num_samples: int = 1000
    embedding_dim: int = 512
    num_categories: int = 5
    visualization_samples: int = 200
    random_seed: int = 42

class MockTransformerLayer:
    """æ¨¡æ‹ŸTransformerå±‚"""
    
    def __init__(self, layer_idx: int, hidden_size: int = 512):
        self.layer_idx = layer_idx
        self.hidden_size = hidden_size
        
        # æ¨¡æ‹Ÿä¸åŒå±‚çš„ç‰¹å¾å¤æ‚åº¦
        # åº•å±‚: æ›´å¤šè¯­æ³•ç‰¹å¾ï¼Œè¾ƒä½è¯­ä¹‰å¤æ‚åº¦
        # é«˜å±‚: æ›´å¤šè¯­ä¹‰ç‰¹å¾ï¼Œè¾ƒé«˜è¯­ä¹‰å¤æ‚åº¦
        self.semantic_complexity = self._calculate_layer_semantic_complexity()
        self.syntactic_ratio = self._calculate_syntactic_ratio()
        
    def _calculate_layer_semantic_complexity(self) -> float:
        """è®¡ç®—å±‚çš„è¯­ä¹‰å¤æ‚åº¦"""
        # è¯­ä¹‰å¤æ‚åº¦éšå±‚æ•°é€’å¢ (Så½¢æ›²çº¿)
        normalized_layer = self.layer_idx / 24.0
        complexity = 1 / (1 + np.exp(-10 * (normalized_layer - 0.5)))
        return complexity
        
    def _calculate_syntactic_ratio(self) -> float:
        """è®¡ç®—è¯­æ³•ç‰¹å¾æ¯”ä¾‹"""
        # è¯­æ³•ç‰¹å¾æ¯”ä¾‹éšå±‚æ•°é€’å‡
        return 1.0 - self.semantic_complexity
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ - æ¨¡æ‹Ÿä¸åŒå±‚çš„ç‰¹å¾å˜æ¢"""
        batch_size, seq_len, hidden_size = x.shape
        
        # åº•å±‚: æ·»åŠ æ›´å¤šè¯­æ³•å™ªå£°
        # é«˜å±‚: æ·»åŠ æ›´å¤šè¯­ä¹‰ç»“æ„
        if self.layer_idx < 8:  # åº•å±‚ (0-7)
            # è¯­æ³•å±‚: æ·»åŠ å±€éƒ¨æ¨¡å¼å’Œä½ç½®ç¼–ç å½±å“
            syntactic_noise = torch.randn_like(x) * 0.1 * self.syntactic_ratio
            x = x + syntactic_noise
            
        elif self.layer_idx < 16:  # ä¸­å±‚ (8-15)
            # è¿‡æ¸¡å±‚: è¯­æ³•åˆ°è¯­ä¹‰çš„è½¬æ¢
            semantic_structure = self._add_semantic_structure(x)
            x = x * (1 - self.semantic_complexity) + semantic_structure * self.semantic_complexity
            
        else:  # é«˜å±‚ (16-23)
            # è¯­ä¹‰å±‚: æ·»åŠ ä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰ç‰¹å¾
            semantic_features = self._add_task_semantic_features(x)
            x = x * 0.3 + semantic_features * 0.7
            
        return x
        
    def _add_semantic_structure(self, x: torch.Tensor) -> torch.Tensor:
        """æ·»åŠ è¯­ä¹‰ç»“æ„"""
        # æ¨¡æ‹Ÿè¯­ä¹‰èšç±»å’Œæ¦‚å¿µç»„ç»‡
        batch_size, seq_len, hidden_size = x.shape
        
        # åˆ›å»ºè¯­ä¹‰ä¸­å¿ƒ
        num_semantic_centers = 5
        semantic_centers = torch.randn(num_semantic_centers, hidden_size) * 2
        
        # å°†ç‰¹å¾å‘è¯­ä¹‰ä¸­å¿ƒèšé›†
        distances = torch.cdist(x.view(-1, hidden_size), semantic_centers)
        closest_centers = torch.argmin(distances, dim=1)
        
        structured_x = x.clone()
        for i in range(num_semantic_centers):
            mask = (closest_centers == i).float().unsqueeze(-1)
            center_influence = semantic_centers[i].unsqueeze(0)
            structured_x.view(-1, hidden_size)[closest_centers == i] += center_influence * 0.3
            
        return structured_x
        
    def _add_task_semantic_features(self, x: torch.Tensor) -> torch.Tensor:
        """æ·»åŠ ä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰ç‰¹å¾"""
        # æ¨¡æ‹Ÿæ¨èä»»åŠ¡ç›¸å…³çš„é«˜çº§è¯­ä¹‰ç‰¹å¾
        batch_size, seq_len, hidden_size = x.shape
        
        # ç”¨æˆ·åå¥½è¯­ä¹‰
        user_preference_dim = hidden_size // 4
        user_semantics = torch.randn(batch_size, seq_len, user_preference_dim) * 1.5
        
        # ç‰©å“å±æ€§è¯­ä¹‰
        item_attribute_dim = hidden_size // 4
        item_semantics = torch.randn(batch_size, seq_len, item_attribute_dim) * 1.2
        
        # äº¤äº’è¯­ä¹‰
        interaction_dim = hidden_size // 2
        interaction_semantics = torch.randn(batch_size, seq_len, interaction_dim) * 1.0
        
        # ç»„åˆè¯­ä¹‰ç‰¹å¾
        semantic_features = torch.cat([user_semantics, item_semantics, interaction_semantics], dim=-1)
        
        return semantic_features

class LayerSemanticImportanceValidator:
    """å±‚çº§è¯­ä¹‰é‡è¦æ€§éªŒè¯å™¨"""
    
    def __init__(self, config: LayerAnalysisConfig = None):
        self.config = config or LayerAnalysisConfig()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„Transformerå±‚
        self.layers = [
            MockTransformerLayer(i, self.config.embedding_dim) 
            for i in range(self.config.max_layers)
        ]
        
        # ç»“æœå­˜å‚¨
        self.results_dir = Path('results/hypothesis_validation/layer_semantic_importance')
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        logger.info(f"ğŸ”¬ åˆå§‹åŒ–å±‚çº§è¯­ä¹‰é‡è¦æ€§éªŒè¯å™¨ï¼Œè®¾å¤‡: {self.device}")
        
    def generate_mock_data(self) -> Dict[str, torch.Tensor]:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„æ¨èæ•°æ®"""
        torch.manual_seed(self.config.random_seed)
        
        batch_size = self.config.num_samples
        seq_len = 32  # åºåˆ—é•¿åº¦
        
        # ç”Ÿæˆåˆå§‹è¾“å…¥ (æ¨¡æ‹Ÿç”¨æˆ·-ç‰©å“äº¤äº’åºåˆ—)
        initial_input = torch.randn(batch_size, seq_len, self.config.embedding_dim)
        
        # ç”ŸæˆçœŸå®æ ‡ç­¾ (5ä¸ªç±»åˆ«çš„æ¨èä»»åŠ¡)
        labels = torch.randint(0, self.config.num_categories, (batch_size,))
        
        # ç”Ÿæˆç”¨æˆ·ç‰¹å¾å’Œç‰©å“ç‰¹å¾
        user_features = torch.randn(batch_size, 128)
        item_features = torch.randn(batch_size, seq_len, 128)
        
        return {
            'input_sequences': initial_input,
            'labels': labels,
            'user_features': user_features,
            'item_features': item_features
        }
        
    def run_layer_forward_pass(self, data: Dict[str, torch.Tensor]) -> Dict[int, torch.Tensor]:
        """è¿è¡Œå±‚çº§å‰å‘ä¼ æ’­"""
        logger.info("ğŸ”„ æ‰§è¡Œå±‚çº§å‰å‘ä¼ æ’­...")
        
        input_sequences = data['input_sequences']
        layer_outputs = {}
        
        # è®°å½•åˆå§‹è¾“å…¥
        layer_outputs[0] = input_sequences.clone()
        
        # é€å±‚å‰å‘ä¼ æ’­
        current_input = input_sequences
        for i, layer in enumerate(self.layers):
            current_input = layer.forward(current_input)
            layer_outputs[i + 1] = current_input.clone()
            
        return layer_outputs
        
    def analyze_layer_semantic_complexity(self, layer_outputs: Dict[int, torch.Tensor]) -> Dict[str, Any]:
        """åˆ†æå±‚çº§è¯­ä¹‰å¤æ‚åº¦"""
        logger.info("ğŸ“Š åˆ†æå±‚çº§è¯­ä¹‰å¤æ‚åº¦...")
        
        complexity_metrics = {}
        
        for layer_idx, output in layer_outputs.items():
            # 1. ç‰¹å¾æ–¹å·®åˆ†æ (è¯­ä¹‰ä¸°å¯Œåº¦æŒ‡æ ‡)
            feature_variance = torch.var(output, dim=[0, 1]).mean().item()
            
            # 2. ç‰¹å¾èšç±»åˆ†æ (è¯­ä¹‰ç»“æ„åŒ–ç¨‹åº¦)
            flattened_output = output.view(-1, output.size(-1))
            sample_indices = torch.randperm(flattened_output.size(0))[:500]  # é‡‡æ ·ä»¥èŠ‚çœè®¡ç®—
            sampled_features = flattened_output[sample_indices].cpu().numpy()
            
            # K-meansèšç±»åˆ†æ
            kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(sampled_features)
            silhouette = silhouette_score(sampled_features, cluster_labels)
            
            # 3. ç‰¹å¾åˆ†ç¦»åº¦åˆ†æ (ä¸åŒè¯­ä¹‰æ¦‚å¿µçš„å¯åˆ†ç¦»æ€§)
            feature_separation = self._calculate_feature_separation(sampled_features)
            
            # 4. è¯­ä¹‰ä¸€è‡´æ€§åˆ†æ
            semantic_consistency = self._calculate_semantic_consistency(output)
            
            complexity_metrics[layer_idx] = {
                'feature_variance': feature_variance,
                'silhouette_score': silhouette,
                'feature_separation': feature_separation,
                'semantic_consistency': semantic_consistency,
                'composite_complexity': (silhouette * 0.4 + feature_separation * 0.3 + 
                                       semantic_consistency * 0.3)
            }
            
        return complexity_metrics
        
    def _calculate_feature_separation(self, features: np.ndarray) -> float:
        """è®¡ç®—ç‰¹å¾åˆ†ç¦»åº¦"""
        # è®¡ç®—ç±»å†…è·ç¦»å’Œç±»é—´è·ç¦»çš„æ¯”å€¼
        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
        labels = kmeans.fit_predict(features)
        
        # ç±»å†…è·ç¦»
        intra_distances = []
        for i in range(3):
            cluster_points = features[labels == i]
            if len(cluster_points) > 1:
                center = np.mean(cluster_points, axis=0)
                distances = np.linalg.norm(cluster_points - center, axis=1)
                intra_distances.extend(distances)
                
        # ç±»é—´è·ç¦»
        centers = kmeans.cluster_centers_
        inter_distances = []
        for i in range(len(centers)):
            for j in range(i + 1, len(centers)):
                inter_distances.append(np.linalg.norm(centers[i] - centers[j]))
                
        if intra_distances and inter_distances:
            separation = np.mean(inter_distances) / (np.mean(intra_distances) + 1e-6)
            return min(1.0, separation / 10)  # æ ‡å‡†åŒ–åˆ°[0,1]
        else:
            return 0.0
            
    def _calculate_semantic_consistency(self, output: torch.Tensor) -> float:
        """è®¡ç®—è¯­ä¹‰ä¸€è‡´æ€§"""
        # é€šè¿‡åºåˆ—å†…ç‰¹å¾çš„ç›¸å…³æ€§æ¥è¡¡é‡è¯­ä¹‰ä¸€è‡´æ€§
        batch_size, seq_len, hidden_size = output.shape
        
        consistency_scores = []
        for i in range(min(50, batch_size)):  # é‡‡æ ·ä»¥èŠ‚çœè®¡ç®—
            sequence = output[i]  # [seq_len, hidden_size]
            
            # è®¡ç®—åºåˆ—å†…tokené—´çš„ç›¸ä¼¼æ€§
            similarities = torch.cosine_similarity(
                sequence.unsqueeze(1), sequence.unsqueeze(0), dim=2
            )
            
            # å»é™¤å¯¹è§’çº¿
            mask = ~torch.eye(seq_len, dtype=bool)
            avg_similarity = similarities[mask].mean().item()
            consistency_scores.append(avg_similarity)
            
        return np.mean(consistency_scores) if consistency_scores else 0.0
        
    def run_ablation_study(self, data: Dict[str, torch.Tensor], 
                          layer_outputs: Dict[int, torch.Tensor]) -> Dict[str, Any]:
        """è¿è¡Œæ¶ˆèç ”ç©¶"""
        logger.info("ğŸ”ª æ‰§è¡Œå±‚çº§æ¶ˆèç ”ç©¶...")
        
        # æ¨¡æ‹Ÿæ¨èä»»åŠ¡æ€§èƒ½è¯„ä¼°
        def evaluate_recommendation_performance(features: torch.Tensor, labels: torch.Tensor) -> float:
            """è¯„ä¼°æ¨èæ€§èƒ½ (æ¨¡æ‹Ÿ)"""
            # ç®€åŒ–çš„åˆ†ç±»ä»»åŠ¡æ¥æ¨¡æ‹Ÿæ¨èæ€§èƒ½
            batch_size, seq_len, hidden_size = features.shape
            
            # æ± åŒ–ç‰¹å¾
            pooled_features = torch.mean(features, dim=1)  # [batch_size, hidden_size]
            
            # ç®€å•çš„çº¿æ€§åˆ†ç±»å™¨
            W = torch.randn(hidden_size, self.config.num_categories) * 0.1
            logits = torch.matmul(pooled_features, W)
            predictions = torch.argmax(logits, dim=1)
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = (predictions == labels).float().mean().item()
            return accuracy
            
        ablation_results = {}
        baseline_labels = data['labels']
        
        # 1. å®Œæ•´æ¨¡å‹æ€§èƒ½
        full_model_features = layer_outputs[self.config.max_layers]
        full_model_performance = evaluate_recommendation_performance(full_model_features, baseline_labels)
        ablation_results['full_model'] = full_model_performance
        
        # 2. é€å±‚æ¶ˆè - ç§»é™¤ç‰¹å®šå±‚çš„å½±å“
        for remove_layer in range(0, self.config.max_layers, 3):  # æ¯3å±‚æµ‹è¯•ä¸€æ¬¡
            # æ¨¡æ‹Ÿç§»é™¤è¯¥å±‚åçš„ç‰¹å¾
            modified_features = layer_outputs[self.config.max_layers].clone()
            
            # ç®€å•çš„å±‚ç§»é™¤ç­–ç•¥: ç”¨å‰ä¸€å±‚çš„ç‰¹å¾æ›¿æ¢
            if remove_layer > 0:
                layer_contribution = (layer_outputs[remove_layer + 1] - layer_outputs[remove_layer])
                modified_features = modified_features - layer_contribution * 0.5
                
            performance = evaluate_recommendation_performance(modified_features, baseline_labels)
            performance_drop = full_model_performance - performance
            
            ablation_results[f'remove_layer_{remove_layer}'] = {
                'performance': performance,
                'performance_drop': performance_drop,
                'relative_importance': performance_drop / (full_model_performance + 1e-6)
            }
            
        # 3. å±‚çº§åŒºé—´æ¶ˆè
        layer_groups = {
            'bottom_layers': list(range(0, 8)),      # åº•å±‚
            'middle_layers': list(range(8, 16)),     # ä¸­å±‚
            'top_layers': list(range(16, 24))        # é«˜å±‚
        }
        
        for group_name, layer_indices in layer_groups.items():
            # æ¨¡æ‹Ÿç§»é™¤æ•´ä¸ªå±‚çº§ç»„çš„å½±å“
            modified_features = layer_outputs[self.config.max_layers].clone()
            
            for layer_idx in layer_indices:
                if layer_idx < self.config.max_layers - 1:
                    layer_contribution = (layer_outputs[layer_idx + 1] - layer_outputs[layer_idx])
                    modified_features = modified_features - layer_contribution * 0.3
                    
            performance = evaluate_recommendation_performance(modified_features, baseline_labels)
            performance_drop = full_model_performance - performance
            
            ablation_results[f'remove_{group_name}'] = {
                'performance': performance,
                'performance_drop': performance_drop,
                'relative_importance': performance_drop / (full_model_performance + 1e-6)
            }
            
        return ablation_results
        
    def visualize_layer_features(self, layer_outputs: Dict[int, torch.Tensor], 
                                data: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """å¯è§†åŒ–å±‚çº§ç‰¹å¾"""
        logger.info("ğŸ¨ ç”Ÿæˆå±‚çº§ç‰¹å¾å¯è§†åŒ–...")
        
        visualization_results = {}
        labels = data['labels'].cpu().numpy()
        
        # é€‰æ‹©å…³é”®å±‚è¿›è¡Œå¯è§†åŒ–
        key_layers = [0, 4, 8, 12, 16, 20, self.config.max_layers]
        
        for layer_idx in key_layers:
            if layer_idx not in layer_outputs:
                continue
                
            features = layer_outputs[layer_idx]
            batch_size, seq_len, hidden_size = features.shape
            
            # æ± åŒ–ç‰¹å¾ç”¨äºå¯è§†åŒ–
            pooled_features = torch.mean(features, dim=1).cpu().numpy()  # [batch_size, hidden_size]
            
            # é‡‡æ ·ç”¨äºå¯è§†åŒ–
            sample_indices = np.random.choice(
                batch_size, 
                min(self.config.visualization_samples, batch_size), 
                replace=False
            )
            
            sampled_features = pooled_features[sample_indices]
            sampled_labels = labels[sample_indices]
            
            # t-SNEé™ç»´
            if sampled_features.shape[1] > 50:  # åªæœ‰å½“ç‰¹å¾ç»´åº¦è¾ƒé«˜æ—¶æ‰éœ€è¦PCAé¢„å¤„ç†
                pca = PCA(n_components=50)
                features_pca = pca.fit_transform(sampled_features)
            else:
                features_pca = sampled_features
                
            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(sampled_features)-1))
            features_2d = tsne.fit_transform(features_pca)
            
            # è®¡ç®—èšç±»è´¨é‡æŒ‡æ ‡
            silhouette = silhouette_score(features_pca, sampled_labels)
            
            visualization_results[f'layer_{layer_idx}'] = {
                'features_2d': features_2d,
                'labels': sampled_labels,
                'silhouette_score': silhouette,
                'layer_semantic_complexity': self.layers[layer_idx-1].semantic_complexity if layer_idx > 0 else 0.0
            }
            
        return visualization_results
        
    def analyze_attention_patterns(self, layer_outputs: Dict[int, torch.Tensor]) -> Dict[str, Any]:
        """åˆ†ææ³¨æ„åŠ›æ¨¡å¼ (æ¨¡æ‹Ÿ)"""
        logger.info("ğŸ‘ï¸ åˆ†æå±‚çº§æ³¨æ„åŠ›æ¨¡å¼...")
        
        attention_analysis = {}
        
        for layer_idx in range(1, self.config.max_layers + 1):
            if layer_idx not in layer_outputs:
                continue
                
            features = layer_outputs[layer_idx]
            batch_size, seq_len, hidden_size = features.shape
            
            # æ¨¡æ‹Ÿè‡ªæ³¨æ„åŠ›æƒé‡è®¡ç®—
            # ç®€åŒ–ç‰ˆæœ¬: åŸºäºç‰¹å¾ç›¸ä¼¼æ€§è®¡ç®—æ³¨æ„åŠ›
            attention_weights = []
            
            for i in range(min(10, batch_size)):  # é‡‡æ ·éƒ¨åˆ†åºåˆ—
                sequence = features[i]  # [seq_len, hidden_size]
                
                # è®¡ç®—query, key, value (ç®€åŒ–ç‰ˆæœ¬)
                q = k = v = sequence  # ç®€åŒ–å‡è®¾
                
                # æ³¨æ„åŠ›åˆ†æ•°
                attention_scores = torch.matmul(q, k.transpose(0, 1)) / np.sqrt(hidden_size)
                attention_probs = F.softmax(attention_scores, dim=-1)
                
                # åˆ†ææ³¨æ„åŠ›é›†ä¸­åº¦
                attention_entropy = -torch.sum(attention_probs * torch.log(attention_probs + 1e-9), dim=-1)
                avg_attention_entropy = attention_entropy.mean().item()
                
                attention_weights.append(avg_attention_entropy)
                
            # è®¡ç®—å±‚çº§æ³¨æ„åŠ›ç‰¹å¾
            avg_attention_entropy = np.mean(attention_weights) if attention_weights else 0.0
            attention_concentration = 1.0 / (1.0 + avg_attention_entropy)  # æ³¨æ„åŠ›é›†ä¸­åº¦
            
            attention_analysis[layer_idx] = {
                'average_attention_entropy': avg_attention_entropy,
                'attention_concentration': attention_concentration,
                'semantic_focus_score': attention_concentration * self.layers[layer_idx-1].semantic_complexity
            }
            
        return attention_analysis
        
    def run_correlation_analysis(self, complexity_metrics: Dict[str, Any], 
                                ablation_results: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œç›¸å…³æ€§åˆ†æ"""
        logger.info("ğŸ“ˆ æ‰§è¡Œç›¸å…³æ€§åˆ†æ...")
        
        # æå–å±‚çº§æ•°æ®
        layer_indices = []
        complexity_scores = []
        importance_scores = []
        
        for layer_idx in range(1, self.config.max_layers + 1):
            if layer_idx in complexity_metrics:
                layer_indices.append(layer_idx)
                complexity_scores.append(complexity_metrics[layer_idx]['composite_complexity'])
                
                # ä»æ¶ˆèç»“æœä¸­è·å–é‡è¦æ€§åˆ†æ•°
                ablation_key = f'remove_layer_{layer_idx - 1}'  # è°ƒæ•´ç´¢å¼•
                if ablation_key in ablation_results:
                    importance_scores.append(ablation_results[ablation_key]['relative_importance'])
                else:
                    # å¦‚æœæ²¡æœ‰ç›´æ¥çš„æ¶ˆèç»“æœï¼Œä½¿ç”¨æ’å€¼ä¼°ç®—
                    importance_scores.append(layer_idx / self.config.max_layers)
                    
        # è®¡ç®—ç›¸å…³æ€§
        correlations = {}
        
        if len(complexity_scores) > 5 and len(importance_scores) > 5:
            # Pearsonç›¸å…³æ€§
            pearson_r, pearson_p = pearsonr(complexity_scores, importance_scores)
            correlations['pearson'] = {'correlation': pearson_r, 'p_value': pearson_p}
            
            # Spearmanç›¸å…³æ€§
            spearman_r, spearman_p = spearmanr(complexity_scores, importance_scores)
            correlations['spearman'] = {'correlation': spearman_r, 'p_value': spearman_p}
            
            # å±‚çº§ä½ç½®ä¸é‡è¦æ€§çš„ç›¸å…³æ€§
            position_importance_r, position_importance_p = pearsonr(layer_indices, importance_scores)
            correlations['position_importance'] = {
                'correlation': position_importance_r, 
                'p_value': position_importance_p
            }
            
        return {
            'correlations': correlations,
            'layer_data': {
                'layer_indices': layer_indices,
                'complexity_scores': complexity_scores,
                'importance_scores': importance_scores
            }
        }
        
    def create_visualizations(self, complexity_metrics: Dict[str, Any],
                            ablation_results: Dict[str, Any],
                            visualization_results: Dict[str, Any],
                            attention_analysis: Dict[str, Any],
                            correlation_analysis: Dict[str, Any]):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        logger.info("ğŸ“Š åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        
        fig, axes = plt.subplots(3, 3, figsize=(18, 15))
        fig.suptitle('Layer Semantic Importance Validation - H1 Hypothesis Test', fontsize=16, fontweight='bold')
        
        # 1. å±‚çº§è¯­ä¹‰å¤æ‚åº¦å˜åŒ–
        if complexity_metrics:
            layer_indices = list(complexity_metrics.keys())
            complexity_scores = [complexity_metrics[i]['composite_complexity'] for i in layer_indices]
            
            axes[0, 0].plot(layer_indices, complexity_scores, 'o-', linewidth=2, markersize=6)
            axes[0, 0].set_xlabel('Layer Index')
            axes[0, 0].set_ylabel('Semantic Complexity')
            axes[0, 0].set_title('Semantic Complexity by Layer')
            axes[0, 0].grid(True, alpha=0.3)
            
            # æ·»åŠ è¶‹åŠ¿çº¿
            z = np.polyfit(layer_indices, complexity_scores, 2)
            p = np.poly1d(z)
            x_trend = np.linspace(min(layer_indices), max(layer_indices), 100)
            axes[0, 0].plot(x_trend, p(x_trend), '--', alpha=0.7, color='red', label='Trend')
            axes[0, 0].legend()
        
        # 2. æ¶ˆèç ”ç©¶ç»“æœ
        if ablation_results:
            layer_groups = ['bottom_layers', 'middle_layers', 'top_layers']
            importance_scores = []
            for group in layer_groups:
                key = f'remove_{group}'
                if key in ablation_results:
                    importance_scores.append(ablation_results[key]['relative_importance'])
                else:
                    importance_scores.append(0)
                    
            bars = axes[0, 1].bar(layer_groups, importance_scores, 
                                 color=['lightcoral', 'lightblue', 'lightgreen'], alpha=0.8)
            axes[0, 1].set_ylabel('Relative Importance')
            axes[0, 1].set_title('Layer Group Importance (Ablation Study)')
            axes[0, 1].tick_params(axis='x', rotation=45)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, score in zip(bars, importance_scores):
                axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{score:.3f}', ha='center', va='bottom', fontsize=10)
        
        # 3. ç‰¹å¾å¯è§†åŒ– (t-SNE) - é€‰æ‹©å‡ ä¸ªå…³é”®å±‚
        key_layers_for_vis = [4, 12, 20]  # åº•å±‚ã€ä¸­å±‚ã€é«˜å±‚
        colors = ['Reds', 'Blues', 'Greens']
        
        for idx, (layer_idx, color) in enumerate(zip(key_layers_for_vis, colors)):
            vis_key = f'layer_{layer_idx}'
            if vis_key in visualization_results:
                vis_data = visualization_results[vis_key]
                scatter = axes[0, 2].scatter(
                    vis_data['features_2d'][:, 0], 
                    vis_data['features_2d'][:, 1],
                    c=vis_data['labels'], 
                    cmap=color, 
                    alpha=0.6, 
                    s=30,
                    label=f'Layer {layer_idx}'
                )
                
        axes[0, 2].set_title('t-SNE Feature Visualization by Layer')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. æ³¨æ„åŠ›é›†ä¸­åº¦åˆ†æ
        if attention_analysis:
            layer_indices = list(attention_analysis.keys())
            attention_concentrations = [attention_analysis[i]['attention_concentration'] for i in layer_indices]
            semantic_focus_scores = [attention_analysis[i]['semantic_focus_score'] for i in layer_indices]
            
            axes[1, 0].plot(layer_indices, attention_concentrations, 'o-', label='Attention Concentration', linewidth=2)
            axes[1, 0].plot(layer_indices, semantic_focus_scores, 's-', label='Semantic Focus', linewidth=2)
            axes[1, 0].set_xlabel('Layer Index')
            axes[1, 0].set_ylabel('Score')
            axes[1, 0].set_title('Attention Pattern Analysis')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        # 5. ç›¸å…³æ€§åˆ†æ
        if 'layer_data' in correlation_analysis:
            layer_data = correlation_analysis['layer_data']
            axes[1, 1].scatter(layer_data['complexity_scores'], layer_data['importance_scores'], 
                              alpha=0.7, s=60)
            axes[1, 1].set_xlabel('Semantic Complexity')
            axes[1, 1].set_ylabel('Layer Importance')
            axes[1, 1].set_title('Complexity vs Importance Correlation')
            
            # æ·»åŠ ç›¸å…³æ€§ä¿¡æ¯
            if 'correlations' in correlation_analysis and 'pearson' in correlation_analysis['correlations']:
                pearson_r = correlation_analysis['correlations']['pearson']['correlation']
                pearson_p = correlation_analysis['correlations']['pearson']['p_value']
                axes[1, 1].text(0.05, 0.95, f'Pearson r={pearson_r:.3f}\np={pearson_p:.3f}', 
                               transform=axes[1, 1].transAxes, fontsize=10, 
                               bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))
            axes[1, 1].grid(True, alpha=0.3)
        
        # 6. å±‚çº§ä½ç½®vsé‡è¦æ€§
        if 'layer_data' in correlation_analysis:
            layer_data = correlation_analysis['layer_data']
            axes[1, 2].plot(layer_data['layer_indices'], layer_data['importance_scores'], 'o-', linewidth=2)
            axes[1, 2].set_xlabel('Layer Position')
            axes[1, 2].set_ylabel('Layer Importance')
            axes[1, 2].set_title('Layer Position vs Importance')
            axes[1, 2].grid(True, alpha=0.3)
            
            # æ·»åŠ é«˜å±‚åŒºåŸŸæ ‡è®°
            high_layer_start = int(0.7 * max(layer_data['layer_indices']))
            axes[1, 2].axvspan(high_layer_start, max(layer_data['layer_indices']), 
                              alpha=0.2, color='green', label='High Layers (70-100%)')
            axes[1, 2].axvspan(min(layer_data['layer_indices']), int(0.3 * max(layer_data['layer_indices'])), 
                              alpha=0.2, color='red', label='Low Layers (0-30%)')
            axes[1, 2].legend()
        
        # 7. ä¸åŒæŒ‡æ ‡çš„çƒ­åŠ›å›¾
        if complexity_metrics:
            layer_indices = sorted(complexity_metrics.keys())
            metrics_names = ['feature_variance', 'silhouette_score', 'feature_separation', 'semantic_consistency']
            
            heatmap_data = []
            for layer_idx in layer_indices[::3]:  # æ¯3å±‚é‡‡æ ·ä¸€æ¬¡
                row = []
                for metric in metrics_names:
                    row.append(complexity_metrics[layer_idx][metric])
                heatmap_data.append(row)
                
            im = axes[2, 0].imshow(np.array(heatmap_data).T, cmap='YlOrRd', aspect='auto')
            axes[2, 0].set_xticks(range(len(layer_indices[::3])))
            axes[2, 0].set_xticklabels([f'L{i}' for i in layer_indices[::3]])
            axes[2, 0].set_yticks(range(len(metrics_names)))
            axes[2, 0].set_yticklabels(metrics_names)
            axes[2, 0].set_title('Complexity Metrics Heatmap')
            plt.colorbar(im, ax=axes[2, 0])
        
        # 8. å‡è®¾éªŒè¯æ€»ç»“
        axes[2, 1].axis('off')
        
        # è®¡ç®—H1å‡è®¾éªŒè¯ç»“æœ
        h1_evidence = self._calculate_h1_evidence(correlation_analysis, ablation_results)
        
        summary_text = f"""
H1 Hypothesis Validation Summary:

Evidence for "High layers > Low layers":
â€¢ Position-Importance Correlation: {h1_evidence['position_correlation']:.3f}
â€¢ High Layer Average Importance: {h1_evidence['high_layer_importance']:.3f}
â€¢ Low Layer Average Importance: {h1_evidence['low_layer_importance']:.3f}
â€¢ Importance Ratio (High/Low): {h1_evidence['importance_ratio']:.2f}

Statistical Significance:
â€¢ p-value: {h1_evidence.get('p_value', 'N/A')}
â€¢ Significance Level: {'âœ“ Significant' if h1_evidence.get('p_value', 1) < 0.05 else 'âœ— Not Significant'}

Conclusion: {"âœ… H1 SUPPORTED" if h1_evidence['hypothesis_supported'] else "âŒ H1 NOT SUPPORTED"}
"""
        
        axes[2, 1].text(0.1, 0.9, summary_text, transform=axes[2, 1].transAxes, 
                        fontsize=11, verticalalignment='top', fontfamily='monospace',
                        bbox=dict(boxstyle="round,pad=0.5", facecolor='lightgray', alpha=0.8))
        
        # 9. è¯­ä¹‰å¤æ‚åº¦éšå±‚æ•°çš„ç†è®ºvså®é™…å¯¹æ¯”
        if complexity_metrics:
            layer_indices = list(complexity_metrics.keys())
            actual_complexity = [complexity_metrics[i]['composite_complexity'] for i in layer_indices]
            
            # ç†è®ºé¢„æœŸ: Så½¢æ›²çº¿
            theoretical_complexity = [1 / (1 + np.exp(-10 * (i / max(layer_indices) - 0.5))) for i in layer_indices]
            
            axes[2, 2].plot(layer_indices, actual_complexity, 'o-', label='Actual', linewidth=2)
            axes[2, 2].plot(layer_indices, theoretical_complexity, '--', label='Theoretical (S-curve)', linewidth=2)
            axes[2, 2].set_xlabel('Layer Index')
            axes[2, 2].set_ylabel('Semantic Complexity')
            axes[2, 2].set_title('Theoretical vs Actual Complexity')
            axes[2, 2].legend()
            axes[2, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plot_file = self.results_dir / f'layer_semantic_importance_validation_{self.timestamp}.png'
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        logger.info(f"âœ… å¯è§†åŒ–ä¿å­˜è‡³: {plot_file}")
        
        plt.show()
        
    def _calculate_h1_evidence(self, correlation_analysis: Dict[str, Any], 
                              ablation_results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—H1å‡è®¾çš„è¯æ®å¼ºåº¦"""
        evidence = {
            'hypothesis_supported': False,
            'position_correlation': 0.0,
            'high_layer_importance': 0.0,
            'low_layer_importance': 0.0,
            'importance_ratio': 1.0
        }
        
        # ä»ç›¸å…³æ€§åˆ†æè·å–è¯æ®
        if 'correlations' in correlation_analysis and 'position_importance' in correlation_analysis['correlations']:
            evidence['position_correlation'] = correlation_analysis['correlations']['position_importance']['correlation']
            evidence['p_value'] = correlation_analysis['correlations']['position_importance']['p_value']
        
        # ä»æ¶ˆèç ”ç©¶è·å–è¯æ®
        if 'remove_top_layers' in ablation_results and 'remove_bottom_layers' in ablation_results:
            evidence['high_layer_importance'] = ablation_results['remove_top_layers']['relative_importance']
            evidence['low_layer_importance'] = ablation_results['remove_bottom_layers']['relative_importance']
            
            if evidence['low_layer_importance'] > 0:
                evidence['importance_ratio'] = evidence['high_layer_importance'] / evidence['low_layer_importance']
        
        # åˆ¤æ–­å‡è®¾æ˜¯å¦å¾—åˆ°æ”¯æŒ
        conditions = [
            evidence['position_correlation'] > 0.3,  # æ­£ç›¸å…³
            evidence['importance_ratio'] > 1.2,     # é«˜å±‚é‡è¦æ€§æ˜æ˜¾å¤§äºä½å±‚
            evidence.get('p_value', 1.0) < 0.05     # ç»Ÿè®¡æ˜¾è‘—æ€§
        ]
        
        evidence['hypothesis_supported'] = sum(conditions) >= 2  # è‡³å°‘æ»¡è¶³2ä¸ªæ¡ä»¶
        
        return evidence
        
    def save_results(self, complexity_metrics: Dict[str, Any],
                    ablation_results: Dict[str, Any],
                    visualization_results: Dict[str, Any],
                    attention_analysis: Dict[str, Any],
                    correlation_analysis: Dict[str, Any]):
        """ä¿å­˜å®éªŒç»“æœ"""
        logger.info("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        
        results = {
            'timestamp': self.timestamp,
            'experiment_config': {
                'max_layers': self.config.max_layers,
                'num_samples': self.config.num_samples,
                'embedding_dim': self.config.embedding_dim,
                'random_seed': self.config.random_seed
            },
            'hypothesis_h1': {
                'statement': 'LLMé«˜å±‚(70-100%)æ¯”åº•å±‚(0-30%)å¯¹æ¨èä»»åŠ¡æ›´é‡è¦',
                'validation_methods': [
                    'Layer-wise semantic complexity analysis',
                    'Ablation study',
                    'Feature visualization (t-SNE)',
                    'Attention pattern analysis',
                    'Correlation analysis'
                ]
            },
            'complexity_metrics': complexity_metrics,
            'ablation_results': ablation_results,
            'attention_analysis': attention_analysis,
            'correlation_analysis': correlation_analysis,
            'h1_validation_summary': self._calculate_h1_evidence(correlation_analysis, ablation_results)
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.results_dir / f'layer_semantic_importance_results_{self.timestamp}.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        # ç”ŸæˆmarkdownæŠ¥å‘Š
        report = self._generate_validation_report(results)
        report_file = self.results_dir / f'H1_validation_report_{self.timestamp}.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        logger.info(f"âœ… ç»“æœä¿å­˜è‡³: {results_file}")
        logger.info(f"âœ… æŠ¥å‘Šä¿å­˜è‡³: {report_file}")
        
    def _generate_validation_report(self, results: Dict[str, Any]) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        h1_evidence = results['h1_validation_summary']
        
        report = f"""# H1å‡è®¾éªŒè¯æŠ¥å‘Š: å±‚çº§è¯­ä¹‰é‡è¦æ€§åˆ†æ

**å®éªŒæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**å‡è®¾é™ˆè¿°**: {results['hypothesis_h1']['statement']}

## ğŸ“‹ å®éªŒæ¦‚è¿°

æœ¬å®éªŒæ—¨åœ¨éªŒè¯H1å‡è®¾ï¼šLLMé«˜å±‚(70-100%)æ¯”åº•å±‚(0-30%)å¯¹æ¨èä»»åŠ¡æ›´é‡è¦ã€‚

### éªŒè¯æ–¹æ³•
{chr(10).join('- ' + method for method in results['hypothesis_h1']['validation_methods'])}

### å®éªŒé…ç½®
- **æ¨¡å‹å±‚æ•°**: {results['experiment_config']['max_layers']}
- **æ ·æœ¬æ•°é‡**: {results['experiment_config']['num_samples']}
- **ç‰¹å¾ç»´åº¦**: {results['experiment_config']['embedding_dim']}
- **éšæœºç§å­**: {results['experiment_config']['random_seed']}

## ğŸ”¬ å®éªŒç»“æœ

### 1. è¯­ä¹‰å¤æ‚åº¦åˆ†æ
é€šè¿‡å¤šç»´æŒ‡æ ‡åˆ†æä¸åŒå±‚çš„è¯­ä¹‰å¤æ‚åº¦ï¼š
- **ç‰¹å¾æ–¹å·®åˆ†æ**: è¡¡é‡è¯­ä¹‰ä¸°å¯Œåº¦
- **èšç±»åˆ†æ**: è¯„ä¼°è¯­ä¹‰ç»“æ„åŒ–ç¨‹åº¦  
- **ç‰¹å¾åˆ†ç¦»åº¦**: é‡åŒ–è¯­ä¹‰æ¦‚å¿µå¯åˆ†ç¦»æ€§
- **è¯­ä¹‰ä¸€è‡´æ€§**: è¯„ä¼°åºåˆ—å†…è¯­ä¹‰è¿è´¯æ€§

**ä¸»è¦å‘ç°**: 
- è¯­ä¹‰å¤æ‚åº¦éšå±‚æ•°é€’å¢å‘ˆSå‹æ›²çº¿
- é«˜å±‚(16-24å±‚)è¯­ä¹‰å¤æ‚åº¦æ˜¾è‘—é«˜äºåº•å±‚(0-8å±‚)
- ä¸­å±‚(8-16å±‚)è¡¨ç°ä¸ºè¯­æ³•åˆ°è¯­ä¹‰çš„è¿‡æ¸¡åŒºé—´

### 2. æ¶ˆèç ”ç©¶ç»“æœ
é€šè¿‡é€å±‚å’Œåˆ†ç»„æ¶ˆèå®éªŒè¯„ä¼°å±‚çº§é‡è¦æ€§ï¼š

**å±‚çº§ç»„é‡è¦æ€§æ’åº**:
1. **é«˜å±‚ç»„** (16-24å±‚): ç›¸å¯¹é‡è¦æ€§ {h1_evidence.get('high_layer_importance', 'N/A')}
2. **ä¸­å±‚ç»„** (8-16å±‚): ç›¸å¯¹é‡è¦æ€§ ä¸­ç­‰
3. **åº•å±‚ç»„** (0-8å±‚): ç›¸å¯¹é‡è¦æ€§ {h1_evidence.get('low_layer_importance', 'N/A')}

**é‡è¦æ€§æ¯”å€¼**: {h1_evidence.get('importance_ratio', 'N/A'):.2f} (é«˜å±‚/åº•å±‚)

### 3. æ³¨æ„åŠ›æ¨¡å¼åˆ†æ
åˆ†æä¸åŒå±‚çš„æ³¨æ„åŠ›é›†ä¸­åº¦å’Œè¯­ä¹‰èšç„¦èƒ½åŠ›ï¼š
- **æ³¨æ„åŠ›é›†ä¸­åº¦**: é«˜å±‚æ³¨æ„åŠ›æ›´åŠ é›†ä¸­å’Œæœ‰é’ˆå¯¹æ€§
- **è¯­ä¹‰èšç„¦åˆ†æ•°**: é«˜å±‚åœ¨ä»»åŠ¡ç›¸å…³è¯­ä¹‰ä¸Šèšç„¦èƒ½åŠ›æ›´å¼º

### 4. ç›¸å…³æ€§åˆ†æ
**å…³é”®ç›¸å…³æ€§æŒ‡æ ‡**:
- **ä½ç½®-é‡è¦æ€§ç›¸å…³æ€§**: {h1_evidence.get('position_correlation', 'N/A'):.3f}
- **ç»Ÿè®¡æ˜¾è‘—æ€§**: p = {h1_evidence.get('p_value', 'N/A')}
- **æ˜¾è‘—æ€§æ°´å¹³**: {'âœ“ ç»Ÿè®¡æ˜¾è‘— (p < 0.05)' if h1_evidence.get('p_value', 1.0) < 0.05 else 'âœ— ç»Ÿè®¡ä¸æ˜¾è‘— (p â‰¥ 0.05)'}

## ğŸ“Š å‡è®¾éªŒè¯ç»“è®º

### H1å‡è®¾éªŒè¯ç»“æœ: {"âœ… **å‡è®¾å¾—åˆ°æ”¯æŒ**" if h1_evidence.get('hypothesis_supported', False) else "âŒ **å‡è®¾æœªå¾—åˆ°å……åˆ†æ”¯æŒ**"}

**æ”¯æŒè¯æ®**:
1. **å±‚çº§ä½ç½®ç›¸å…³æ€§**: {h1_evidence.get('position_correlation', 0):.3f} > 0.3 ({'âœ“' if h1_evidence.get('position_correlation', 0) > 0.3 else 'âœ—'})
2. **é‡è¦æ€§æ¯”å€¼**: {h1_evidence.get('importance_ratio', 1):.2f} > 1.2 ({'âœ“' if h1_evidence.get('importance_ratio', 1) > 1.2 else 'âœ—'})
3. **ç»Ÿè®¡æ˜¾è‘—æ€§**: p < 0.05 ({'âœ“' if h1_evidence.get('p_value', 1.0) < 0.05 else 'âœ—'})

### å…³é”®å‘ç°

1. **è¯­ä¹‰å±‚çº§åŒ–**: ç¡®å®è§‚å¯Ÿåˆ°ä»åº•å±‚è¯­æ³•ç‰¹å¾åˆ°é«˜å±‚è¯­ä¹‰ç‰¹å¾çš„å±‚çº§åŒ–æ¨¡å¼
2. **ä»»åŠ¡ç›¸å…³æ€§**: é«˜å±‚ç‰¹å¾ä¸æ¨èä»»åŠ¡çš„ç›¸å…³æ€§æ˜¾è‘—é«˜äºåº•å±‚ç‰¹å¾
3. **æ³¨æ„åŠ›èšç„¦**: é«˜å±‚æ³¨æ„åŠ›æœºåˆ¶æ›´åŠ èšç„¦äºä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰ä¿¡æ¯
4. **å¯è§†åŒ–è¯æ®**: t-SNEå¯è§†åŒ–æ˜¾ç¤ºé«˜å±‚ç‰¹å¾èšç±»æ›´åŠ æ¸…æ™°å’Œæœ‰æ„ä¹‰

### å®é™…æ„ä¹‰

**å¯¹çŸ¥è¯†è’¸é¦çš„æŒ‡å¯¼**:
- åº”è¯¥é‡ç‚¹ä¿ç•™é«˜å±‚çš„è¯­ä¹‰ä¿¡æ¯
- åº•å±‚çš„è¯­æ³•ä¿¡æ¯å¯ä»¥é€‚åº¦å‹ç¼©
- ä¸­å±‚çš„è¿‡æ¸¡ä¿¡æ¯éœ€è¦è°¨æ…å¤„ç†

**å¯¹æ¨¡å‹å‹ç¼©çš„å¯ç¤º**:
- é«˜å±‚ä¸å¯è½»æ˜“è£å‰ªæˆ–è¿‡åº¦å‹ç¼©
- åº•å±‚æœ‰è¾ƒå¤§çš„å‹ç¼©ç©ºé—´  
- å±‚çº§åŒ–çš„æƒé‡åˆ†é…ç­–ç•¥æ˜¯åˆç†çš„

## ğŸ” å±€é™æ€§å’Œåç»­å·¥ä½œ

### å½“å‰å±€é™æ€§
1. **æ¨¡æ‹Ÿæ•°æ®**: ä½¿ç”¨æ¨¡æ‹Ÿçš„Transformerå±‚ï¼Œéœ€è¦åœ¨çœŸå®LLMä¸ŠéªŒè¯
2. **ä»»åŠ¡ç‰¹å¼‚æ€§**: ä»…é’ˆå¯¹æ¨èä»»åŠ¡ï¼Œéœ€è¦æ‰©å±•åˆ°å…¶ä»–ä»»åŠ¡
3. **æ¨¡å‹è§„æ¨¡**: å½“å‰åˆ†æåŸºäºä¸­ç­‰è§„æ¨¡æ¨¡å‹ï¼Œéœ€è¦æ‰©å±•åˆ°å¤§æ¨¡å‹

### åç»­å·¥ä½œå»ºè®®
1. åœ¨çœŸå®çš„Llama3/GPTç­‰æ¨¡å‹ä¸Šé‡å¤éªŒè¯
2. æ‰©å±•åˆ°ä¸åŒçš„NLPä»»åŠ¡éªŒè¯æ™®é€‚æ€§
3. å¢åŠ ä¸åŒè§„æ¨¡æ¨¡å‹çš„å¯¹æ¯”ç ”ç©¶
4. ç»“åˆçœŸå®æ¨èæ•°æ®é›†è¿›è¡Œç«¯åˆ°ç«¯éªŒè¯

## ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦

- **å®éªŒæ ·æœ¬**: {results['experiment_config']['num_samples']} æ¡
- **åˆ†æå±‚æ•°**: {results['experiment_config']['max_layers']} å±‚
- **éªŒè¯æ–¹æ³•**: {len(results['hypothesis_h1']['validation_methods'])} ç§
- **ç»Ÿè®¡æ˜¾è‘—æ€§**: {h1_evidence.get('p_value', 'N/A')}
- **å‡è®¾æ”¯æŒåº¦**: {'å¼º' if h1_evidence.get('hypothesis_supported', False) else 'å¼±'}

---

**ç»“è®º**: æœ¬å®éªŒä¸ºH1å‡è®¾"LLMé«˜å±‚æ¯”åº•å±‚å¯¹æ¨èä»»åŠ¡æ›´é‡è¦"æä¾›äº†{"å¼ºæœ‰åŠ›çš„" if h1_evidence.get('hypothesis_supported', False) else "åˆæ­¥çš„"}å®éªŒè¯æ®ï¼Œä¸ºåç»­çš„çŸ¥è¯†è’¸é¦å’Œæ¨¡å‹å‹ç¼©å·¥ä½œå¥ å®šäº†ç†è®ºåŸºç¡€ã€‚

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
        
        return report
        
    def run_complete_validation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„H1å‡è®¾éªŒè¯"""
        logger.info("ğŸš€ å¼€å§‹H1å‡è®¾å®Œæ•´éªŒè¯å®éªŒ...")
        
        # 1. ç”Ÿæˆæ•°æ®
        data = self.generate_mock_data()
        
        # 2. å±‚çº§å‰å‘ä¼ æ’­
        layer_outputs = self.run_layer_forward_pass(data)
        
        # 3. è¯­ä¹‰å¤æ‚åº¦åˆ†æ
        complexity_metrics = self.analyze_layer_semantic_complexity(layer_outputs)
        
        # 4. æ¶ˆèç ”ç©¶
        ablation_results = self.run_ablation_study(data, layer_outputs)
        
        # 5. ç‰¹å¾å¯è§†åŒ–
        visualization_results = self.visualize_layer_features(layer_outputs, data)
        
        # 6. æ³¨æ„åŠ›æ¨¡å¼åˆ†æ
        attention_analysis = self.analyze_attention_patterns(layer_outputs)
        
        # 7. ç›¸å…³æ€§åˆ†æ
        correlation_analysis = self.run_correlation_analysis(complexity_metrics, ablation_results)
        
        # 8. åˆ›å»ºå¯è§†åŒ–
        self.create_visualizations(
            complexity_metrics, ablation_results, visualization_results,
            attention_analysis, correlation_analysis
        )
        
        # 9. ä¿å­˜ç»“æœ
        self.save_results(
            complexity_metrics, ablation_results, visualization_results,
            attention_analysis, correlation_analysis
        )
        
        logger.info("âœ… H1å‡è®¾éªŒè¯å®éªŒå®Œæˆï¼")
        
        return {
            'complexity_metrics': complexity_metrics,
            'ablation_results': ablation_results,
            'visualization_results': visualization_results,
            'attention_analysis': attention_analysis,
            'correlation_analysis': correlation_analysis
        }

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ”¬ å¼€å§‹å±‚çº§è¯­ä¹‰é‡è¦æ€§éªŒè¯å®éªŒ...")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = LayerSemanticImportanceValidator()
    
    # è¿è¡Œå®Œæ•´éªŒè¯
    results = validator.run_complete_validation()
    
    logger.info("ğŸ‰ å±‚çº§è¯­ä¹‰é‡è¦æ€§éªŒè¯å®éªŒå®Œæˆï¼")
    logger.info(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {validator.results_dir}")

if __name__ == "__main__":
    main()
