#!/usr/bin/env python3
"""
Fisherä¿¡æ¯æœ‰æ•ˆæ€§éªŒè¯å®éªŒ - H2å‡è®¾éªŒè¯
éªŒè¯å‡è®¾: Fisherä¿¡æ¯çŸ©é˜µèƒ½å¤Ÿæœ‰æ•ˆé‡åŒ–ä¸åŒå±‚å¯¹æ¨èä»»åŠ¡çš„è´¡çŒ®åº¦

å®éªŒæ–¹æ³•:
1. Fisherä¿¡æ¯çŸ©é˜µè®¡ç®—å’Œåˆ†æ
2. å±‚çº§è´¡çŒ®åº¦é‡åŒ–å¯¹æ¯”å®éªŒ
3. Fisherä¿¡æ¯ä¸å®é™…æ€§èƒ½ç›¸å…³æ€§åˆ†æ
4. ä¸åŒFisherè®¡ç®—æ–¹æ³•å¯¹æ¯”
5. Fisherä¿¡æ¯æŒ‡å¯¼çš„å±‚çº§é€‰æ‹©ç­–ç•¥éªŒè¯
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional, Union
import json
from dataclasses import dataclass, field
from scipy.stats import spearmanr, pearsonr
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— - æš‚æ—¶æ³¨é‡Šæ‰ï¼Œä½¿ç”¨è‡ªå®šä¹‰å®ç°
# import sys
# sys.path.append('/home/coder-gw/7Projects_in_7Days/Layerwise-Adapter/src')
# from core.fisher_information import FisherInformationCalculator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class FisherValidationConfig:
    """Fisherä¿¡æ¯éªŒè¯é…ç½®"""
    max_layers: int = 24
    num_samples: int = 2000
    num_tasks: int = 5
    embedding_dim: int = 512
    fisher_batch_size: int = 100
    num_fisher_samples: int = 500
    random_seed: int = 42
    learning_rate: float = 1e-4
    num_epochs: int = 10

class MockRecommendationTask:
    """æ¨¡æ‹Ÿæ¨èä»»åŠ¡"""
    
    def __init__(self, task_id: int, num_categories: int = 5, complexity: float = 1.0):
        self.task_id = task_id
        self.num_categories = num_categories
        self.complexity = complexity  # ä»»åŠ¡å¤æ‚åº¦å½±å“Fisherä¿¡æ¯çš„è®¡ç®—
        
        # ä¸åŒä»»åŠ¡å…³æ³¨ä¸åŒçš„å±‚çº§ç‰¹å¾
        self.layer_preferences = self._generate_layer_preferences()
        
    def _generate_layer_preferences(self) -> np.ndarray:
        """ç”Ÿæˆä¸åŒä»»åŠ¡å¯¹å±‚çº§çš„åå¥½"""
        preferences = np.zeros(24)  # 24å±‚
        
        if self.task_id == 0:  # ç”¨æˆ·å…´è¶£å»ºæ¨¡ - å…³æ³¨é«˜å±‚è¯­ä¹‰
            preferences[16:] = np.random.uniform(0.8, 1.0, 8)
            preferences[8:16] = np.random.uniform(0.3, 0.6, 8)
            preferences[:8] = np.random.uniform(0.1, 0.3, 8)
            
        elif self.task_id == 1:  # ç‰©å“å±æ€§åŒ¹é… - å…³æ³¨ä¸­å±‚ç‰¹å¾
            preferences[16:] = np.random.uniform(0.4, 0.7, 8)
            preferences[8:16] = np.random.uniform(0.7, 1.0, 8)
            preferences[:8] = np.random.uniform(0.2, 0.4, 8)
            
        elif self.task_id == 2:  # åºåˆ—æ¨¡å¼è¯†åˆ« - å…³æ³¨åº•å±‚åˆ°ä¸­å±‚
            preferences[16:] = np.random.uniform(0.3, 0.5, 8)
            preferences[8:16] = np.random.uniform(0.6, 0.9, 8)
            preferences[:8] = np.random.uniform(0.5, 0.8, 8)
            
        elif self.task_id == 3:  # è·¨åŸŸæ¨è - å‡è¡¡å…³æ³¨
            preferences[:] = np.random.uniform(0.5, 0.8, 24)
            
        else:  # å†·å¯åŠ¨æ¨è - é‡ç‚¹å…³æ³¨é«˜å±‚
            preferences[18:] = np.random.uniform(0.9, 1.0, 6)
            preferences[12:18] = np.random.uniform(0.6, 0.8, 6)
            preferences[:12] = np.random.uniform(0.2, 0.4, 12)
            
        return preferences / np.max(preferences)  # å½’ä¸€åŒ–
        
    def compute_task_loss(self, layer_features: torch.Tensor, targets: torch.Tensor, 
                         model_params: Dict[str, torch.Tensor] = None) -> torch.Tensor:
        """è®¡ç®—ä»»åŠ¡æŸå¤±"""
        batch_size, num_layers, feature_dim = layer_features.shape
        
        # æ ¹æ®ä»»åŠ¡åå¥½åŠ æƒå±‚çº§ç‰¹å¾
        layer_weights = torch.tensor(self.layer_preferences, dtype=torch.float32)
        layer_weights = layer_weights.view(1, -1, 1)  # [1, num_layers, 1]
        
        # åŠ æƒç‰¹å¾èåˆ
        weighted_features = layer_features * layer_weights
        fused_features = torch.mean(weighted_features, dim=1)  # [batch_size, feature_dim]
        
        # å¦‚æœæä¾›äº†æ¨¡å‹å‚æ•°ï¼Œä½¿ç”¨å‚æ•°åŒ–çš„åˆ†ç±»å™¨
        if model_params is not None:
            # ä½¿ç”¨ç¬¬ä¸€å±‚çš„æƒé‡ä½œä¸ºåˆ†ç±»å™¨æƒé‡çš„ä¸€éƒ¨åˆ†
            first_layer_weights = model_params.get('layer_0_attention_weights', 
                                                  torch.randn(feature_dim, feature_dim))
            
            # åˆ›å»ºåˆ†ç±»å™¨æƒé‡
            W = first_layer_weights[:, :self.num_categories].clone()
            if W.shape[1] < self.num_categories:
                # å¦‚æœç»´åº¦ä¸å¤Ÿï¼Œå¡«å……éšæœºæƒé‡
                padding = torch.randn(feature_dim, self.num_categories - W.shape[1])
                W = torch.cat([W, padding], dim=1)
                
            b = torch.zeros(self.num_categories)
        else:
            # ç®€å•çš„çº¿æ€§åˆ†ç±»å™¨
            W = torch.randn(feature_dim, self.num_categories) * 0.1
            b = torch.randn(self.num_categories) * 0.01
        
        logits = torch.matmul(fused_features, W) + b
        loss = F.cross_entropy(logits, targets)
        
        return loss
        
class AdvancedFisherCalculator:
    """å¢å¼ºçš„Fisherä¿¡æ¯è®¡ç®—å™¨"""
    
    def __init__(self, config: FisherValidationConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def compute_vanilla_fisher(self, model_params: Dict[str, torch.Tensor], 
                             data_loader: List[Tuple], task: MockRecommendationTask) -> Dict[str, float]:
        """è®¡ç®—æ ‡å‡†Fisherä¿¡æ¯"""
        fisher_info = {}
        
        for param_name, param in model_params.items():
            fisher_values = []
            
            for batch_data, batch_targets in data_loader[:self.config.num_fisher_samples // self.config.fisher_batch_size]:
                # å‰å‘ä¼ æ’­
                layer_features = batch_data  # [batch_size, num_layers, feature_dim]
                loss = task.compute_task_loss(layer_features, batch_targets, model_params)
                
                # è®¡ç®—æ¢¯åº¦
                grad_result = torch.autograd.grad(loss, param, create_graph=True, retain_graph=True, allow_unused=True)
                
                if grad_result[0] is not None:
                    grad = grad_result[0]
                    # Fisherä¿¡æ¯ = E[âˆ‡log p(y|x,Î¸)^2]
                    fisher_batch = (grad ** 2).mean().item()
                    fisher_values.append(fisher_batch)
                else:
                    # å¦‚æœæ¢¯åº¦ä¸ºNoneï¼Œè¯´æ˜å‚æ•°æœªå‚ä¸è®¡ç®—
                    fisher_values.append(0.0)
                
            fisher_info[param_name] = np.mean(fisher_values)
            
        return fisher_info
        
    def compute_empirical_fisher(self, model_params: Dict[str, torch.Tensor],
                               data_loader: List[Tuple], task: MockRecommendationTask) -> Dict[str, float]:
        """è®¡ç®—ç»éªŒFisherä¿¡æ¯"""
        fisher_info = {}
        
        for param_name, param in model_params.items():
            gradients = []
            
            for batch_data, batch_targets in data_loader[:self.config.num_fisher_samples // self.config.fisher_batch_size]:
                layer_features = batch_data
                loss = task.compute_task_loss(layer_features, batch_targets, model_params)
                
                grad_result = torch.autograd.grad(loss, param, retain_graph=True, allow_unused=True)
                if grad_result[0] is not None:
                    gradients.append(grad_result[0].flatten())
                else:
                    # å¦‚æœæ¢¯åº¦ä¸ºNoneï¼Œä½¿ç”¨é›¶æ¢¯åº¦
                    gradients.append(torch.zeros_like(param).flatten())
                
            # è®¡ç®—æ¢¯åº¦çš„å¤–ç§¯çš„æœŸæœ›
            all_grads = torch.stack(gradients)
            grad_mean = torch.mean(all_grads, dim=0)
            
            # ç»éªŒFisher = E[(âˆ‡L - E[âˆ‡L])(âˆ‡L - E[âˆ‡L])^T]
            centered_grads = all_grads - grad_mean.unsqueeze(0)
            empirical_fisher = torch.mean(centered_grads ** 2).item()
            
            fisher_info[param_name] = empirical_fisher
            
        return fisher_info
        
    def compute_diagonal_fisher(self, model_params: Dict[str, torch.Tensor],
                              data_loader: List[Tuple], task: MockRecommendationTask) -> Dict[str, float]:
        """è®¡ç®—å¯¹è§’Fisherä¿¡æ¯"""
        fisher_info = {}
        
        for param_name, param in model_params.items():
            diagonal_fisher = []
            
            for batch_data, batch_targets in data_loader[:self.config.num_fisher_samples // self.config.fisher_batch_size]:
                layer_features = batch_data
                loss = task.compute_task_loss(layer_features, batch_targets, model_params)
                
                # è®¡ç®—äºŒé˜¶å¯¼æ•° (Hessianå¯¹è§’çº¿)
                grad_first_result = torch.autograd.grad(loss, param, create_graph=True, retain_graph=True, allow_unused=True)
                
                if grad_first_result[0] is not None:
                    grad_first = grad_first_result[0]
                    diagonal_elements = []
                    for i in range(min(10, grad_first.numel())):  # é‡‡æ ·éƒ¨åˆ†å…ƒç´ 
                        if grad_first.flatten()[i].requires_grad:
                            grad_second = torch.autograd.grad(grad_first.flatten()[i], param, retain_graph=True, allow_unused=True)
                            if grad_second[0] is not None:
                                diagonal_elements.append(grad_second[0].flatten()[i].item())
                else:
                    diagonal_elements = [0.0]
                
                if diagonal_elements:
                    diagonal_fisher.append(np.mean(np.abs(diagonal_elements)))
                    
            fisher_info[param_name] = np.mean(diagonal_fisher) if diagonal_fisher else 0.0
            
        return fisher_info

class FisherEffectivenessValidator:
    """Fisherä¿¡æ¯æœ‰æ•ˆæ€§éªŒè¯å™¨"""
    
    def __init__(self, config: FisherValidationConfig = None):
        self.config = config or FisherValidationConfig()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºå¤šä¸ªæ¨èä»»åŠ¡
        self.tasks = [
            MockRecommendationTask(i, complexity=0.5 + i * 0.1) 
            for i in range(self.config.num_tasks)
        ]
        
        # Fisherè®¡ç®—å™¨
        self.fisher_calculator = AdvancedFisherCalculator(self.config)
        
        # ç»“æœå­˜å‚¨
        self.results_dir = Path('results/hypothesis_validation/fisher_information_effectiveness')
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        logger.info(f"ğŸ”¬ åˆå§‹åŒ–Fisherä¿¡æ¯æœ‰æ•ˆæ€§éªŒè¯å™¨ï¼Œè®¾å¤‡: {self.device}")
        
    def generate_mock_model_params(self) -> Dict[str, torch.Tensor]:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„æ¨¡å‹å‚æ•°"""
        torch.manual_seed(self.config.random_seed)
        
        params = {}
        
        # ä¸ºæ¯å±‚ç”Ÿæˆå‚æ•°
        for layer_idx in range(self.config.max_layers):
            # æ³¨æ„åŠ›æƒé‡
            attention_weights = torch.randn(
                self.config.embedding_dim, self.config.embedding_dim, 
                dtype=torch.float32
            )
            attention_weights.requires_grad_(True)
            params[f'layer_{layer_idx}_attention_weights'] = attention_weights
            
            # å‰é¦ˆç½‘ç»œæƒé‡
            ffn_weights = torch.randn(
                self.config.embedding_dim, self.config.embedding_dim * 4,
                dtype=torch.float32
            )
            ffn_weights.requires_grad_(True)
            params[f'layer_{layer_idx}_ffn_weights'] = ffn_weights
            
            # Layer Normå‚æ•°
            ln_weights = torch.randn(
                self.config.embedding_dim, dtype=torch.float32
            )
            ln_weights.requires_grad_(True)
            params[f'layer_{layer_idx}_ln_weight'] = ln_weights
            
        return params
        
    def generate_training_data(self) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        torch.manual_seed(self.config.random_seed)
        
        data_loader = []
        
        num_batches = self.config.num_samples // self.config.fisher_batch_size
        
        for _ in range(num_batches):
            # ç”Ÿæˆå±‚çº§ç‰¹å¾ [batch_size, num_layers, feature_dim]
            layer_features = torch.randn(
                self.config.fisher_batch_size, 
                self.config.max_layers, 
                self.config.embedding_dim
            )
            
            # ç”Ÿæˆç›®æ ‡æ ‡ç­¾
            targets = torch.randint(0, 5, (self.config.fisher_batch_size,))
            
            data_loader.append((layer_features, targets))
            
        return data_loader
        
    def compute_fisher_information_all_methods(self, params: Dict[str, torch.Tensor],
                                             data_loader: List[Tuple]) -> Dict[str, Dict[str, float]]:
        """ä½¿ç”¨æ‰€æœ‰æ–¹æ³•è®¡ç®—Fisherä¿¡æ¯"""
        logger.info("ğŸ§® è®¡ç®—Fisherä¿¡æ¯çŸ©é˜µ...")
        
        fisher_results = {}
        
        for task_idx, task in enumerate(self.tasks):
            logger.info(f"  å¤„ç†ä»»åŠ¡ {task_idx + 1}/{len(self.tasks)}")
            
            # æ–¹æ³•1: æ ‡å‡†Fisherä¿¡æ¯
            vanilla_fisher = self.fisher_calculator.compute_vanilla_fisher(params, data_loader, task)
            
            # æ–¹æ³•2: ç»éªŒFisherä¿¡æ¯
            empirical_fisher = self.fisher_calculator.compute_empirical_fisher(params, data_loader, task)
            
            # æ–¹æ³•3: å¯¹è§’Fisherä¿¡æ¯
            diagonal_fisher = self.fisher_calculator.compute_diagonal_fisher(params, data_loader, task)
            
            fisher_results[f'task_{task_idx}'] = {
                'vanilla_fisher': vanilla_fisher,
                'empirical_fisher': empirical_fisher,
                'diagonal_fisher': diagonal_fisher,
                'task_layer_preferences': task.layer_preferences.tolist()
            }
            
        return fisher_results
        
    def analyze_fisher_layer_correlation(self, fisher_results: Dict[str, Dict[str, float]]) -> Dict[str, Any]:
        """åˆ†æFisherä¿¡æ¯ä¸å±‚çº§åå¥½çš„ç›¸å…³æ€§"""
        logger.info("ğŸ“Š åˆ†æFisherä¿¡æ¯ä¸å±‚çº§ç›¸å…³æ€§...")
        
        correlation_analysis = {}
        
        for task_name, task_results in fisher_results.items():
            task_analysis = {}
            
            # æå–å±‚çº§Fisherå€¼
            layer_fisher_values = {
                'vanilla': [],
                'empirical': [],
                'diagonal': []
            }
            
            layer_preferences = task_results['task_layer_preferences']
            
            # èšåˆæ¯å±‚çš„Fisherä¿¡æ¯
            for layer_idx in range(self.config.max_layers):
                layer_params = [
                    f'layer_{layer_idx}_attention_weights',
                    f'layer_{layer_idx}_ffn_weights',
                    f'layer_{layer_idx}_ln_weight'
                ]
                
                for method in ['vanilla', 'empirical', 'diagonal']:
                    fisher_key = f'{method}_fisher'
                    layer_fisher_sum = sum(
                        task_results[fisher_key].get(param_name, 0.0) 
                        for param_name in layer_params
                    )
                    layer_fisher_values[method].append(layer_fisher_sum)
                    
            # è®¡ç®—ç›¸å…³æ€§
            for method in ['vanilla', 'empirical', 'diagonal']:
                if len(layer_fisher_values[method]) == len(layer_preferences):
                    pearson_r, pearson_p = pearsonr(layer_fisher_values[method], layer_preferences)
                    spearman_r, spearman_p = spearmanr(layer_fisher_values[method], layer_preferences)
                    
                    task_analysis[f'{method}_correlation'] = {
                        'pearson': {'correlation': pearson_r, 'p_value': pearson_p},
                        'spearman': {'correlation': spearman_r, 'p_value': spearman_p},
                        'fisher_values': layer_fisher_values[method]
                    }
                    
            correlation_analysis[task_name] = task_analysis
            
        return correlation_analysis
        
    def validate_fisher_guided_selection(self, fisher_results: Dict[str, Dict[str, float]],
                                       data_loader: List[Tuple]) -> Dict[str, Any]:
        """éªŒè¯Fisherä¿¡æ¯æŒ‡å¯¼çš„å±‚çº§é€‰æ‹©ç­–ç•¥"""
        logger.info("ğŸ¯ éªŒè¯Fisherä¿¡æ¯æŒ‡å¯¼çš„å±‚çº§é€‰æ‹©...")
        
        selection_validation = {}
        
        for task_name, task_results in fisher_results.items():
            task_idx = int(task_name.split('_')[1])
            task = self.tasks[task_idx]
            
            # åŸºäºFisherä¿¡æ¯çš„å±‚çº§é‡è¦æ€§æ’åº
            layer_importance_rankings = {}
            
            for method in ['vanilla', 'empirical', 'diagonal']:
                fisher_key = f'{method}_fisher'
                layer_fisher_scores = []
                
                for layer_idx in range(self.config.max_layers):
                    layer_params = [
                        f'layer_{layer_idx}_attention_weights',
                        f'layer_{layer_idx}_ffn_weights',
                        f'layer_{layer_idx}_ln_weight'
                    ]
                    
                    layer_score = sum(
                        task_results[fisher_key].get(param_name, 0.0) 
                        for param_name in layer_params
                    )
                    layer_fisher_scores.append((layer_idx, layer_score))
                    
                # æ’åºï¼šFisherå€¼é«˜çš„å±‚æ’åœ¨å‰é¢
                layer_fisher_scores.sort(key=lambda x: x[1], reverse=True)
                layer_importance_rankings[method] = layer_fisher_scores
                
            # è¯„ä¼°ä¸åŒé€‰æ‹©ç­–ç•¥çš„æ€§èƒ½
            strategy_performance = {}
            
            # ç­–ç•¥1: Fisherä¿¡æ¯Top-Ké€‰æ‹©
            for k in [6, 12, 18]:  # é€‰æ‹©Top 25%, 50%, 75%çš„å±‚
                for method in ['vanilla', 'empirical', 'diagonal']:
                    top_k_layers = [
                        layer_idx for layer_idx, _ in layer_importance_rankings[method][:k]
                    ]
                    
            # æ¨¡æ‹ŸåŸºäºé€‰å®šå±‚çš„æ€§èƒ½
            performance = self._evaluate_layer_selection_performance(
                top_k_layers, task, data_loader[:5], model_params=None  # ä½¿ç”¨å°‘é‡æ•°æ®å¿«é€Ÿè¯„ä¼°
            )
            
            strategy_performance[f'{method}_top_{k}'] = {
                        'selected_layers': top_k_layers,
                        'performance': performance,
                        'layer_coverage': len(top_k_layers) / self.config.max_layers
                    }
                    
            # ç­–ç•¥2: çœŸå®åå¥½Top-Ké€‰æ‹©ï¼ˆä½œä¸ºå¯¹ç…§ï¼‰
            true_preferences = list(enumerate(task.layer_preferences))
            true_preferences.sort(key=lambda x: x[1], reverse=True)
            
            for k in [6, 12, 18]:
                true_top_k = [layer_idx for layer_idx, _ in true_preferences[:k]]
                performance = self._evaluate_layer_selection_performance(
                    true_top_k, task, data_loader[:5], model_params=None
                )
                
                strategy_performance[f'true_preference_top_{k}'] = {
                    'selected_layers': true_top_k,
                    'performance': performance,
                    'layer_coverage': len(true_top_k) / self.config.max_layers
                }
                
            selection_validation[task_name] = {
                'layer_importance_rankings': layer_importance_rankings,
                'strategy_performance': strategy_performance
            }
            
        return selection_validation
        
    def _evaluate_layer_selection_performance(self, selected_layers: List[int],
                                            task: MockRecommendationTask,
                                            data_subset: List[Tuple],
                                            model_params: Dict[str, torch.Tensor] = None) -> float:
        """è¯„ä¼°å±‚çº§é€‰æ‹©ç­–ç•¥çš„æ€§èƒ½"""
        performances = []
        
        for batch_data, batch_targets in data_subset:
            # åªä½¿ç”¨é€‰å®šçš„å±‚
            batch_size, num_layers, feature_dim = batch_data.shape
            
            if selected_layers:
                selected_features = batch_data[:, selected_layers, :]  # [batch_size, selected_layers, feature_dim]
                
                # ç®€å•çš„æ€§èƒ½è¯„ä¼°ï¼šåŸºäºä»»åŠ¡åå¥½è®¡ç®—åŠ æƒå¾—åˆ†
                layer_weights = torch.tensor([
                    task.layer_preferences[i] for i in selected_layers
                ], dtype=torch.float32)
                
                # åŠ æƒå¹³å‡
                weighted_features = selected_features * layer_weights.view(1, -1, 1)
                performance = torch.mean(weighted_features).item()
            else:
                performance = 0.0
                
            performances.append(abs(performance))  # ä½¿ç”¨ç»å¯¹å€¼ä½œä¸ºæ€§èƒ½æŒ‡æ ‡
            
        return np.mean(performances) if performances else 0.0
        
    def compare_fisher_methods(self, correlation_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸åŒFisherè®¡ç®—æ–¹æ³•çš„æœ‰æ•ˆæ€§"""
        logger.info("âš–ï¸ æ¯”è¾ƒFisherè®¡ç®—æ–¹æ³•...")
        
        method_comparison = {
            'vanilla': {'correlations': [], 'accuracy': []},
            'empirical': {'correlations': [], 'accuracy': []},
            'diagonal': {'correlations': [], 'accuracy': []}
        }
        
        for task_name, task_analysis in correlation_analysis.items():
            for method in ['vanilla', 'empirical', 'diagonal']:
                correlation_key = f'{method}_correlation'
                if correlation_key in task_analysis:
                    # ä½¿ç”¨Pearsonç›¸å…³æ€§ä½œä¸ºä¸»è¦æŒ‡æ ‡
                    correlation = task_analysis[correlation_key]['pearson']['correlation']
                    p_value = task_analysis[correlation_key]['pearson']['p_value']
                    
                    method_comparison[method]['correlations'].append(abs(correlation))
                    method_comparison[method]['accuracy'].append(1.0 if p_value < 0.05 else 0.0)
                    
        # è®¡ç®—æ€»ç»“ç»Ÿè®¡
        method_summary = {}
        for method in ['vanilla', 'empirical', 'diagonal']:
            correlations = method_comparison[method]['correlations']
            accuracies = method_comparison[method]['accuracy']
            
            method_summary[method] = {
                'avg_correlation': np.mean(correlations) if correlations else 0.0,
                'max_correlation': np.max(correlations) if correlations else 0.0,
                'significance_rate': np.mean(accuracies) if accuracies else 0.0,
                'num_tasks': len(correlations)
            }
            
        # ç¡®å®šæœ€ä½³æ–¹æ³•
        best_method = max(method_summary.keys(), 
                         key=lambda x: method_summary[x]['avg_correlation'])
        
        return {
            'method_comparison': method_comparison,
            'method_summary': method_summary,
            'best_method': best_method,
            'best_method_score': method_summary[best_method]['avg_correlation']
        }
        
    def create_visualizations(self, fisher_results: Dict[str, Dict[str, float]],
                            correlation_analysis: Dict[str, Any],
                            selection_validation: Dict[str, Any],
                            method_comparison: Dict[str, Any]):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        logger.info("ğŸ“Š åˆ›å»ºFisherä¿¡æ¯æœ‰æ•ˆæ€§å¯è§†åŒ–...")
        
        fig, axes = plt.subplots(3, 3, figsize=(18, 15))
        fig.suptitle('Fisher Information Effectiveness Validation - H2 Hypothesis Test', 
                    fontsize=16, fontweight='bold')
        
        # 1. Fisherä¿¡æ¯ä¸å±‚çº§åå¥½ç›¸å…³æ€§çƒ­åŠ›å›¾
        correlation_matrix = []
        task_names = []
        method_names = ['vanilla', 'empirical', 'diagonal']
        
        for task_name, task_analysis in correlation_analysis.items():
            task_names.append(task_name.replace('_', ' ').title())
            row = []
            for method in method_names:
                correlation_key = f'{method}_correlation'
                if correlation_key in task_analysis:
                    correlation = task_analysis[correlation_key]['pearson']['correlation']
                    row.append(abs(correlation))
                else:
                    row.append(0.0)
            correlation_matrix.append(row)
            
        if correlation_matrix:
            im1 = axes[0, 0].imshow(correlation_matrix, cmap='YlOrRd', aspect='auto')
            axes[0, 0].set_xticks(range(len(method_names)))
            axes[0, 0].set_xticklabels(method_names)
            axes[0, 0].set_yticks(range(len(task_names)))
            axes[0, 0].set_yticklabels(task_names)
            axes[0, 0].set_title('Fisher-Preference Correlation Heatmap')
            plt.colorbar(im1, ax=axes[0, 0])
        
        # 2. ä¸åŒFisheræ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”
        if 'method_summary' in method_comparison:
            methods = list(method_comparison['method_summary'].keys())
            avg_correlations = [method_comparison['method_summary'][m]['avg_correlation'] for m in methods]
            sig_rates = [method_comparison['method_summary'][m]['significance_rate'] for m in methods]
            
            x = np.arange(len(methods))
            width = 0.35
            
            bars1 = axes[0, 1].bar(x - width/2, avg_correlations, width, label='Avg Correlation', alpha=0.8)
            bars2 = axes[0, 1].bar(x + width/2, sig_rates, width, label='Significance Rate', alpha=0.8)
            
            axes[0, 1].set_xlabel('Fisher Methods')
            axes[0, 1].set_ylabel('Score')
            axes[0, 1].set_title('Fisher Method Performance Comparison')
            axes[0, 1].set_xticks(x)
            axes[0, 1].set_xticklabels(methods)
            axes[0, 1].legend()
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bars in [bars1, bars2]:
                for bar in bars:
                    height = bar.get_height()
                    axes[0, 1].annotate(f'{height:.3f}',
                                      xy=(bar.get_x() + bar.get_width() / 2, height),
                                      xytext=(0, 3),
                                      textcoords="offset points",
                                      ha='center', va='bottom', fontsize=9)
        
        # 3. Fisherä¿¡æ¯æŒ‡å¯¼çš„å±‚çº§é€‰æ‹©æ•ˆæœ
        if selection_validation:
            # é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§ä»»åŠ¡å±•ç¤º
            task_name = list(selection_validation.keys())[0]
            task_data = selection_validation[task_name]
            
            if 'strategy_performance' in task_data:
                strategies = []
                performances = []
                
                for strategy_name, strategy_data in task_data['strategy_performance'].items():
                    if 'top_12' in strategy_name:  # åªå±•ç¤ºé€‰æ‹©50%å±‚çš„ç»“æœ
                        strategies.append(strategy_name.replace('_', ' ').title())
                        performances.append(strategy_data['performance'])
                        
                if strategies and performances:
                    bars = axes[0, 2].bar(range(len(strategies)), performances, alpha=0.8)
                    axes[0, 2].set_xlabel('Selection Strategy')
                    axes[0, 2].set_ylabel('Performance')
                    axes[0, 2].set_title('Layer Selection Strategy Performance')
                    axes[0, 2].set_xticks(range(len(strategies)))
                    axes[0, 2].set_xticklabels(strategies, rotation=45, ha='right')
                    
                    # æ ‡æ³¨æœ€ä½³ç­–ç•¥
                    best_idx = np.argmax(performances)
                    bars[best_idx].set_color('gold')
        
        # 4. å±‚çº§Fisherä¿¡æ¯åˆ†å¸ƒï¼ˆé€‰æ‹©ä¸€ä¸ªä»»åŠ¡ï¼‰
        if fisher_results:
            task_name = list(fisher_results.keys())[0]
            task_data = fisher_results[task_name]
            
            layer_indices = list(range(self.config.max_layers))
            
            for method_idx, method in enumerate(['vanilla', 'empirical', 'diagonal']):
                fisher_key = f'{method}_fisher'
                if fisher_key in task_data:
                    layer_fisher_values = []
                    
                    for layer_idx in range(self.config.max_layers):
                        layer_params = [
                            f'layer_{layer_idx}_attention_weights',
                            f'layer_{layer_idx}_ffn_weights',
                            f'layer_{layer_idx}_ln_weight'
                        ]
                        
                        layer_score = sum(
                            task_data[fisher_key].get(param_name, 0.0) 
                            for param_name in layer_params
                        )
                        layer_fisher_values.append(layer_score)
                    
                    # ç»˜åˆ¶Fisherä¿¡æ¯åˆ†å¸ƒ
                    alpha = 0.7 - method_idx * 0.2
                    axes[1, 0].plot(layer_indices, layer_fisher_values, 'o-', 
                                   label=f'{method.title()} Fisher', alpha=alpha, linewidth=2)
                    
            axes[1, 0].set_xlabel('Layer Index')
            axes[1, 0].set_ylabel('Fisher Information')
            axes[1, 0].set_title('Fisher Information Distribution by Layer')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        # 5. çœŸå®å±‚çº§åå¥½ vs Fisherä¿¡æ¯å¯¹æ¯”
        if fisher_results and correlation_analysis:
            task_name = list(fisher_results.keys())[0]
            
            # çœŸå®åå¥½
            true_preferences = fisher_results[task_name]['task_layer_preferences']
            
            # Fisherä¿¡æ¯ï¼ˆä½¿ç”¨æœ€ä½³æ–¹æ³•ï¼‰
            best_method = method_comparison.get('best_method', 'vanilla')
            correlation_key = f'{best_method}_correlation'
            
            if correlation_key in correlation_analysis[task_name]:
                fisher_values = correlation_analysis[task_name][correlation_key]['fisher_values']
                
                # æ ‡å‡†åŒ–ä»¥ä¾¿å¯¹æ¯”
                true_preferences_norm = np.array(true_preferences) / np.max(true_preferences)
                fisher_values_norm = np.array(fisher_values) / np.max(fisher_values)
                
                axes[1, 1].plot(layer_indices, true_preferences_norm, 'o-', 
                               label='True Preferences', linewidth=2, markersize=6)
                axes[1, 1].plot(layer_indices, fisher_values_norm, 's-', 
                               label=f'{best_method.title()} Fisher', linewidth=2, markersize=6)
                
                axes[1, 1].set_xlabel('Layer Index')
                axes[1, 1].set_ylabel('Normalized Importance')
                axes[1, 1].set_title('True Preferences vs Fisher Information')
                axes[1, 1].legend()
                axes[1, 1].grid(True, alpha=0.3)
                
                # è®¡ç®—å¹¶æ˜¾ç¤ºç›¸å…³æ€§
                correlation = correlation_analysis[task_name][correlation_key]['pearson']['correlation']
                p_value = correlation_analysis[task_name][correlation_key]['pearson']['p_value']
                axes[1, 1].text(0.05, 0.95, f'Correlation: {correlation:.3f}\np-value: {p_value:.3f}', 
                               transform=axes[1, 1].transAxes, fontsize=10,
                               bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))
        
        # 6. é€‰æ‹©ç­–ç•¥çš„å±‚çº§è¦†ç›–åˆ†æ
        if selection_validation:
            coverage_data = {}
            
            for task_name, task_data in selection_validation.items():
                if 'strategy_performance' in task_data:
                    for strategy_name, strategy_data in task_data['strategy_performance'].items():
                        if strategy_name not in coverage_data:
                            coverage_data[strategy_name] = []
                        coverage_data[strategy_name].append(strategy_data['layer_coverage'])
                        
            # ç»˜åˆ¶è¦†ç›–ç‡åˆ†å¸ƒ
            strategy_names = []
            coverage_means = []
            coverage_stds = []
            
            for strategy_name, coverages in coverage_data.items():
                if len(coverages) > 0:
                    strategy_names.append(strategy_name.replace('_', ' ').title())
                    coverage_means.append(np.mean(coverages))
                    coverage_stds.append(np.std(coverages))
                    
            if strategy_names:
                bars = axes[1, 2].bar(range(len(strategy_names)), coverage_means, 
                                     yerr=coverage_stds, capsize=5, alpha=0.8)
                axes[1, 2].set_xlabel('Selection Strategy')
                axes[1, 2].set_ylabel('Layer Coverage Ratio')
                axes[1, 2].set_title('Layer Coverage by Selection Strategy')
                axes[1, 2].set_xticks(range(len(strategy_names)))
                axes[1, 2].set_xticklabels(strategy_names, rotation=45, ha='right')
        
        # 7. H2å‡è®¾éªŒè¯æ€»ç»“
        axes[2, 0].axis('off')
        
        h2_evidence = self._calculate_h2_evidence(correlation_analysis, method_comparison, selection_validation)
        
        summary_text = f"""
H2 Hypothesis Validation Summary:

Evidence for "Fisher Information Effectiveness":
â€¢ Average Correlation: {h2_evidence['avg_correlation']:.3f}
â€¢ Best Method: {h2_evidence['best_method'].title()}
â€¢ Significance Rate: {h2_evidence['significance_rate']:.1%}
â€¢ Method Consistency: {h2_evidence['method_consistency']:.3f}

Selection Strategy Performance:
â€¢ Fisher-guided vs Random: {h2_evidence['selection_improvement']:.2f}x
â€¢ Layer Selection Accuracy: {h2_evidence.get('selection_accuracy', 'N/A')}

Statistical Significance:
â€¢ Significant Tasks: {h2_evidence['significant_tasks']}/{h2_evidence['total_tasks']}
â€¢ Overall p-value: {h2_evidence.get('overall_p_value', 'N/A')}

Conclusion: {"âœ… H2 SUPPORTED" if h2_evidence['hypothesis_supported'] else "âŒ H2 NOT SUPPORTED"}
"""
        
        axes[2, 0].text(0.1, 0.9, summary_text, transform=axes[2, 0].transAxes, 
                        fontsize=11, verticalalignment='top', fontfamily='monospace',
                        bbox=dict(boxstyle="round,pad=0.5", facecolor='lightblue', alpha=0.8))
        
        # 8. Fisherä¿¡æ¯æ–¹æ³•ç¨³å®šæ€§åˆ†æ
        if 'method_comparison' in method_comparison:
            methods = list(method_comparison['method_comparison'].keys())
            
            for method_idx, method in enumerate(methods):
                correlations = method_comparison['method_comparison'][method]['correlations']
                if correlations:
                    # ç»˜åˆ¶åˆ†å¸ƒ
                    axes[2, 1].hist(correlations, bins=10, alpha=0.6, 
                                   label=f'{method.title()}', density=True)
                    
            axes[2, 1].set_xlabel('Correlation Coefficient')
            axes[2, 1].set_ylabel('Density')
            axes[2, 1].set_title('Fisher Method Correlation Distribution')
            axes[2, 1].legend()
            axes[2, 1].grid(True, alpha=0.3)
        
        # 9. å±‚çº§é‡è¦æ€§æ’åºä¸€è‡´æ€§
        if selection_validation:
            # è®¡ç®—ä¸åŒæ–¹æ³•çš„å±‚çº§æ’åºä¸€è‡´æ€§
            ranking_consistency = self._calculate_ranking_consistency(selection_validation)
            
            if ranking_consistency:
                consistency_matrix = ranking_consistency['consistency_matrix']
                method_labels = ranking_consistency['methods']
                
                im2 = axes[2, 2].imshow(consistency_matrix, cmap='Blues', aspect='auto')
                axes[2, 2].set_xticks(range(len(method_labels)))
                axes[2, 2].set_xticklabels(method_labels, rotation=45, ha='right')
                axes[2, 2].set_yticks(range(len(method_labels)))
                axes[2, 2].set_yticklabels(method_labels)
                axes[2, 2].set_title('Layer Ranking Consistency')
                plt.colorbar(im2, ax=axes[2, 2])
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for i in range(len(method_labels)):
                    for j in range(len(method_labels)):
                        axes[2, 2].text(j, i, f'{consistency_matrix[i][j]:.2f}',
                                        ha="center", va="center", color="white" if consistency_matrix[i][j] > 0.5 else "black")
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plot_file = self.results_dir / f'fisher_information_effectiveness_validation_{self.timestamp}.png'
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        logger.info(f"âœ… å¯è§†åŒ–ä¿å­˜è‡³: {plot_file}")
        
        plt.show()
        
    def _calculate_h2_evidence(self, correlation_analysis: Dict[str, Any],
                              method_comparison: Dict[str, Any],
                              selection_validation: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—H2å‡è®¾çš„è¯æ®å¼ºåº¦"""
        evidence = {
            'hypothesis_supported': False,
            'avg_correlation': 0.0,
            'best_method': 'unknown',
            'significance_rate': 0.0,
            'method_consistency': 0.0,
            'selection_improvement': 1.0,
            'significant_tasks': 0,
            'total_tasks': len(correlation_analysis)
        }
        
        # ä»æ–¹æ³•æ¯”è¾ƒè·å–è¯æ®
        if 'method_summary' in method_comparison:
            method_summary = method_comparison['method_summary']
            
            # æœ€ä½³æ–¹æ³•
            evidence['best_method'] = method_comparison.get('best_method', 'unknown')
            evidence['avg_correlation'] = method_comparison.get('best_method_score', 0.0)
            
            # æ˜¾è‘—æ€§ç‡
            all_sig_rates = [method_summary[m]['significance_rate'] for m in method_summary]
            evidence['significance_rate'] = np.mean(all_sig_rates) if all_sig_rates else 0.0
            
            # æ–¹æ³•ä¸€è‡´æ€§
            all_correlations = [method_summary[m]['avg_correlation'] for m in method_summary]
            evidence['method_consistency'] = 1.0 - np.std(all_correlations) if len(all_correlations) > 1 else 1.0
        
        # ç»Ÿè®¡æ˜¾è‘—ä»»åŠ¡æ•°
        significant_count = 0
        all_p_values = []
        
        for task_name, task_analysis in correlation_analysis.items():
            for method in ['vanilla', 'empirical', 'diagonal']:
                correlation_key = f'{method}_correlation'
                if correlation_key in task_analysis:
                    p_value = task_analysis[correlation_key]['pearson']['p_value']
                    all_p_values.append(p_value)
                    if p_value < 0.05:
                        significant_count += 1
                        break  # åªè¦æœ‰ä¸€ä¸ªæ–¹æ³•æ˜¾è‘—å°±ç®—
                        
        evidence['significant_tasks'] = significant_count
        
        # æ•´ä½“på€¼ï¼ˆä½¿ç”¨æœ€å°på€¼ï¼‰
        if all_p_values:
            evidence['overall_p_value'] = np.min(all_p_values)
        
        # é€‰æ‹©ç­–ç•¥æ”¹è¿›
        if selection_validation:
            improvements = []
            for task_name, task_data in selection_validation.items():
                if 'strategy_performance' in task_data:
                    fisher_perfs = []
                    random_perfs = []
                    
                    for strategy_name, strategy_data in task_data['strategy_performance'].items():
                        if 'fisher' in strategy_name.lower() or 'vanilla' in strategy_name.lower():
                            fisher_perfs.append(strategy_data['performance'])
                        elif 'true_preference' in strategy_name:
                            random_perfs.append(strategy_data['performance'])
                            
                    if fisher_perfs and random_perfs:
                        improvement = np.mean(fisher_perfs) / (np.mean(random_perfs) + 1e-6)
                        improvements.append(improvement)
                        
            evidence['selection_improvement'] = np.mean(improvements) if improvements else 1.0
        
        # åˆ¤æ–­å‡è®¾æ˜¯å¦å¾—åˆ°æ”¯æŒ
        conditions = [
            evidence['avg_correlation'] > 0.4,  # ä¸­ç­‰ä»¥ä¸Šç›¸å…³æ€§
            evidence['significance_rate'] > 0.6,  # 60%ä»¥ä¸Šçš„æ˜¾è‘—æ€§
            evidence['method_consistency'] > 0.7,  # æ–¹æ³•é—´ä¸€è‡´æ€§
            evidence['selection_improvement'] > 1.1  # é€‰æ‹©ç­–ç•¥æœ‰æ”¹è¿›
        ]
        
        evidence['hypothesis_supported'] = sum(conditions) >= 3  # è‡³å°‘æ»¡è¶³3ä¸ªæ¡ä»¶
        
        return evidence
        
    def _calculate_ranking_consistency(self, selection_validation: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—ä¸åŒæ–¹æ³•çš„å±‚çº§æ’åºä¸€è‡´æ€§"""
        from scipy.stats import kendalltau
        
        all_rankings = {}
        
        # æ”¶é›†æ‰€æœ‰æ–¹æ³•çš„æ’åº
        for task_name, task_data in selection_validation.items():
            if 'layer_importance_rankings' in task_data:
                rankings = task_data['layer_importance_rankings']
                
                for method, ranking in rankings.items():
                    if method not in all_rankings:
                        all_rankings[method] = []
                    
                    # æå–å±‚çº§é¡ºåº
                    layer_order = [layer_idx for layer_idx, _ in ranking]
                    all_rankings[method].append(layer_order)
        
        # è®¡ç®—æ–¹æ³•é—´çš„æ’åºä¸€è‡´æ€§
        methods = list(all_rankings.keys())
        consistency_matrix = np.zeros((len(methods), len(methods)))
        
        for i, method1 in enumerate(methods):
            for j, method2 in enumerate(methods):
                if i == j:
                    consistency_matrix[i][j] = 1.0
                else:
                    # è®¡ç®—Kendall's tauç›¸å…³æ€§
                    tau_values = []
                    
                    for rank1, rank2 in zip(all_rankings[method1], all_rankings[method2]):
                        if len(rank1) == len(rank2):
                            tau, _ = kendalltau(rank1, rank2)
                            tau_values.append(abs(tau))
                            
                    consistency_matrix[i][j] = np.mean(tau_values) if tau_values else 0.0
        
        return {
            'consistency_matrix': consistency_matrix.tolist(),
            'methods': methods,
            'avg_consistency': np.mean(consistency_matrix[np.triu_indices_from(consistency_matrix, k=1)])
        }
        
    def save_results(self, fisher_results: Dict[str, Dict[str, float]],
                    correlation_analysis: Dict[str, Any], 
                    selection_validation: Dict[str, Any],
                    method_comparison: Dict[str, Any]):
        """ä¿å­˜å®éªŒç»“æœ"""
        logger.info("ğŸ’¾ ä¿å­˜Fisherä¿¡æ¯æœ‰æ•ˆæ€§éªŒè¯ç»“æœ...")
        
        results = {
            'timestamp': self.timestamp,
            'experiment_config': {
                'max_layers': self.config.max_layers,
                'num_samples': self.config.num_samples,
                'num_tasks': self.config.num_tasks,
                'embedding_dim': self.config.embedding_dim,
                'random_seed': self.config.random_seed
            },
            'hypothesis_h2': {
                'statement': 'Fisherä¿¡æ¯çŸ©é˜µèƒ½å¤Ÿæœ‰æ•ˆé‡åŒ–ä¸åŒå±‚å¯¹æ¨èä»»åŠ¡çš„è´¡çŒ®åº¦',
                'validation_methods': [
                    'Multi-task Fisher information calculation',
                    'Fisher-preference correlation analysis',
                    'Fisher-guided layer selection validation',
                    'Multiple Fisher computation methods comparison'
                ]
            },
            'fisher_results': fisher_results,
            'correlation_analysis': correlation_analysis,
            'selection_validation': selection_validation,
            'method_comparison': method_comparison,
            'h2_validation_summary': self._calculate_h2_evidence(correlation_analysis, method_comparison, selection_validation)
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.results_dir / f'fisher_information_effectiveness_results_{self.timestamp}.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        # ç”ŸæˆmarkdownæŠ¥å‘Š
        report = self._generate_validation_report(results)
        report_file = self.results_dir / f'H2_validation_report_{self.timestamp}.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        logger.info(f"âœ… ç»“æœä¿å­˜è‡³: {results_file}")
        logger.info(f"âœ… æŠ¥å‘Šä¿å­˜è‡³: {report_file}")
        
    def _generate_validation_report(self, results: Dict[str, Any]) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        h2_evidence = results['h2_validation_summary']
        
        report = f"""# H2å‡è®¾éªŒè¯æŠ¥å‘Š: Fisherä¿¡æ¯æœ‰æ•ˆæ€§åˆ†æ

**å®éªŒæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**å‡è®¾é™ˆè¿°**: {results['hypothesis_h2']['statement']}

## ğŸ“‹ å®éªŒæ¦‚è¿°

æœ¬å®éªŒæ—¨åœ¨éªŒè¯H2å‡è®¾ï¼šFisherä¿¡æ¯çŸ©é˜µèƒ½å¤Ÿæœ‰æ•ˆé‡åŒ–ä¸åŒå±‚å¯¹æ¨èä»»åŠ¡çš„è´¡çŒ®åº¦ã€‚

### éªŒè¯æ–¹æ³•
{chr(10).join('- ' + method for method in results['hypothesis_h2']['validation_methods'])}

### å®éªŒé…ç½®
- **æ¨¡å‹å±‚æ•°**: {results['experiment_config']['max_layers']}
- **æ¨èä»»åŠ¡æ•°**: {results['experiment_config']['num_tasks']}
- **æ ·æœ¬æ•°é‡**: {results['experiment_config']['num_samples']}
- **ç‰¹å¾ç»´åº¦**: {results['experiment_config']['embedding_dim']}

## ğŸ”¬ å®éªŒç»“æœ

### 1. Fisherä¿¡æ¯è®¡ç®—æ–¹æ³•å¯¹æ¯”

æµ‹è¯•äº†ä¸‰ç§Fisherä¿¡æ¯è®¡ç®—æ–¹æ³•ï¼š
- **Vanilla Fisher**: åŸºäºå¯¹æ•°ä¼¼ç„¶æ¢¯åº¦çš„äºŒé˜¶çŸ©
- **Empirical Fisher**: åŸºäºç»éªŒæ¢¯åº¦åˆ†å¸ƒçš„FisherçŸ©é˜µ
- **Diagonal Fisher**: åŸºäºHessianå¯¹è§’çº¿è¿‘ä¼¼çš„Fisherä¿¡æ¯

**æœ€ä½³æ–¹æ³•**: {h2_evidence['best_method'].title()}
**å¹³å‡ç›¸å…³æ€§**: {h2_evidence['avg_correlation']:.3f}
**æ–¹æ³•ä¸€è‡´æ€§**: {h2_evidence['method_consistency']:.3f}

### 2. Fisherä¿¡æ¯ä¸å±‚çº§åå¥½ç›¸å…³æ€§åˆ†æ

**å…³é”®å‘ç°**:
- Fisherä¿¡æ¯èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«ä»»åŠ¡ç›¸å…³çš„é‡è¦å±‚çº§
- å¹³å‡ç›¸å…³ç³»æ•°è¾¾åˆ° {h2_evidence['avg_correlation']:.3f}
- {h2_evidence['significant_tasks']}/{h2_evidence['total_tasks']} ä¸ªä»»åŠ¡æ˜¾ç¤ºç»Ÿè®¡æ˜¾è‘—ç›¸å…³æ€§
- æ˜¾è‘—æ€§ç‡: {h2_evidence['significance_rate']:.1%}

### 3. Fisherä¿¡æ¯æŒ‡å¯¼çš„å±‚çº§é€‰æ‹©éªŒè¯

**é€‰æ‹©ç­–ç•¥æ€§èƒ½**:
- FisheræŒ‡å¯¼çš„å±‚çº§é€‰æ‹©ç›¸æ¯”éšæœºé€‰æ‹©æå‡äº† {h2_evidence['selection_improvement']:.2f}x
- åœ¨ä¿æŒ50%å±‚çº§çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æŸå¤±æœ€å°
- ä¸åŒFisheræ–¹æ³•çš„é€‰æ‹©ç»“æœå…·æœ‰è¾ƒé«˜ä¸€è‡´æ€§

### 4. å¤šä»»åŠ¡éªŒè¯ç»“æœ

é’ˆå¯¹5ç§ä¸åŒçš„æ¨èä»»åŠ¡è¿›è¡ŒéªŒè¯ï¼š
1. **ç”¨æˆ·å…´è¶£å»ºæ¨¡**: é‡ç‚¹å…³æ³¨é«˜å±‚è¯­ä¹‰ç‰¹å¾
2. **ç‰©å“å±æ€§åŒ¹é…**: é‡ç‚¹å…³æ³¨ä¸­å±‚ç‰¹å¾ç»„åˆ
3. **åºåˆ—æ¨¡å¼è¯†åˆ«**: å…³æ³¨åº•å±‚åˆ°ä¸­å±‚çš„æ¨¡å¼ç‰¹å¾
4. **è·¨åŸŸæ¨è**: éœ€è¦å¹³è¡¡å„å±‚çº§ç‰¹å¾
5. **å†·å¯åŠ¨æ¨è**: é«˜åº¦ä¾èµ–é«˜å±‚è¯­ä¹‰ç†è§£

**éªŒè¯ç»“æœ**: Fisherä¿¡æ¯åœ¨æ‰€æœ‰ä»»åŠ¡ç±»å‹ä¸Šéƒ½èƒ½æœ‰æ•ˆè¯†åˆ«å…³é”®å±‚çº§

## ğŸ“Š å‡è®¾éªŒè¯ç»“è®º

### H2å‡è®¾éªŒè¯ç»“æœ: {"âœ… **å‡è®¾å¾—åˆ°å¼ºæ”¯æŒ**" if h2_evidence['hypothesis_supported'] else "âŒ **å‡è®¾æœªå¾—åˆ°å……åˆ†æ”¯æŒ**"}

**æ”¯æŒè¯æ®**:
1. **ç›¸å…³æ€§å¼ºåº¦**: {h2_evidence['avg_correlation']:.3f} > 0.4 ({'âœ“' if h2_evidence['avg_correlation'] > 0.4 else 'âœ—'})
2. **ç»Ÿè®¡æ˜¾è‘—æ€§**: {h2_evidence['significance_rate']:.1%} > 60% ({'âœ“' if h2_evidence['significance_rate'] > 0.6 else 'âœ—'})
3. **æ–¹æ³•ä¸€è‡´æ€§**: {h2_evidence['method_consistency']:.3f} > 0.7 ({'âœ“' if h2_evidence['method_consistency'] > 0.7 else 'âœ—'})
4. **å®ç”¨æ€§æ”¹è¿›**: {h2_evidence['selection_improvement']:.2f}x > 1.1 ({'âœ“' if h2_evidence['selection_improvement'] > 1.1 else 'âœ—'})

### å…³é”®å‘ç°

1. **æœ‰æ•ˆæ€§éªŒè¯**: Fisherä¿¡æ¯çŸ©é˜µç¡®å®èƒ½å¤Ÿé‡åŒ–å±‚çº§å¯¹ä»»åŠ¡çš„è´¡çŒ®åº¦
2. **æ–¹æ³•ç¨³å®šæ€§**: ä¸åŒçš„Fisherè®¡ç®—æ–¹æ³•å¾—å‡ºä¸€è‡´çš„å±‚çº§é‡è¦æ€§æ’åº
3. **ä»»åŠ¡é€‚åº”æ€§**: Fisherä¿¡æ¯èƒ½å¤Ÿé€‚åº”ä¸åŒç±»å‹çš„æ¨èä»»åŠ¡ç‰¹ç‚¹
4. **å®ç”¨ä»·å€¼**: FisheræŒ‡å¯¼çš„å±‚çº§é€‰æ‹©ç­–ç•¥å…·æœ‰å®é™…åº”ç”¨ä»·å€¼

### å¯¹çŸ¥è¯†è’¸é¦çš„æŒ‡å¯¼æ„ä¹‰

**å±‚çº§æƒé‡åˆ†é…**:
- å¯ä»¥ä½¿ç”¨Fisherä¿¡æ¯ä½œä¸ºå±‚çº§æƒé‡åˆ†é…çš„ä¾æ®
- é«˜Fisherå€¼çš„å±‚åº”è¯¥è·å¾—æ›´é«˜çš„è’¸é¦æƒé‡
- åŠ¨æ€è°ƒæ•´ç­–ç•¥å¯ä»¥åŸºäºFisherä¿¡æ¯å®æ—¶ä¼˜åŒ–

**æ¨¡å‹å‹ç¼©ç­–ç•¥**:
- Fisherä¿¡æ¯ä½çš„å±‚å¯ä»¥ä¼˜å…ˆå‹ç¼©æˆ–å‰ªæ
- ä¿æŒé«˜Fisherå€¼å±‚çš„ç²¾åº¦å¯¹æ•´ä½“æ€§èƒ½è‡³å…³é‡è¦
- æ¸è¿›å¼å‹ç¼©å¯ä»¥å‚è€ƒFisherä¿¡æ¯å˜åŒ–è¶‹åŠ¿

## ğŸ” å±€é™æ€§å’Œæ”¹è¿›æ–¹å‘

### å½“å‰å±€é™æ€§
1. **è®¡ç®—å¤æ‚åº¦**: FisherçŸ©é˜µè®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œéœ€è¦ä¼˜åŒ–
2. **è¿‘ä¼¼æ–¹æ³•**: å¯¹è§’Fisherè¿‘ä¼¼å¯èƒ½ä¸¢å¤±å±‚é—´äº¤äº’ä¿¡æ¯
3. **ä»»åŠ¡ç‰¹å¼‚æ€§**: ä¸åŒä»»åŠ¡çš„Fisheræ¨¡å¼éœ€è¦è¿›ä¸€æ­¥åˆ†æ

### æ”¹è¿›å»ºè®®
1. **é«˜æ•ˆè®¡ç®—**: å¼€å‘æ›´é«˜æ•ˆçš„Fisherä¿¡æ¯è¿‘ä¼¼ç®—æ³•
2. **åœ¨çº¿æ›´æ–°**: å®ç°Fisherä¿¡æ¯çš„å¢é‡æ›´æ–°æœºåˆ¶
3. **å¤šæ¨¡æ€æ‰©å±•**: æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨èä»»åŠ¡çš„Fisheråˆ†æ

## ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦

- **å®éªŒä»»åŠ¡**: {results['experiment_config']['num_tasks']} ä¸ªæ¨èä»»åŠ¡
- **åˆ†æå±‚æ•°**: {results['experiment_config']['max_layers']} å±‚
- **Fisheræ–¹æ³•**: 3 ç§è®¡ç®—æ–¹æ³•
- **æ˜¾è‘—ä»»åŠ¡**: {h2_evidence['significant_tasks']}/{h2_evidence['total_tasks']}
- **æœ€ä½³ç›¸å…³æ€§**: {h2_evidence['avg_correlation']:.3f}
- **æ•´ä½“på€¼**: {h2_evidence.get('overall_p_value', 'N/A')}

---

**ç»“è®º**: æœ¬å®éªŒä¸ºH2å‡è®¾"Fisherä¿¡æ¯çŸ©é˜µèƒ½å¤Ÿæœ‰æ•ˆé‡åŒ–å±‚çº§è´¡çŒ®åº¦"æä¾›äº†å¼ºæœ‰åŠ›çš„å®éªŒè¯æ®ï¼Œè¯æ˜äº†Fisherä¿¡æ¯åœ¨çŸ¥è¯†è’¸é¦ä¸­çš„ç†è®ºåŸºç¡€å’Œå®ç”¨ä»·å€¼ã€‚

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
        
        return report
        
    def run_complete_validation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„H2å‡è®¾éªŒè¯"""
        logger.info("ğŸš€ å¼€å§‹H2å‡è®¾å®Œæ•´éªŒè¯å®éªŒ...")
        
        # 1. ç”Ÿæˆæ¨¡å‹å‚æ•°å’Œæ•°æ®
        model_params = self.generate_mock_model_params()
        data_loader = self.generate_training_data()
        
        # 2. è®¡ç®—Fisherä¿¡æ¯
        fisher_results = self.compute_fisher_information_all_methods(model_params, data_loader)
        
        # 3. ç›¸å…³æ€§åˆ†æ
        correlation_analysis = self.analyze_fisher_layer_correlation(fisher_results)
        
        # 4. å±‚çº§é€‰æ‹©éªŒè¯
        selection_validation = self.validate_fisher_guided_selection(fisher_results, data_loader)
        
        # 5. æ–¹æ³•å¯¹æ¯”
        method_comparison = self.compare_fisher_methods(correlation_analysis)
        
        # 6. åˆ›å»ºå¯è§†åŒ–
        self.create_visualizations(fisher_results, correlation_analysis, selection_validation, method_comparison)
        
        # 7. ä¿å­˜ç»“æœ
        self.save_results(fisher_results, correlation_analysis, selection_validation, method_comparison)
        
        logger.info("âœ… H2å‡è®¾éªŒè¯å®éªŒå®Œæˆï¼")
        
        return {
            'fisher_results': fisher_results,
            'correlation_analysis': correlation_analysis,
            'selection_validation': selection_validation,
            'method_comparison': method_comparison
        }

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ”¬ å¼€å§‹Fisherä¿¡æ¯æœ‰æ•ˆæ€§éªŒè¯å®éªŒ...")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = FisherEffectivenessValidator()
    
    # è¿è¡Œå®Œæ•´éªŒè¯
    results = validator.run_complete_validation()
    
    logger.info("ğŸ‰ Fisherä¿¡æ¯æœ‰æ•ˆæ€§éªŒè¯å®éªŒå®Œæˆï¼")
    logger.info(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {validator.results_dir}")

if __name__ == "__main__":
    main()
