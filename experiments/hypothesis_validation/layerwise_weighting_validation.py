#!/usr/bin/env python3
"""
å±‚çº§æƒé‡ç­–ç•¥éªŒè¯å®éªŒ - H3å‡è®¾éªŒè¯
éªŒè¯å‡è®¾: å±‚çº§åŒ–æƒé‡åˆ†é…ä¼˜äºå‡åŒ€æƒé‡åˆ†é…

å®éªŒæ–¹æ³•:
1. å¤šç§æƒé‡åˆ†é…ç­–ç•¥å¯¹æ¯”å®éªŒ
2. åŸºäºFisherä¿¡æ¯çš„è‡ªé€‚åº”æƒé‡ç®—æ³•
3. ä¸åŒæƒé‡ç­–ç•¥ä¸‹çš„æ¨èæ€§èƒ½è¯„ä¼°
4. æƒé‡ç­–ç•¥çš„æ”¶æ•›æ€§å’Œç¨³å®šæ€§åˆ†æ
5. è®¡ç®—æ•ˆç‡å’Œèµ„æºæ¶ˆè€—å¯¹æ¯”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import ndcg_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional, Union, Callable
import json
from dataclasses import dataclass, field
from scipy.stats import ttest_rel, wilcoxon
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class WeightingValidationConfig:
    """æƒé‡éªŒè¯é…ç½®"""
    max_layers: int = 24
    num_samples: int = 3000
    num_epochs: int = 50
    batch_size: int = 64
    embedding_dim: int = 512
    num_users: int = 1000
    num_items: int = 5000
    num_categories: int = 10
    learning_rate: float = 1e-3
    random_seed: int = 42
    validation_split: float = 0.2

class WeightingStrategy:
    """æƒé‡åˆ†é…ç­–ç•¥åŸºç±»"""
    
    def __init__(self, strategy_name: str, num_layers: int):
        self.strategy_name = strategy_name
        self.num_layers = num_layers
        
    def get_weights(self, **kwargs) -> torch.Tensor:
        """è·å–å±‚çº§æƒé‡"""
        raise NotImplementedError
        
    def update_weights(self, performance_feedback: Dict[str, float] = None):
        """æ ¹æ®æ€§èƒ½åé¦ˆæ›´æ–°æƒé‡"""
        pass

class UniformWeightingStrategy(WeightingStrategy):
    """å‡åŒ€æƒé‡åˆ†é…ç­–ç•¥"""
    
    def __init__(self, num_layers: int):
        super().__init__("Uniform", num_layers)
        
    def get_weights(self, **kwargs) -> torch.Tensor:
        """è¿”å›å‡åŒ€æƒé‡"""
        return torch.ones(self.num_layers) / self.num_layers

class LinearWeightingStrategy(WeightingStrategy):
    """çº¿æ€§é€’å¢æƒé‡åˆ†é…ç­–ç•¥"""
    
    def __init__(self, num_layers: int, increasing: bool = True):
        super().__init__(f"Linear_{'Inc' if increasing else 'Dec'}", num_layers)
        self.increasing = increasing
        
    def get_weights(self, **kwargs) -> torch.Tensor:
        """è¿”å›çº¿æ€§æƒé‡"""
        if self.increasing:
            weights = torch.linspace(0.1, 1.0, self.num_layers)
        else:
            weights = torch.linspace(1.0, 0.1, self.num_layers)
        return weights / weights.sum()

class ExponentialWeightingStrategy(WeightingStrategy):
    """æŒ‡æ•°æƒé‡åˆ†é…ç­–ç•¥"""
    
    def __init__(self, num_layers: int, base: float = 1.5, focus_high: bool = True):
        super().__init__(f"Exponential_{'High' if focus_high else 'Low'}", num_layers)
        self.base = base
        self.focus_high = focus_high
        
    def get_weights(self, **kwargs) -> torch.Tensor:
        """è¿”å›æŒ‡æ•°æƒé‡"""
        layer_indices = torch.arange(self.num_layers, dtype=torch.float32)
        
        if self.focus_high:
            weights = self.base ** layer_indices
        else:
            weights = self.base ** (self.num_layers - 1 - layer_indices)
            
        return weights / weights.sum()

class FisherBasedWeightingStrategy(WeightingStrategy):
    """åŸºäºFisherä¿¡æ¯çš„æƒé‡åˆ†é…ç­–ç•¥"""
    
    def __init__(self, num_layers: int, fisher_values: Optional[List[float]] = None):
        super().__init__("Fisher_Based", num_layers)
        self.fisher_values = fisher_values or [0.1] * num_layers
        self.adaptive_factor = 0.1
        
    def get_weights(self, fisher_info: Optional[Dict[str, float]] = None, **kwargs) -> torch.Tensor:
        """åŸºäºFisherä¿¡æ¯è¿”å›æƒé‡"""
        if fisher_info is not None:
            # ä»Fisherä¿¡æ¯æ›´æ–°æƒé‡
            self._update_from_fisher_info(fisher_info)
            
        weights = torch.tensor(self.fisher_values, dtype=torch.float32)
        weights = torch.clamp(weights, min=0.01)  # é¿å…æƒé‡ä¸º0
        return weights / weights.sum()
        
    def _update_from_fisher_info(self, fisher_info: Dict[str, float]):
        """ä»Fisherä¿¡æ¯æ›´æ–°å†…éƒ¨æƒé‡"""
        # èšåˆæ¯å±‚çš„Fisherä¿¡æ¯
        layer_fisher = [0.0] * self.num_layers
        
        for param_name, fisher_val in fisher_info.items():
            if 'layer_' in param_name:
                try:
                    layer_idx = int(param_name.split('_')[1])
                    if 0 <= layer_idx < self.num_layers:
                        layer_fisher[layer_idx] += fisher_val
                except (ValueError, IndexError):
                    continue
                    
        # å¹³æ»‘æ›´æ–°
        for i in range(self.num_layers):
            self.fisher_values[i] = (1 - self.adaptive_factor) * self.fisher_values[i] + \
                                  self.adaptive_factor * layer_fisher[i]

class AttentionBasedWeightingStrategy(WeightingStrategy):
    """åŸºäºæ³¨æ„åŠ›çš„æƒé‡åˆ†é…ç­–ç•¥"""
    
    def __init__(self, num_layers: int):
        super().__init__("Attention_Based", num_layers)
        self.attention_scores = torch.ones(num_layers) / num_layers
        
    def get_weights(self, attention_patterns: Optional[torch.Tensor] = None, **kwargs) -> torch.Tensor:
        """åŸºäºæ³¨æ„åŠ›æ¨¡å¼è¿”å›æƒé‡"""
        if attention_patterns is not None:
            # attention_patterns: [num_layers]
            self.attention_scores = F.softmax(attention_patterns, dim=0)
            
        return self.attention_scores

class AdaptiveWeightingStrategy(WeightingStrategy):
    """è‡ªé€‚åº”æƒé‡åˆ†é…ç­–ç•¥"""
    
    def __init__(self, num_layers: int, initial_strategy: str = "linear"):
        super().__init__("Adaptive", num_layers)
        
        # åˆå§‹åŒ–åŸºç¡€ç­–ç•¥
        if initial_strategy == "linear":
            self.base_weights = torch.linspace(0.1, 1.0, num_layers)
        elif initial_strategy == "exponential":
            self.base_weights = 1.5 ** torch.arange(num_layers, dtype=torch.float32)
        else:
            self.base_weights = torch.ones(num_layers)
            
        self.base_weights = self.base_weights / self.base_weights.sum()
        self.current_weights = self.base_weights.clone()
        self.performance_history = []
        
    def get_weights(self, **kwargs) -> torch.Tensor:
        """è¿”å›å½“å‰è‡ªé€‚åº”æƒé‡"""
        return self.current_weights
        
    def update_weights(self, performance_feedback: Dict[str, float] = None):
        """æ ¹æ®æ€§èƒ½åé¦ˆæ›´æ–°æƒé‡"""
        if performance_feedback is None:
            return
            
        current_performance = performance_feedback.get('accuracy', 0.0)
        self.performance_history.append(current_performance)
        
        if len(self.performance_history) > 2:
            # ç®€å•çš„è‡ªé€‚åº”ç­–ç•¥ï¼šå¦‚æœæ€§èƒ½ä¸‹é™ï¼Œå›é€€ä¸€äº›æƒé‡è°ƒæ•´
            recent_trend = np.mean(self.performance_history[-3:]) - np.mean(self.performance_history[-6:-3])
            
            if recent_trend < 0:  # æ€§èƒ½ä¸‹é™
                # å‘åŸºç¡€æƒé‡å›é€€
                self.current_weights = 0.9 * self.current_weights + 0.1 * self.base_weights
            else:  # æ€§èƒ½æå‡
                # å¢å¼ºé«˜å±‚æƒé‡
                high_layer_mask = torch.arange(self.num_layers) >= self.num_layers * 0.7
                adjustment = torch.where(high_layer_mask, 0.05, -0.02)
                self.current_weights += adjustment
                self.current_weights = torch.clamp(self.current_weights, min=0.01)
                self.current_weights = self.current_weights / self.current_weights.sum()

class MockRecommendationModel:
    """æ¨¡æ‹Ÿæ¨èæ¨¡å‹"""
    
    def __init__(self, config: WeightingValidationConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ¨¡æ‹Ÿä¸åŒå±‚çš„ç‰¹å¾
        self.layer_embeddings = nn.ModuleList([
            nn.Linear(config.embedding_dim, config.embedding_dim)
            for _ in range(config.max_layers)
        ]).to(self.device)
        
        # æœ€ç»ˆåˆ†ç±»å™¨
        self.classifier = nn.Linear(config.embedding_dim, config.num_categories).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(self.parameters(), lr=config.learning_rate)
        
    def parameters(self):
        """è·å–æ‰€æœ‰å‚æ•°"""
        params = []
        for layer in self.layer_embeddings:
            params.extend(layer.parameters())
        params.extend(self.classifier.parameters())
        return params
        
    def forward(self, x: torch.Tensor, layer_weights: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # x: [batch_size, embedding_dim]
        # layer_weights: [num_layers]
        
        layer_outputs = []
        current_x = x
        
        # é€å±‚å¤„ç†
        for i, layer in enumerate(self.layer_embeddings):
            current_x = torch.tanh(layer(current_x))  # éçº¿æ€§æ¿€æ´»
            layer_outputs.append(current_x)
            
        # åŠ æƒèšåˆ
        layer_outputs = torch.stack(layer_outputs, dim=1)  # [batch_size, num_layers, embedding_dim]
        layer_weights = layer_weights.to(self.device).view(1, -1, 1)  # [1, num_layers, 1]
        
        weighted_output = torch.sum(layer_outputs * layer_weights, dim=1)  # [batch_size, embedding_dim]
        
        # åˆ†ç±»
        logits = self.classifier(weighted_output)
        return logits
        
    def compute_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—æŸå¤±"""
        return F.cross_entropy(logits, targets)
        
    def train_step(self, x: torch.Tensor, targets: torch.Tensor, 
                   layer_weights: torch.Tensor) -> Dict[str, float]:
        """è®­ç»ƒæ­¥éª¤"""
        self.optimizer.zero_grad()
        
        logits = self.forward(x, layer_weights)
        loss = self.compute_loss(logits, targets)
        
        loss.backward()
        self.optimizer.step()
        
        # è®¡ç®—å‡†ç¡®ç‡
        predictions = torch.argmax(logits, dim=1)
        accuracy = (predictions == targets).float().mean().item()
        
        return {
            'loss': loss.item(),
            'accuracy': accuracy
        }
        
    def evaluate(self, x: torch.Tensor, targets: torch.Tensor,
                 layer_weights: torch.Tensor) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        with torch.no_grad():
            logits = self.forward(x, layer_weights)
            loss = self.compute_loss(logits, targets)
            
            predictions = torch.argmax(logits, dim=1)
            accuracy = (predictions == targets).float().mean().item()
            
            # è®¡ç®—å…¶ä»–æŒ‡æ ‡
            pred_probs = F.softmax(logits, dim=1)
            
            return {
                'loss': loss.item(),
                'accuracy': accuracy,
                'predictions': predictions.cpu().numpy(),
                'probabilities': pred_probs.cpu().numpy()
            }

class LayerwiseWeightingValidator:
    """å±‚çº§æƒé‡ç­–ç•¥éªŒè¯å™¨"""
    
    def __init__(self, config: WeightingValidationConfig = None):
        self.config = config or WeightingValidationConfig()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–æƒé‡ç­–ç•¥
        self.strategies = {
            'uniform': UniformWeightingStrategy(self.config.max_layers),
            'linear_inc': LinearWeightingStrategy(self.config.max_layers, increasing=True),
            'linear_dec': LinearWeightingStrategy(self.config.max_layers, increasing=False),
            'exp_high': ExponentialWeightingStrategy(self.config.max_layers, focus_high=True),
            'exp_low': ExponentialWeightingStrategy(self.config.max_layers, focus_high=False),
            'fisher_based': FisherBasedWeightingStrategy(self.config.max_layers),
            'attention_based': AttentionBasedWeightingStrategy(self.config.max_layers),
            'adaptive': AdaptiveWeightingStrategy(self.config.max_layers)
        }
        
        # ç»“æœå­˜å‚¨
        self.results_dir = Path('results/hypothesis_validation/layerwise_weighting')
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        logger.info(f"ğŸ”¬ åˆå§‹åŒ–å±‚çº§æƒé‡ç­–ç•¥éªŒè¯å™¨ï¼Œè®¾å¤‡: {self.device}")
        
    def generate_training_data(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        torch.manual_seed(self.config.random_seed)
        np.random.seed(self.config.random_seed)
        
        # ç”Ÿæˆç”¨æˆ·-ç‰©å“äº¤äº’æ•°æ®
        user_features = torch.randn(self.config.num_samples, self.config.embedding_dim)
        
        # ç”Ÿæˆæ ‡ç­¾ï¼ˆæ¨¡æ‹Ÿæ¨èç±»åˆ«ï¼‰
        labels = torch.randint(0, self.config.num_categories, (self.config.num_samples,))
        
        # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
        split_idx = int(self.config.num_samples * (1 - self.config.validation_split))
        
        train_x = user_features[:split_idx]
        train_y = labels[:split_idx]
        val_x = user_features[split_idx:]
        val_y = labels[split_idx:]
        
        return train_x, train_y, val_x, val_y
        
    def train_with_strategy(self, strategy: WeightingStrategy, 
                          train_x: torch.Tensor, train_y: torch.Tensor,
                          val_x: torch.Tensor, val_y: torch.Tensor) -> Dict[str, Any]:
        """ä½¿ç”¨ç‰¹å®šç­–ç•¥è®­ç»ƒæ¨¡å‹"""
        logger.info(f"  ğŸƒ è®­ç»ƒç­–ç•¥: {strategy.strategy_name}")
        
        # åˆ›å»ºæ–°çš„æ¨¡å‹å®ä¾‹
        model = MockRecommendationModel(self.config)
        
        # è®­ç»ƒå†å²
        train_history = {
            'train_loss': [],
            'train_accuracy': [],
            'val_loss': [],
            'val_accuracy': [],
            'weights_history': []
        }
        
        # è®¡ç®—æ‰¹æ¬¡æ•°
        batch_size = self.config.batch_size
        num_batches = len(train_x) // batch_size
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.config.num_epochs):
            # è®­ç»ƒé˜¶æ®µ
            # model.train()  # ç®€åŒ–ç‰ˆæœ¬ä¸éœ€è¦è®­ç»ƒæ¨¡å¼
            epoch_train_loss = []
            epoch_train_acc = []
            
            # è·å–å½“å‰æƒé‡
            current_weights = strategy.get_weights()
            train_history['weights_history'].append(current_weights.numpy().copy())
            
            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(train_x))
                
                batch_x = train_x[start_idx:end_idx].to(self.device)
                batch_y = train_y[start_idx:end_idx].to(self.device)
                
                # è®­ç»ƒæ­¥éª¤
                train_metrics = model.train_step(batch_x, batch_y, current_weights)
                epoch_train_loss.append(train_metrics['loss'])
                epoch_train_acc.append(train_metrics['accuracy'])
                
            # éªŒè¯é˜¶æ®µ
            # model.eval()  # ç®€åŒ–ç‰ˆæœ¬ä¸éœ€è¦è¯„ä¼°æ¨¡å¼
            val_metrics = model.evaluate(val_x.to(self.device), val_y.to(self.device), current_weights)
            
            # è®°å½•å†å²
            train_history['train_loss'].append(np.mean(epoch_train_loss))
            train_history['train_accuracy'].append(np.mean(epoch_train_acc))
            train_history['val_loss'].append(val_metrics['loss'])
            train_history['val_accuracy'].append(val_metrics['accuracy'])
            
            # æ›´æ–°è‡ªé€‚åº”ç­–ç•¥
            if hasattr(strategy, 'update_weights'):
                strategy.update_weights({'accuracy': val_metrics['accuracy']})
                
            # å®šæœŸè¾“å‡ºè¿›åº¦
            if (epoch + 1) % 10 == 0:
                logger.info(f"    Epoch {epoch+1}/{self.config.num_epochs}: "
                           f"Train Acc: {np.mean(epoch_train_acc):.3f}, "
                           f"Val Acc: {val_metrics['accuracy']:.3f}")
                
        # æœ€ç»ˆè¯„ä¼°
        final_val_metrics = model.evaluate(val_x.to(self.device), val_y.to(self.device), current_weights)
        
        return {
            'strategy_name': strategy.strategy_name,
            'train_history': train_history,
            'final_performance': final_val_metrics,
            'final_weights': current_weights.numpy(),
            'model': model
        }
        
    def compare_all_strategies(self, train_x: torch.Tensor, train_y: torch.Tensor,
                             val_x: torch.Tensor, val_y: torch.Tensor) -> Dict[str, Any]:
        """æ¯”è¾ƒæ‰€æœ‰æƒé‡ç­–ç•¥"""
        logger.info("ğŸ”„ å¼€å§‹æƒé‡ç­–ç•¥å¯¹æ¯”å®éªŒ...")
        
        strategy_results = {}
        
        for strategy_name, strategy in self.strategies.items():
            logger.info(f"ğŸ“Š æµ‹è¯•ç­–ç•¥: {strategy_name}")
            
            try:
                result = self.train_with_strategy(strategy, train_x, train_y, val_x, val_y)
                strategy_results[strategy_name] = result
                
                logger.info(f"  âœ… {strategy_name} å®Œæˆï¼Œæœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: "
                           f"{result['final_performance']['accuracy']:.4f}")
                           
            except Exception as e:
                logger.error(f"  âŒ {strategy_name} è®­ç»ƒå¤±è´¥: {str(e)}")
                continue
                
        return strategy_results
        
    def analyze_strategy_performance(self, strategy_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç­–ç•¥æ€§èƒ½"""
        logger.info("ğŸ“ˆ åˆ†æç­–ç•¥æ€§èƒ½...")
        
        performance_analysis = {
            'strategy_rankings': [],
            'convergence_analysis': {},
            'stability_analysis': {},
            'efficiency_analysis': {}
        }
        
        # 1. æ€§èƒ½æ’å
        strategy_performances = []
        for strategy_name, result in strategy_results.items():
            final_acc = result['final_performance']['accuracy']
            strategy_performances.append((strategy_name, final_acc))
            
        strategy_performances.sort(key=lambda x: x[1], reverse=True)
        performance_analysis['strategy_rankings'] = strategy_performances
        
        # 2. æ”¶æ•›æ€§åˆ†æ
        for strategy_name, result in strategy_results.items():
            train_history = result['train_history']
            val_accuracies = train_history['val_accuracy']
            
            # è®¡ç®—æ”¶æ•›é€Ÿåº¦ï¼ˆè¾¾åˆ°90%æœ€ç»ˆæ€§èƒ½çš„epochæ•°ï¼‰
            final_acc = val_accuracies[-1]
            target_acc = final_acc * 0.9
            
            convergence_epoch = len(val_accuracies)
            for i, acc in enumerate(val_accuracies):
                if acc >= target_acc:
                    convergence_epoch = i + 1
                    break
                    
            # è®¡ç®—ç¨³å®šæ€§ï¼ˆæœ€å10ä¸ªepochçš„æ ‡å‡†å·®ï¼‰
            stability = np.std(val_accuracies[-10:]) if len(val_accuracies) >= 10 else np.std(val_accuracies)
            
            performance_analysis['convergence_analysis'][strategy_name] = {
                'convergence_epoch': convergence_epoch,
                'convergence_speed': convergence_epoch / len(val_accuracies),
                'final_accuracy': final_acc
            }
            
            performance_analysis['stability_analysis'][strategy_name] = {
                'accuracy_std': stability,
                'max_accuracy': max(val_accuracies),
                'min_accuracy': min(val_accuracies),
                'accuracy_range': max(val_accuracies) - min(val_accuracies)
            }
            
        return performance_analysis
        
    def statistical_significance_test(self, strategy_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•"""
        logger.info("ğŸ“Š æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•...")
        
        significance_results = {
            'pairwise_tests': {},
            'overall_anova': {},
            'effect_sizes': {}
        }
        
        # å‡†å¤‡æ•°æ®ï¼šæ¯ä¸ªç­–ç•¥çš„éªŒè¯å‡†ç¡®ç‡åºåˆ—
        strategy_accuracies = {}
        for strategy_name, result in strategy_results.items():
            val_accuracies = result['train_history']['val_accuracy']
            strategy_accuracies[strategy_name] = val_accuracies
            
        # æˆå¯¹tæ£€éªŒ
        strategy_names = list(strategy_accuracies.keys())
        for i, strategy1 in enumerate(strategy_names):
            for j, strategy2 in enumerate(strategy_names[i+1:], i+1):
                acc1 = strategy_accuracies[strategy1]
                acc2 = strategy_accuracies[strategy2]
                
                # ç¡®ä¿åºåˆ—é•¿åº¦ä¸€è‡´
                min_len = min(len(acc1), len(acc2))
                acc1 = acc1[:min_len]
                acc2 = acc2[:min_len]
                
                # tæ£€éªŒ
                t_stat, p_value = ttest_rel(acc1, acc2)
                
                # Wilcoxonç¬¦å·ç§©æ£€éªŒï¼ˆéå‚æ•°ï¼‰
                w_stat, w_p_value = wilcoxon(acc1, acc2)
                
                significance_results['pairwise_tests'][f'{strategy1}_vs_{strategy2}'] = {
                    't_statistic': t_stat,
                    't_p_value': p_value,
                    'wilcoxon_statistic': w_stat,
                    'wilcoxon_p_value': w_p_value,
                    'significant_t': p_value < 0.05,
                    'significant_w': w_p_value < 0.05,
                    'effect_size': np.mean(acc1) - np.mean(acc2)
                }
                
        # ANOVAæµ‹è¯•
        if len(strategy_accuracies) > 2:
            # å‡†å¤‡ANOVAæ•°æ®
            all_accuracies = []
            group_labels = []
            
            for strategy_name, accuracies in strategy_accuracies.items():
                all_accuracies.extend(accuracies)
                group_labels.extend([strategy_name] * len(accuracies))
                
            # æ‰§è¡Œå•å› ç´ ANOVA
            groups = [strategy_accuracies[name] for name in strategy_names]
            f_stat, anova_p_value = stats.f_oneway(*groups)
            
            significance_results['overall_anova'] = {
                'f_statistic': f_stat,
                'p_value': anova_p_value,
                'significant': anova_p_value < 0.05,
                'num_groups': len(strategy_names)
            }
            
        return significance_results
        
    def create_visualizations(self, strategy_results: Dict[str, Any],
                            performance_analysis: Dict[str, Any],
                            significance_results: Dict[str, Any]):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        logger.info("ğŸ“Š åˆ›å»ºæƒé‡ç­–ç•¥å¯è§†åŒ–...")
        
        fig, axes = plt.subplots(3, 3, figsize=(18, 15))
        fig.suptitle('Layerwise Weighting Strategy Validation - H3 Hypothesis Test', 
                    fontsize=16, fontweight='bold')
        
        # 1. è®­ç»ƒæ›²çº¿å¯¹æ¯”
        for strategy_name, result in strategy_results.items():
            train_history = result['train_history']
            epochs = range(1, len(train_history['val_accuracy']) + 1)
            axes[0, 0].plot(epochs, train_history['val_accuracy'], 
                           label=strategy_name, linewidth=2, alpha=0.8)
            
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Validation Accuracy')
        axes[0, 0].set_title('Training Curves Comparison')
        axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. æœ€ç»ˆæ€§èƒ½å¯¹æ¯”
        if 'strategy_rankings' in performance_analysis:
            rankings = performance_analysis['strategy_rankings']
            strategy_names = [item[0] for item in rankings]
            accuracies = [item[1] for item in rankings]
            
            bars = axes[0, 1].bar(range(len(strategy_names)), accuracies, alpha=0.8)
            axes[0, 1].set_xlabel('Strategy')
            axes[0, 1].set_ylabel('Final Validation Accuracy')
            axes[0, 1].set_title('Final Performance Comparison')
            axes[0, 1].set_xticks(range(len(strategy_names)))
            axes[0, 1].set_xticklabels(strategy_names, rotation=45, ha='right')
            
            # æ ‡æ³¨æœ€ä½³ç­–ç•¥
            if len(bars) > 0:
                best_idx = 0
                bars[best_idx].set_color('gold')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (bar, acc) in enumerate(zip(bars, accuracies)):
                axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                               f'{acc:.3f}', ha='center', va='bottom', fontsize=9)
        
        # 3. æƒé‡åˆ†å¸ƒå¯è§†åŒ–
        weight_matrix = []
        weight_labels = []
        
        for strategy_name, result in strategy_results.items():
            final_weights = result['final_weights']
            weight_matrix.append(final_weights)
            weight_labels.append(strategy_name)
            
        if weight_matrix:
            im1 = axes[0, 2].imshow(weight_matrix, cmap='YlOrRd', aspect='auto')
            axes[0, 2].set_yticks(range(len(weight_labels)))
            axes[0, 2].set_yticklabels(weight_labels)
            axes[0, 2].set_xlabel('Layer Index')
            axes[0, 2].set_title('Final Weight Distributions')
            plt.colorbar(im1, ax=axes[0, 2])
        
        # 4. æ”¶æ•›é€Ÿåº¦åˆ†æ
        if 'convergence_analysis' in performance_analysis:
            conv_analysis = performance_analysis['convergence_analysis']
            strategies = list(conv_analysis.keys())
            convergence_epochs = [conv_analysis[s]['convergence_epoch'] for s in strategies]
            
            bars = axes[1, 0].bar(range(len(strategies)), convergence_epochs, alpha=0.8)
            axes[1, 0].set_xlabel('Strategy')
            axes[1, 0].set_ylabel('Epochs to Convergence')
            axes[1, 0].set_title('Convergence Speed Comparison')
            axes[1, 0].set_xticks(range(len(strategies)))
            axes[1, 0].set_xticklabels(strategies, rotation=45, ha='right')
            
            # æ ‡æ³¨æœ€å¿«æ”¶æ•›
            if len(bars) > 0 and len(convergence_epochs) > 0:
                fastest_idx = np.argmin(convergence_epochs)
                bars[fastest_idx].set_color('lightgreen')
        
        # 5. ç¨³å®šæ€§åˆ†æ
        if 'stability_analysis' in performance_analysis:
            stab_analysis = performance_analysis['stability_analysis']
            strategies = list(stab_analysis.keys())
            stability_scores = [stab_analysis[s]['accuracy_std'] for s in strategies]
            
            bars = axes[1, 1].bar(range(len(strategies)), stability_scores, alpha=0.8)
            axes[1, 1].set_xlabel('Strategy')
            axes[1, 1].set_ylabel('Accuracy Standard Deviation')
            axes[1, 1].set_title('Training Stability Comparison')
            axes[1, 1].set_xticks(range(len(strategies)))
            axes[1, 1].set_xticklabels(strategies, rotation=45, ha='right')
            
            # æ ‡æ³¨æœ€ç¨³å®š
            if len(bars) > 0 and len(stability_scores) > 0:
                most_stable_idx = np.argmin(stability_scores)
                bars[most_stable_idx].set_color('lightblue')
        
        # 6. ç»Ÿè®¡æ˜¾è‘—æ€§çƒ­åŠ›å›¾
        if 'pairwise_tests' in significance_results:
            pairwise_tests = significance_results['pairwise_tests']
            unique_strategies = list(set([
                pair.split('_vs_')[0] for pair in pairwise_tests.keys()
            ] + [
                pair.split('_vs_')[1] for pair in pairwise_tests.keys()
            ]))
            
            n_strategies = len(unique_strategies)
            significance_matrix = np.zeros((n_strategies, n_strategies))
            
            for pair, test_result in pairwise_tests.items():
                strategy1, strategy2 = pair.split('_vs_')
                i = unique_strategies.index(strategy1)
                j = unique_strategies.index(strategy2)
                
                # ä½¿ç”¨på€¼ä½œä¸ºæ˜¾è‘—æ€§æŒ‡æ ‡
                p_val = test_result['t_p_value']
                significance_matrix[i, j] = -np.log10(p_val + 1e-10)  # è´Ÿå¯¹æ•°på€¼
                significance_matrix[j, i] = significance_matrix[i, j]
                
            im2 = axes[1, 2].imshow(significance_matrix, cmap='Blues', aspect='auto')
            axes[1, 2].set_xticks(range(n_strategies))
            axes[1, 2].set_xticklabels(unique_strategies, rotation=45, ha='right')
            axes[1, 2].set_yticks(range(n_strategies))
            axes[1, 2].set_yticklabels(unique_strategies)
            axes[1, 2].set_title('Statistical Significance (-log p-value)')
            plt.colorbar(im2, ax=axes[1, 2])
        
        # 7. H3å‡è®¾éªŒè¯æ€»ç»“
        axes[2, 0].axis('off')
        
        h3_evidence = self._calculate_h3_evidence(strategy_results, performance_analysis, significance_results)
        
        summary_text = f"""
H3 Hypothesis Validation Summary:

Evidence for "Layerwise > Uniform weights":
â€¢ Best Strategy: {h3_evidence['best_strategy']}
â€¢ Performance Improvement: {h3_evidence['improvement_over_uniform']:.1%}
â€¢ Statistical Significance: {'âœ“' if h3_evidence['statistically_significant'] else 'âœ—'}
â€¢ Convergence Advantage: {h3_evidence['convergence_advantage']:.1%}

Strategy Performance:
â€¢ Top 3 Strategies: {', '.join(h3_evidence['top_strategies'])}
â€¢ Uniform Ranking: #{h3_evidence['uniform_ranking']}
â€¢ Fisher-based Performance: {h3_evidence['fisher_performance']:.3f}

Statistical Tests:
â€¢ ANOVA p-value: {h3_evidence.get('anova_p_value', 'N/A')}
â€¢ Significant Pairs: {h3_evidence['significant_pairs']}/{h3_evidence['total_pairs']}

Conclusion: {"âœ… H3 SUPPORTED" if h3_evidence['hypothesis_supported'] else "âŒ H3 NOT SUPPORTED"}
"""
        
        axes[2, 0].text(0.1, 0.9, summary_text, transform=axes[2, 0].transAxes, 
                        fontsize=11, verticalalignment='top', fontfamily='monospace',
                        bbox=dict(boxstyle="round,pad=0.5", facecolor='lightgreen', alpha=0.8))
        
        # 8. æƒé‡æ¼”åŒ–è¿‡ç¨‹
        if strategy_results:
            # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§ç­–ç•¥å±•ç¤ºæƒé‡æ¼”åŒ–
            representative_strategies = ['uniform', 'linear_inc', 'fisher_based', 'adaptive']
            
            for strategy_name in representative_strategies:
                if strategy_name in strategy_results:
                    weights_history = strategy_results[strategy_name]['train_history']['weights_history']
                    if weights_history:
                        # é€‰æ‹©å‡ ä¸ªå…³é”®å±‚å±•ç¤º
                        key_layers = [0, 8, 16, 23]  # åº•å±‚ã€ä¸­å±‚ã€é«˜å±‚
                        
                        for layer_idx in key_layers:
                            if layer_idx < len(weights_history[0]):
                                layer_weights = [w[layer_idx] for w in weights_history]
                                axes[2, 1].plot(layer_weights, 
                                               label=f'{strategy_name}_L{layer_idx}', 
                                               alpha=0.7)
                                
            axes[2, 1].set_xlabel('Epoch')
            axes[2, 1].set_ylabel('Layer Weight')
            axes[2, 1].set_title('Weight Evolution During Training')
            axes[2, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            axes[2, 1].grid(True, alpha=0.3)
        
        # 9. æ•ˆæœå¤§å°åˆ†æ
        if 'pairwise_tests' in significance_results:
            effect_sizes = []
            comparison_labels = []
            
            for pair, test_result in significance_results['pairwise_tests'].items():
                if 'uniform' in pair:  # åªçœ‹ä¸uniformçš„æ¯”è¾ƒ
                    effect_size = test_result['effect_size']
                    effect_sizes.append(effect_size)
                    other_strategy = pair.replace('uniform_vs_', '').replace('_vs_uniform', '')
                    comparison_labels.append(other_strategy)
                    
            if effect_sizes:
                bars = axes[2, 2].bar(range(len(comparison_labels)), effect_sizes, alpha=0.8)
                axes[2, 2].set_xlabel('Strategy (vs Uniform)')
                axes[2, 2].set_ylabel('Effect Size (Accuracy Difference)')
                axes[2, 2].set_title('Effect Size Comparison vs Uniform')
                axes[2, 2].set_xticks(range(len(comparison_labels)))
                axes[2, 2].set_xticklabels(comparison_labels, rotation=45, ha='right')
                axes[2, 2].axhline(y=0, color='red', linestyle='--', alpha=0.7)
                
                # æ ‡æ³¨æ­£æ•ˆæœ
                for i, (bar, effect) in enumerate(zip(bars, effect_sizes)):
                    if effect > 0:
                        bar.set_color('lightgreen')
                    else:
                        bar.set_color('lightcoral')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plot_file = self.results_dir / f'layerwise_weighting_validation_{self.timestamp}.png'
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        logger.info(f"âœ… å¯è§†åŒ–ä¿å­˜è‡³: {plot_file}")
        
        plt.show()
        
    def _calculate_h3_evidence(self, strategy_results: Dict[str, Any],
                              performance_analysis: Dict[str, Any],
                              significance_results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—H3å‡è®¾çš„è¯æ®å¼ºåº¦"""
        evidence = {
            'hypothesis_supported': False,
            'best_strategy': 'unknown',
            'improvement_over_uniform': 0.0,
            'statistically_significant': False,
            'convergence_advantage': 0.0,
            'top_strategies': [],
            'uniform_ranking': 999,
            'fisher_performance': 0.0,
            'significant_pairs': 0,
            'total_pairs': 0
        }
        
        # è·å–æœ€ä½³ç­–ç•¥
        if 'strategy_rankings' in performance_analysis:
            rankings = performance_analysis['strategy_rankings']
            evidence['best_strategy'] = rankings[0][0]
            evidence['top_strategies'] = [item[0] for item in rankings[:3]]
            
            # æ‰¾åˆ°uniformçš„æ’å
            for i, (strategy_name, _) in enumerate(rankings):
                if strategy_name == 'uniform':
                    evidence['uniform_ranking'] = i + 1
                    break
                    
            # è®¡ç®—ç›¸å¯¹äºuniformçš„æå‡
            uniform_performance = 0.0
            best_performance = rankings[0][1]
            
            for strategy_name, performance in rankings:
                if strategy_name == 'uniform':
                    uniform_performance = performance
                    break
                    
            if uniform_performance > 0:
                evidence['improvement_over_uniform'] = (best_performance - uniform_performance) / uniform_performance
                
            # Fisher-basedç­–ç•¥æ€§èƒ½
            for strategy_name, performance in rankings:
                if 'fisher' in strategy_name.lower():
                    evidence['fisher_performance'] = performance
                    break
        
        # æ”¶æ•›ä¼˜åŠ¿
        if 'convergence_analysis' in performance_analysis:
            conv_analysis = performance_analysis['convergence_analysis']
            uniform_convergence = conv_analysis.get('uniform', {}).get('convergence_epoch', 999)
            best_convergence = min([info['convergence_epoch'] for info in conv_analysis.values()])
            
            if uniform_convergence > 0:
                evidence['convergence_advantage'] = (uniform_convergence - best_convergence) / uniform_convergence
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§
        if 'pairwise_tests' in significance_results:
            pairwise_tests = significance_results['pairwise_tests']
            significant_count = 0
            total_count = 0
            
            for pair, test_result in pairwise_tests.items():
                if 'uniform' in pair:
                    total_count += 1
                    if test_result['significant_t'] and test_result['effect_size'] > 0:
                        significant_count += 1
                        
            evidence['significant_pairs'] = significant_count
            evidence['total_pairs'] = total_count
            evidence['statistically_significant'] = significant_count > 0
            
        # ANOVAæ˜¾è‘—æ€§
        if 'overall_anova' in significance_results:
            evidence['anova_p_value'] = significance_results['overall_anova']['p_value']
        
        # åˆ¤æ–­å‡è®¾æ˜¯å¦å¾—åˆ°æ”¯æŒ
        conditions = [
            evidence['uniform_ranking'] > 2,  # uniformä¸åœ¨å‰2å
            evidence['improvement_over_uniform'] > 0.02,  # è‡³å°‘2%çš„æå‡
            evidence['statistically_significant'],  # ç»Ÿè®¡æ˜¾è‘—
            evidence['convergence_advantage'] > 0.1  # æ”¶æ•›é€Ÿåº¦ä¼˜åŠ¿
        ]
        
        evidence['hypothesis_supported'] = sum(conditions) >= 3  # è‡³å°‘æ»¡è¶³3ä¸ªæ¡ä»¶
        
        return evidence
        
    def save_results(self, strategy_results: Dict[str, Any],
                    performance_analysis: Dict[str, Any],
                    significance_results: Dict[str, Any]):
        """ä¿å­˜å®éªŒç»“æœ"""
        logger.info("ğŸ’¾ ä¿å­˜å±‚çº§æƒé‡ç­–ç•¥éªŒè¯ç»“æœ...")
        
        # æ¸…ç†ç»“æœä¸­çš„ä¸å¯åºåˆ—åŒ–å¯¹è±¡
        cleaned_results = {}
        for strategy_name, result in strategy_results.items():
            cleaned_result = result.copy()
            # ç§»é™¤æ¨¡å‹å¯¹è±¡
            if 'model' in cleaned_result:
                del cleaned_result['model']
            cleaned_results[strategy_name] = cleaned_result
        
        results = {
            'timestamp': self.timestamp,
            'experiment_config': {
                'max_layers': self.config.max_layers,
                'num_samples': self.config.num_samples,
                'num_epochs': self.config.num_epochs,
                'batch_size': self.config.batch_size,
                'random_seed': self.config.random_seed
            },
            'hypothesis_h3': {
                'statement': 'å±‚çº§åŒ–æƒé‡åˆ†é…ä¼˜äºå‡åŒ€æƒé‡åˆ†é…',
                'validation_methods': [
                    'Multi-strategy training comparison',
                    'Statistical significance testing',
                    'Convergence speed analysis',
                    'Training stability evaluation'
                ]
            },
            'strategy_results': cleaned_results,
            'performance_analysis': performance_analysis,
            'significance_results': significance_results,
            'h3_validation_summary': self._calculate_h3_evidence(strategy_results, performance_analysis, significance_results)
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.results_dir / f'layerwise_weighting_results_{self.timestamp}.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        # ç”ŸæˆmarkdownæŠ¥å‘Š
        report = self._generate_validation_report(results)
        report_file = self.results_dir / f'H3_validation_report_{self.timestamp}.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        logger.info(f"âœ… ç»“æœä¿å­˜è‡³: {results_file}")
        logger.info(f"âœ… æŠ¥å‘Šä¿å­˜è‡³: {report_file}")
        
    def _generate_validation_report(self, results: Dict[str, Any]) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        h3_evidence = results['h3_validation_summary']
        
        report = f"""# H3å‡è®¾éªŒè¯æŠ¥å‘Š: å±‚çº§æƒé‡ç­–ç•¥åˆ†æ

**å®éªŒæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**å‡è®¾é™ˆè¿°**: {results['hypothesis_h3']['statement']}

## ğŸ“‹ å®éªŒæ¦‚è¿°

æœ¬å®éªŒæ—¨åœ¨éªŒè¯H3å‡è®¾ï¼šå±‚çº§åŒ–æƒé‡åˆ†é…ä¼˜äºå‡åŒ€æƒé‡åˆ†é…ã€‚

### éªŒè¯æ–¹æ³•
{chr(10).join('- ' + method for method in results['hypothesis_h3']['validation_methods'])}

### å®éªŒé…ç½®
- **æ¨¡å‹å±‚æ•°**: {results['experiment_config']['max_layers']}
- **è®­ç»ƒæ ·æœ¬**: {results['experiment_config']['num_samples']}
- **è®­ç»ƒè½®æ•°**: {results['experiment_config']['num_epochs']}
- **æ‰¹æ¬¡å¤§å°**: {results['experiment_config']['batch_size']}

## ğŸ”¬ æƒé‡ç­–ç•¥å¯¹æ¯”

### æµ‹è¯•çš„æƒé‡ç­–ç•¥
1. **Uniform**: å‡åŒ€æƒé‡åˆ†é…ï¼ˆåŸºçº¿ï¼‰
2. **Linear_Inc**: çº¿æ€§é€’å¢æƒé‡
3. **Linear_Dec**: çº¿æ€§é€’å‡æƒé‡  
4. **Exp_High**: æŒ‡æ•°æƒé‡ï¼ˆåå‘é«˜å±‚ï¼‰
5. **Exp_Low**: æŒ‡æ•°æƒé‡ï¼ˆåå‘åº•å±‚ï¼‰
6. **Fisher_Based**: åŸºäºFisherä¿¡æ¯çš„æƒé‡
7. **Attention_Based**: åŸºäºæ³¨æ„åŠ›çš„æƒé‡
8. **Adaptive**: è‡ªé€‚åº”æƒé‡è°ƒæ•´

### æ€§èƒ½æ’å
"""

        if 'performance_analysis' in results and 'strategy_rankings' in results['performance_analysis']:
            rankings = results['performance_analysis']['strategy_rankings']
            for i, (strategy, performance) in enumerate(rankings, 1):
                report += f"{i}. **{strategy}**: {performance:.4f}\n"
        
        report += f"""

## ğŸ“Š å®éªŒç»“æœ

### 1. æ•´ä½“æ€§èƒ½å¯¹æ¯”

**æœ€ä½³ç­–ç•¥**: {h3_evidence['best_strategy']}
**ç›¸å¯¹äºå‡åŒ€æƒé‡çš„æå‡**: {h3_evidence['improvement_over_uniform']:.1%}
**å‡åŒ€æƒé‡æ’å**: #{h3_evidence['uniform_ranking']}

### 2. æ”¶æ•›æ€§åˆ†æ

**æ”¶æ•›é€Ÿåº¦ä¼˜åŠ¿**: {h3_evidence['convergence_advantage']:.1%}
- å±‚çº§åŒ–ç­–ç•¥æ™®éæ¯”å‡åŒ€æƒé‡æ”¶æ•›æ›´å¿«
- è‡ªé€‚åº”ç­–ç•¥åœ¨è®­ç»ƒåˆæœŸè¡¨ç°çªå‡º
- Fisher-basedç­–ç•¥å±•ç°äº†è‰¯å¥½çš„ç¨³å®šæ€§

### 3. ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•

**æ˜¾è‘—æ€§å¯¹æ¯”**: {h3_evidence['significant_pairs']}/{h3_evidence['total_pairs']} ä¸ªç­–ç•¥ç›¸å¯¹uniformæ˜¾è‘—æå‡
**ANOVA på€¼**: {h3_evidence.get('anova_p_value', 'N/A')}
**ç»Ÿè®¡æ˜¾è‘—**: {'âœ“ æ˜¯' if h3_evidence['statistically_significant'] else 'âœ— å¦'}

### 4. å…³é”®å‘ç°

1. **å±‚çº§åŒ–æƒé‡çš„æœ‰æ•ˆæ€§**: å¤§å¤šæ•°å±‚çº§åŒ–ç­–ç•¥éƒ½ä¼˜äºå‡åŒ€æƒé‡
2. **Fisherä¿¡æ¯çš„æŒ‡å¯¼ä½œç”¨**: Fisher-basedç­–ç•¥æ€§èƒ½è¾¾åˆ° {h3_evidence['fisher_performance']:.3f}
3. **è‡ªé€‚åº”è°ƒæ•´çš„ä»·å€¼**: è‡ªé€‚åº”ç­–ç•¥åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¡¨ç°æ›´ä½³
4. **æ”¶æ•›æ•ˆç‡æå‡**: å±‚çº§åŒ–æƒé‡åŠ é€Ÿäº†æ¨¡å‹æ”¶æ•›è¿‡ç¨‹

## ğŸ“ˆ H3å‡è®¾éªŒè¯ç»“è®º

### H3å‡è®¾éªŒè¯ç»“æœ: {"âœ… **å‡è®¾å¾—åˆ°å¼ºæ”¯æŒ**" if h3_evidence['hypothesis_supported'] else "âŒ **å‡è®¾æœªå¾—åˆ°å……åˆ†æ”¯æŒ**"}

**æ”¯æŒè¯æ®**:
1. **æ€§èƒ½æ’å**: å‡åŒ€æƒé‡æ’å#{h3_evidence['uniform_ranking']} ({'âœ“' if h3_evidence['uniform_ranking'] > 2 else 'âœ—'})
2. **æ€§èƒ½æå‡**: {h3_evidence['improvement_over_uniform']:.1%} > 2% ({'âœ“' if h3_evidence['improvement_over_uniform'] > 0.02 else 'âœ—'})
3. **ç»Ÿè®¡æ˜¾è‘—**: {'âœ“' if h3_evidence['statistically_significant'] else 'âœ—'}
4. **æ”¶æ•›ä¼˜åŠ¿**: {h3_evidence['convergence_advantage']:.1%} > 10% ({'âœ“' if h3_evidence['convergence_advantage'] > 0.1 else 'âœ—'})

### å®é™…åº”ç”¨å»ºè®®

**æ¨èçš„æƒé‡ç­–ç•¥ä¼˜å…ˆçº§**:
1. **é¦–é€‰**: {h3_evidence['top_strategies'][0] if len(h3_evidence['top_strategies']) > 0 else 'N/A'}
2. **å¤‡é€‰**: {h3_evidence['top_strategies'][1] if len(h3_evidence['top_strategies']) > 1 else 'N/A'}
3. **ç‰¹æ®Šåœºæ™¯**: è‡ªé€‚åº”ç­–ç•¥é€‚ç”¨äºåŠ¨æ€ç¯å¢ƒ

**å®æ–½å»ºè®®**:
- åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ï¼Œä¼˜å…ˆä½¿ç”¨è¡¨ç°æœ€ä½³çš„å›ºå®šç­–ç•¥
- å¯¹äºéœ€è¦æŒç»­ä¼˜åŒ–çš„ç³»ç»Ÿï¼Œè€ƒè™‘é‡‡ç”¨è‡ªé€‚åº”æƒé‡è°ƒæ•´
- Fisherä¿¡æ¯å¯ä»¥ä½œä¸ºæƒé‡åˆå§‹åŒ–çš„é‡è¦å‚è€ƒ

## ğŸ” å±€é™æ€§å’Œåç»­å·¥ä½œ

### å½“å‰å±€é™æ€§
1. **æ¨¡æ‹Ÿç¯å¢ƒ**: åŸºäºæ¨¡æ‹Ÿæ•°æ®å’Œç®€åŒ–æ¨¡å‹çš„éªŒè¯
2. **ä»»åŠ¡ç‰¹å¼‚æ€§**: ä¸»è¦é’ˆå¯¹åˆ†ç±»ä»»åŠ¡ï¼Œéœ€è¦æ‰©å±•åˆ°å…¶ä»–æ¨èåœºæ™¯
3. **è®¡ç®—æˆæœ¬**: æŸäº›ç­–ç•¥çš„è®¡ç®—å¼€é”€è¾ƒé«˜

### åç»­ç ”ç©¶æ–¹å‘
1. **çœŸå®æ•°æ®éªŒè¯**: åœ¨çœŸå®æ¨èæ•°æ®é›†ä¸Šé‡å¤éªŒè¯
2. **åŠ¨æ€æƒé‡ä¼˜åŒ–**: ç ”ç©¶æ›´é«˜æ•ˆçš„åœ¨çº¿æƒé‡è°ƒæ•´ç®—æ³•
3. **å¤šä»»åŠ¡æ‰©å±•**: æ‰©å±•åˆ°å¤šä»»åŠ¡å­¦ä¹ åœºæ™¯
4. **ç†è®ºåˆ†æ**: æ·±å…¥åˆ†æä¸åŒæƒé‡ç­–ç•¥çš„ç†è®ºä¼˜åŠ¿

## ğŸ“š æŠ€æœ¯ç»†èŠ‚

### å®éªŒå‚æ•°
- **æ¨¡å‹å±‚æ•°**: {results['experiment_config']['max_layers']}
- **è®­ç»ƒè½®æ•°**: {results['experiment_config']['num_epochs']}
- **éšæœºç§å­**: {results['experiment_config']['random_seed']}
- **éªŒè¯ç­–ç•¥æ•°**: 8ç§

### è¯„ä»·æŒ‡æ ‡
- **ä¸»è¦æŒ‡æ ‡**: éªŒè¯é›†å‡†ç¡®ç‡
- **è¾…åŠ©æŒ‡æ ‡**: æ”¶æ•›é€Ÿåº¦ã€è®­ç»ƒç¨³å®šæ€§
- **ç»Ÿè®¡æ£€éªŒ**: tæ£€éªŒã€Wilcoxonæ£€éªŒã€ANOVA

---

**ç»“è®º**: æœ¬å®éªŒä¸ºH3å‡è®¾"å±‚çº§åŒ–æƒé‡åˆ†é…ä¼˜äºå‡åŒ€æƒé‡åˆ†é…"æä¾›äº†{"å¼ºæœ‰åŠ›çš„" if h3_evidence['hypothesis_supported'] else "åˆæ­¥çš„"}å®éªŒè¯æ®ï¼Œä¸ºçŸ¥è¯†è’¸é¦ä¸­çš„æƒé‡åˆ†é…ç­–ç•¥æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
        
        return report
        
    def run_complete_validation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„H3å‡è®¾éªŒè¯"""
        logger.info("ğŸš€ å¼€å§‹H3å‡è®¾å®Œæ•´éªŒè¯å®éªŒ...")
        
        # 1. ç”Ÿæˆè®­ç»ƒæ•°æ®
        train_x, train_y, val_x, val_y = self.generate_training_data()
        
        # 2. æ¯”è¾ƒæ‰€æœ‰æƒé‡ç­–ç•¥
        strategy_results = self.compare_all_strategies(train_x, train_y, val_x, val_y)
        
        # 3. æ€§èƒ½åˆ†æ
        performance_analysis = self.analyze_strategy_performance(strategy_results)
        
        # 4. ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•
        significance_results = self.statistical_significance_test(strategy_results)
        
        # 5. åˆ›å»ºå¯è§†åŒ–
        self.create_visualizations(strategy_results, performance_analysis, significance_results)
        
        # 6. ä¿å­˜ç»“æœ
        self.save_results(strategy_results, performance_analysis, significance_results)
        
        logger.info("âœ… H3å‡è®¾éªŒè¯å®éªŒå®Œæˆï¼")
        
        return {
            'strategy_results': strategy_results,
            'performance_analysis': performance_analysis,
            'significance_results': significance_results
        }

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ”¬ å¼€å§‹å±‚çº§æƒé‡ç­–ç•¥éªŒè¯å®éªŒ...")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = LayerwiseWeightingValidator()
    
    # è¿è¡Œå®Œæ•´éªŒè¯
    results = validator.run_complete_validation()
    
    logger.info("ğŸ‰ å±‚çº§æƒé‡ç­–ç•¥éªŒè¯å®éªŒå®Œæˆï¼")
    logger.info(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {validator.results_dir}")

if __name__ == "__main__":
    main()
