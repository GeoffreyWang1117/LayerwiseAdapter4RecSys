#!/usr/bin/env python3
"""
H4å‡è®¾éªŒè¯å®éªŒ - çœŸå®LLMæ¨¡å‹å¯¹æ¯”
ä½¿ç”¨Ollamaè°ƒç”¨çœŸå®çš„LLMæ¨¡å‹è¿›è¡Œæ¨èä»»åŠ¡å¯¹æ¯”
"""

# å°è¯•å¯¼å…¥åŒ…ï¼Œå¤„ç†ç‰ˆæœ¬å†²çª
try:
    import torch
    print("âœ… PyTorch å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ PyTorch å¯¼å…¥å¤±è´¥: {e}")
    import sys
    sys.exit(1)

# å¤„ç†ollamaåŒ…çš„ç‰ˆæœ¬å†²çª
HAS_OLLAMA = False
try:
    # æ–¹æ³•1: å°è¯•ç›´æ¥å¯¼å…¥
    import ollama
    HAS_OLLAMA = True
    print("âœ… Ollama åŒ…å¯¼å…¥æˆåŠŸ")
except Exception as e1:
    try:
        # æ–¹æ³•2: å°è¯•é‡è£…å…¼å®¹ç‰ˆæœ¬
        import subprocess
        import sys
        print("ğŸ”„ å°è¯•ä¿®å¤OllamaåŒ…ä¾èµ–...")
        subprocess.check_call([
            sys.executable, "-m", "pip", "install", 
            "ollama==0.3.1", "pydantic>=1.10.0,<2.0.0", "--force-reinstall", "--quiet"
        ])
        import ollama
        HAS_OLLAMA = True
        print("âœ… Ollama åŒ…ä¿®å¤æˆåŠŸ")
    except Exception as e2:
        print(f"âš ï¸ Ollama åŒ…ä¸å¯ç”¨: {e2}")
        HAS_OLLAMA = False

# å°è¯•å¯¼å…¥openai
HAS_OPENAI = False  
try:
    import openai
    HAS_OPENAI = True
    print("âœ… OpenAI åŒ…å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ OpenAI åŒ…ä¸å¯ç”¨: {e}")
    HAS_OPENAI = False
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import logging  
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import json
from dataclasses import dataclass
from scipy.stats import ttest_ind
import time
import re
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass 
class RealModelConfig:
    """çœŸå®æ¨¡å‹é…ç½®"""
    num_samples: int = 500  # å‡å°‘æ ·æœ¬æ•°ï¼Œå› ä¸ºLLMæ¨ç†è¾ƒæ…¢
    num_test_cases: int = 100  # æµ‹è¯•æ¡ˆä¾‹æ•°
    max_retries: int = 3
    timeout: int = 30
    temperature: float = 0.1  # é™ä½éšæœºæ€§
    random_seed: int = 42
    openai_api_key: str = "YOUR_API_KEY_HERE"
    use_chatgpt_api: bool = True  # æ˜¯å¦ä½¿ç”¨ChatGPT API

class UnifiedModelWrapper:
    """ç»Ÿä¸€çš„æ¨¡å‹åŒ…è£…å™¨ï¼Œæ”¯æŒOllamaå’ŒOpenAIæ¨¡å‹"""
    
    def __init__(self, model_name: str, config: RealModelConfig):
        self.model_name = model_name
        self.config = config
        # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šç±»å‹
        self.model_type = self.get_model_type(model_name)
        
        if self.model_type == 'ollama':
            if HAS_OLLAMA:
                self.client = ollama
            else:
                raise ValueError(f"Ollama not available for model {model_name}")
        elif self.model_type == 'openai':
            if HAS_OPENAI and config.use_chatgpt_api and config.openai_api_key:
                self.client = openai.OpenAI(api_key=config.openai_api_key)
            else:
                raise ValueError("OpenAI API key is required and OpenAI package must be available")
                
        # æ·»åŠ æ¨¡å‹è§„æ ¼ä¿¡æ¯
        self.spec = self._get_model_spec(model_name)
    
    def get_model_type(self, model_name: str) -> str:
        """æ ¹æ®æ¨¡å‹åç§°ç¡®å®šç±»å‹"""
        if model_name.startswith(('gpt-3.5', 'gpt-4')):
            return 'openai'
        else:
            return 'ollama'
            
    def _get_model_spec(self, model_name: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹è§„æ ¼ä¿¡æ¯"""
        model_specs = {
            'llama3:latest': {
                'display_name': 'Llama3',
                'context_length': 8192,
                'efficiency': 0.85,
                'reasoning': 0.95,
                'size_gb': 4.7,
                'type': 'ollama'
            },
            'qwen3:latest': {
                'display_name': 'Qwen3',
                'context_length': 32768,
                'efficiency': 0.82,
                'reasoning': 0.88,
                'size_gb': 5.2,
                'type': 'ollama'
            },
            'gpt-3.5-turbo': {
                'display_name': 'GPT-3.5-Turbo',
                'context_length': 16384,
                'efficiency': 0.88,
                'reasoning': 0.90,
                'size_gb': 0.0,  # APIæ¨¡å‹
                'type': 'openai'
            },
            'gpt-4': {
                'display_name': 'GPT-4',
                'context_length': 8192,
                'efficiency': 0.75,
                'reasoning': 0.98,
                'size_gb': 0.0,  # APIæ¨¡å‹
                'type': 'openai'
            }
        }
        
        return model_specs.get(model_name, {
            'display_name': model_name,
            'context_length': 4096,
            'efficiency': 0.75,
            'reasoning': 0.80,
            'size_gb': 5.0,
            'type': self.model_type
        })
        
        # æ¨¡å‹ç‰¹æ€§ï¼ˆåŸºäºçœŸå®æ¨¡å‹ç‰¹ç‚¹ï¼‰
        self.model_specs = {
            'llama3:latest': {
                'display_name': 'Llama3',
                'context_length': 8192,
                'efficiency': 0.85,
                'reasoning': 0.95,
                'size_gb': 4.7,
                'type': 'ollama'
            },
            'llama3.2:latest': {
                'display_name': 'Llama3.2',
                'context_length': 8192,
                'efficiency': 0.88,
                'reasoning': 0.90,
                'size_gb': 2.0,
                'type': 'ollama'
            },
            'qwen3:latest': {
                'display_name': 'Qwen3',
                'context_length': 32768,
                'efficiency': 0.82,
                'reasoning': 0.88,
                'size_gb': 5.2,
                'type': 'ollama'
            },
            'gemma2:2b': {
                'display_name': 'Gemma2-2B',
                'context_length': 8192,
                'efficiency': 0.90,
                'reasoning': 0.75,
                'size_gb': 1.6,
                'type': 'ollama'
            },
            'gpt-oss:latest': {
                'display_name': 'GPT-OSS',
                'context_length': 4096,
                'efficiency': 0.70,
                'reasoning': 0.92,
                'size_gb': 13.0,
                'type': 'ollama'
            },
            'gpt-3.5-turbo': {
                'display_name': 'GPT-3.5-Turbo',
                'context_length': 16384,
                'efficiency': 0.88,
                'reasoning': 0.90,
                'size_gb': 0.0,  # APIæ¨¡å‹
                'type': 'openai'
            },
            'gpt-4': {
                'display_name': 'GPT-4',
                'context_length': 8192,
                'efficiency': 0.75,
                'reasoning': 0.98,
                'size_gb': 0.0,  # APIæ¨¡å‹
                'type': 'openai'
            },
            'gpt-4-turbo': {
                'display_name': 'GPT-4-Turbo',
                'context_length': 128000,
                'efficiency': 0.80,
                'reasoning': 0.96,
                'size_gb': 0.0,  # APIæ¨¡å‹
                'type': 'openai'
            },
            'gpt-4o-mini': {
                'display_name': 'GPT-4O-Mini',
                'context_length': 128000,
                'efficiency': 0.95,
                'reasoning': 0.92,
                'size_gb': 0.0,  # APIæ¨¡å‹
                'type': 'openai'
            }
        }
        
        self.spec = self.model_specs.get(model_name, {
            'display_name': model_name,
            'context_length': 4096,
            'efficiency': 0.75,
            'reasoning': 0.80,
            'size_gb': 5.0
        })
        
        logger.info(f"ğŸ¤– åˆå§‹åŒ– {self.spec['display_name']} æ¨¡å‹")
        
    def generate_recommendation(self, user_profile: str, item_candidates: List[str], 
                              context: str = "") -> Dict[str, Any]:
        """ç”Ÿæˆæ¨èç»“æœ"""
        
        # æ„å»ºæ¨èæç¤º
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¨èç³»ç»Ÿã€‚è¯·æ ¹æ®ç”¨æˆ·æ¡£æ¡ˆæ¨èæœ€åˆé€‚çš„ç‰©å“ã€‚

ç”¨æˆ·æ¡£æ¡ˆ: {user_profile}
å€™é€‰ç‰©å“: {', '.join(item_candidates)}
{f'é¢å¤–ä¸Šä¸‹æ–‡: {context}' if context else ''}

è¯·ä»å€™é€‰ç‰©å“ä¸­é€‰æ‹©æœ€ä½³æ¨èï¼Œå¹¶ç»™å‡ºæ¨èç†ç”±ã€‚
è¾“å‡ºæ ¼å¼ï¼š
æ¨èç‰©å“: [ç‰©å“åç§°]
æ¨èç†ç”±: [ç®€æ´ç†ç”±]
ç½®ä¿¡åº¦: [0-100çš„æ•´æ•°]"""

        start_time = time.time()
        
        for attempt in range(self.config.max_retries):
            try:
                if self.model_type == 'ollama':
                    response = self.client.generate(
                        model=self.model_name,
                        prompt=prompt,
                        options={
                            'temperature': self.config.temperature,
                            'top_p': 0.9,
                            'top_k': 40
                        }
                    )
                    response_text = response['response']
                elif self.model_type == 'openai':
                    response = self.client.chat.completions.create(
                        model=self.model_name,
                        messages=[{"role": "user", "content": prompt}],
                        temperature=self.config.temperature,
                        max_tokens=500
                    )
                    response_text = response.choices[0].message.content
                
                inference_time = time.time() - start_time
                
                # è§£æå“åº”
                result = self._parse_recommendation_response(response_text)
                result['raw_response'] = response_text  # ä¿å­˜åŸå§‹å“åº”ç”¨äºè°ƒè¯•
                result['inference_time'] = inference_time

                
                return result
                
            except Exception as e:
                logger.warning(f"âš ï¸ {self.spec['display_name']} ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥: {str(e)}")
                if attempt == self.config.max_retries - 1:
                    return {
                        'recommended_item': None,
                        'reason': "ç”Ÿæˆå¤±è´¥",
                        'confidence': 0,
                        'inference_time': time.time() - start_time,
                        'error': str(e)
                    }
                time.sleep(1)
                
    def _parse_recommendation_response(self, response: str) -> Dict[str, Any]:
        """è§£ææ¨èå“åº”"""
        result = {
            'recommended_item': None,
            'reason': '',
            'confidence': 50
        }
        
        try:
            # æå–æ¨èç‰©å“
            item_match = re.search(r'æ¨èç‰©å“[:ï¼š]\s*(.+?)(?:\n|$)', response, re.IGNORECASE)
            if item_match:
                result['recommended_item'] = item_match.group(1).strip()
                
            # æå–æ¨èç†ç”±
            reason_match = re.search(r'æ¨èç†ç”±[:ï¼š]\s*(.+?)(?:\n|ç½®ä¿¡åº¦|$)', response, re.IGNORECASE | re.DOTALL)
            if reason_match:
                result['reason'] = reason_match.group(1).strip()
                
            # æå–ç½®ä¿¡åº¦
            confidence_match = re.search(r'ç½®ä¿¡åº¦[:ï¼š]\s*(\d+)', response, re.IGNORECASE)
            if confidence_match:
                result['confidence'] = int(confidence_match.group(1))
                
        except Exception as e:
            logger.warning(f"âš ï¸ è§£æå“åº”å¤±è´¥: {str(e)}")
            
        return result
        
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'name': self.model_name,
            'display_name': self.spec['display_name'],
            'efficiency_score': self.spec['efficiency'],
            'reasoning_score': self.spec['reasoning'],
            'model_size_gb': self.spec['size_gb'],
            'context_length': self.spec['context_length']
        }

class RecommendationTestDataset:
    """æ¨èæµ‹è¯•æ•°æ®é›†"""
    
    def __init__(self, config: RealModelConfig):
        self.config = config
        np.random.seed(config.random_seed)
        
        # é¢„å®šä¹‰çš„æµ‹è¯•åœºæ™¯
        self.test_scenarios = self._create_test_scenarios()
        
    def _create_test_scenarios(self) -> List[Dict[str, Any]]:
        """åˆ›å»ºæµ‹è¯•åœºæ™¯"""
        scenarios = []
        
        # ç”µå½±æ¨èåœºæ™¯
        movie_scenarios = [
            {
                'user_profile': '25å²ç¨‹åºå‘˜ï¼Œå–œæ¬¢ç§‘å¹»å’ŒåŠ¨ä½œç”µå½±ï¼Œæœ€è¿‘çœ‹äº†ã€Šç›—æ¢¦ç©ºé—´ã€‹å’Œã€Šé»‘å®¢å¸å›½ã€‹',
                'candidates': ['æ˜Ÿé™…ç©¿è¶Š', 'å¤ä»‡è€…è”ç›Ÿ', 'æ³°å¦å°¼å…‹å·', 'è‚–ç”³å…‹çš„æ•‘èµ', 'é˜¿å‡¡è¾¾'],
                'ground_truth': 'æ˜Ÿé™…ç©¿è¶Š',
                'category': 'ç”µå½±æ¨è'
            },
            {
                'user_profile': '35å²å¥³æ€§ï¼Œå–œæ¬¢æµªæ¼«å–œå‰§ï¼Œç»å¸¸çœ‹ã€Šå‚²æ…¢ä¸åè§ã€‹ç±»å‹çš„ç”µå½±',
                'candidates': ['è¯ºä¸å±±', 'é€Ÿåº¦ä¸æ¿€æƒ…', 'å˜å½¢é‡‘åˆš', 'çˆ±åœ¨æ—¥è½é»„æ˜æ—¶', 'ç»ˆç»“è€…'],
                'ground_truth': 'è¯ºä¸å±±',
                'category': 'ç”µå½±æ¨è'
            },
            {
                'user_profile': 'å¤§å­¦ç”Ÿï¼Œå–œæ¬¢æ‚¬ç–‘æ¨ç†ç±»å†…å®¹ï¼Œæœ€è¿‘åœ¨è¿½ã€Šå¤æ´›å…‹ã€‹',
                'candidates': ['ç¦å°”æ‘©æ–¯', 'é€Ÿåº¦ä¸æ¿€æƒ…', 'å“ˆåˆ©æ³¢ç‰¹', 'è‡´å‘½é­”æœ¯', 'å–œå‰§ä¹‹ç‹'],
                'ground_truth': 'ç¦å°”æ‘©æ–¯',
                'category': 'ç”µå½±æ¨è'
            }
        ]
        
        # å›¾ä¹¦æ¨èåœºæ™¯
        book_scenarios = [
            {
                'user_profile': 'è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæƒ³å­¦ä¹ äººå·¥æ™ºèƒ½ï¼Œæœ‰PythonåŸºç¡€',
                'candidates': ['æ·±åº¦å­¦ä¹ ', 'ç®—æ³•å¯¼è®º', 'ä¸‰ä½“', 'ç™¾å¹´å­¤ç‹¬', 'æœºå™¨å­¦ä¹ å®æˆ˜'],
                'ground_truth': 'æ·±åº¦å­¦ä¹ ',
                'category': 'å›¾ä¹¦æ¨è'
            },
            {
                'user_profile': 'å–œæ¬¢ç§‘å¹»å°è¯´çš„ä¸­å­¦ç”Ÿï¼Œè¯»è¿‡ã€Šä¸‰ä½“ã€‹ç³»åˆ—',
                'candidates': ['é“¶æ²³ç³»æ¼«æ¸¸æŒ‡å—', 'éœ¸é“æ€»è£çˆ±ä¸Šæˆ‘', 'ç™¾å¹´å­¤ç‹¬', 'åŸºåœ°ç³»åˆ—', 'çº¢æ¥¼æ¢¦'],
                'ground_truth': 'é“¶æ²³ç³»æ¼«æ¸¸æŒ‡å—',
                'category': 'å›¾ä¹¦æ¨è'
            }
        ]
        
        # å•†å“æ¨èåœºæ™¯
        product_scenarios = [
            {
                'user_profile': 'å¥èº«çˆ±å¥½è€…ï¼Œæ¯å‘¨å»å¥èº«æˆ¿3æ¬¡ï¼Œéœ€è¦è›‹ç™½è´¨è¡¥å……',
                'candidates': ['ä¹³æ¸…è›‹ç™½ç²‰', 'è·‘æ­¥æœº', 'ç‘œä¼½å«', 'åŒ–å¦†å“', 'è›‹ç™½æ£’'],
                'ground_truth': 'ä¹³æ¸…è›‹ç™½ç²‰',
                'category': 'å•†å“æ¨è'
            },
            {
                'user_profile': 'æ–°æ‰‹å¦ˆå¦ˆï¼Œå®å®6ä¸ªæœˆå¤§ï¼Œéœ€è¦è¾…é£Ÿç”¨å“',
                'candidates': ['å©´å„¿è¾…é£Ÿæœº', 'æˆäººç»´ç”Ÿç´ ', 'å® ç‰©ç”¨å“', 'åŠå…¬ç”¨å“', 'å©´å„¿é¤å…·'],
                'ground_truth': 'å©´å„¿è¾…é£Ÿæœº',
                'category': 'å•†å“æ¨è'
            }
        ]
        
        # åˆå¹¶æ‰€æœ‰åœºæ™¯
        all_scenarios = movie_scenarios + book_scenarios + product_scenarios
        
        # æ ¹æ®é…ç½®é€‰æ‹©æµ‹è¯•æ¡ˆä¾‹æ•°é‡
        selected_scenarios = np.random.choice(
            all_scenarios, 
            size=min(len(all_scenarios), self.config.num_test_cases),
            replace=False
        ).tolist()
        
        return selected_scenarios
        
    def get_test_cases(self) -> List[Dict[str, Any]]:
        """è·å–æµ‹è¯•æ¡ˆä¾‹"""
        return self.test_scenarios

class RealLLMComparator:
    """çœŸå®LLMæ¨¡å‹å¯¹æ¯”å™¨"""
    
    def __init__(self, config: RealModelConfig = None):
        self.config = config or RealModelConfig()
        
        # è·å–å¯ç”¨çš„Ollamaæ¨¡å‹
        self.available_models = self._get_available_models()
        
        # é€‰æ‹©è¦å¯¹æ¯”çš„æ¨¡å‹ï¼ˆä¼˜å…ˆé€‰æ‹©ä¸»æµæ¨¡å‹ï¼‰
        self.selected_models = self._select_models()
        
        # ç»“æœå­˜å‚¨
        self.results_dir = Path('results/hypothesis_validation/real_llm_comparison')
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        logger.info(f"ğŸ”¬ çœŸå®LLMå¯¹æ¯”å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“‹ é€‰æ‹©çš„æ¨¡å‹: {[model for model in self.selected_models]}")
        
    def _get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬Ollamaå’ŒOpenAIï¼‰"""
        available_models = []
        
        # è·å–Ollamaæ¨¡å‹
        if HAS_OLLAMA:
            try:
                models_response = ollama.list()
                ollama_models = [model['name'] for model in models_response['models']]
                available_models.extend(ollama_models)
                logger.info(f"ğŸ“‹ å‘ç° {len(ollama_models)} ä¸ªOllamaæ¨¡å‹: {ollama_models}")
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–Ollamaæ¨¡å‹å¤±è´¥: {str(e)}")
                # æ·»åŠ å¸¸è§æ¨¡å‹ä½œä¸ºfallback
                fallback_ollama = ['llama3:latest', 'qwen3:latest', 'gemma2:2b']
                available_models.extend(fallback_ollama)
                logger.info(f"ğŸ“‹ ä½¿ç”¨Ollama fallbackæ¨¡å‹: {fallback_ollama}")
        
        # æ·»åŠ OpenAIæ¨¡å‹
        if HAS_OPENAI and self.config.use_chatgpt_api and self.config.openai_api_key:
            openai_models = ['gpt-3.5-turbo', 'gpt-4', 'gpt-4o-mini']
            available_models.extend(openai_models)
            logger.info(f"ğŸ“‹ æ·»åŠ OpenAIæ¨¡å‹: {openai_models}")
            
        # ç¡®ä¿è‡³å°‘æœ‰åŸºç¡€æ¨¡å‹
        if not available_models:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹åˆ—è¡¨")
            available_models = ['llama3:latest', 'qwen3:latest']
            
        return available_models
            
    def _select_models(self) -> List[str]:
        """é€‰æ‹©è¦å¯¹æ¯”çš„æ¨¡å‹"""
        selected = []
        
        # Ollamaæ¨¡å‹ä¼˜å…ˆçº§
        ollama_priority = [
            'llama3:latest',
            'llama3.2:latest', 
            'qwen3:latest',
            'gemma2:2b',
            'gpt-oss:latest'
        ]
        
        # é€‰æ‹©å¯ç”¨çš„Ollamaæ¨¡å‹
        for model in ollama_priority:
            if model in self.available_models:
                selected.append(model)
                
        # æ·»åŠ OpenAIæ¨¡å‹
        openai_models = ['gpt-3.5-turbo', 'gpt-4', 'gpt-4o-mini']
        for model in openai_models:
            if model in self.available_models:
                selected.append(model)
                
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼˜å…ˆæ¨¡å‹ï¼Œé€‰æ‹©å‰å‡ ä¸ªå¯ç”¨æ¨¡å‹
        if not selected:
            selected = self.available_models[:3]
            
        # æœ€å¤šé€‰æ‹©3ä¸ªæ¨¡å‹ä»¥èŠ‚çœæ—¶é—´
        return selected[:3]
        
    def run_model_comparison(self) -> Dict[str, Any]:
        """è¿è¡Œæ¨¡å‹å¯¹æ¯”"""
        logger.info("ğŸš€ å¼€å§‹çœŸå®LLMæ¨¡å‹å¯¹æ¯”å®éªŒ...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        dataset = RecommendationTestDataset(self.config)
        test_cases = dataset.get_test_cases()
        
        logger.info(f"ğŸ“Š æµ‹è¯•æ¡ˆä¾‹æ•°: {len(test_cases)}")
        
        # åˆå§‹åŒ–æ¨¡å‹åŒ…è£…å™¨
        model_wrappers = {}
        for model_name in self.selected_models:
            model_wrappers[model_name] = UnifiedModelWrapper(model_name, self.config)
            
        # è¿è¡Œæµ‹è¯•
        results = {}
        
        for model_name, wrapper in model_wrappers.items():
            logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {wrapper.spec['display_name']}")
            
            model_results = {
                'model_info': wrapper.get_model_info(),
                'test_results': [],
                'performance_metrics': {},
                'timing_stats': []
            }
            
            correct_predictions = 0
            total_confidence = 0
            inference_times = []
            
            for i, test_case in enumerate(test_cases):
                logger.info(f"  æµ‹è¯•æ¡ˆä¾‹ {i+1}/{len(test_cases)}: {test_case['category']}")
                
                # ç”Ÿæˆæ¨è
                recommendation = wrapper.generate_recommendation(
                    test_case['user_profile'],
                    test_case['candidates']
                )
                
                # è¯„ä¼°ç»“æœ
                is_correct = (recommendation['recommended_item'] and 
                            recommendation['recommended_item'].strip() == test_case['ground_truth'])
                
                if is_correct:
                    correct_predictions += 1
                    
                total_confidence += recommendation.get('confidence', 0)
                inference_times.append(recommendation.get('inference_time', 0))
                
                # è®°å½•ç»“æœ
                test_result = {
                    'test_case_id': i,
                    'category': test_case['category'],
                    'user_profile': test_case['user_profile'],
                    'candidates': test_case['candidates'],
                    'ground_truth': test_case['ground_truth'],
                    'prediction': recommendation['recommended_item'],
                    'correct': is_correct,
                    'confidence': recommendation.get('confidence', 0),
                    'inference_time': recommendation.get('inference_time', 0),
                    'reason': recommendation.get('reason', ''),
                    'error': recommendation.get('error', None)
                }
                
                model_results['test_results'].append(test_result)
                
                # æ˜¾ç¤ºè¿›åº¦
                if (i + 1) % 10 == 0:
                    current_accuracy = correct_predictions / (i + 1)
                    avg_time = np.mean(inference_times)
                    logger.info(f"    è¿›åº¦: {i+1}/{len(test_cases)}, "
                              f"å½“å‰å‡†ç¡®ç‡: {current_accuracy:.3f}, "
                              f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f}s")
                    
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            accuracy = correct_predictions / len(test_cases) if test_cases else 0
            avg_confidence = total_confidence / len(test_cases) if test_cases else 0
            avg_inference_time = np.mean(inference_times) if inference_times else 0
            
            model_results['performance_metrics'] = {
                'accuracy': accuracy,
                'correct_predictions': correct_predictions,
                'total_test_cases': len(test_cases),
                'average_confidence': avg_confidence,
                'average_inference_time': avg_inference_time,
                'total_inference_time': sum(inference_times),
                'inference_time_std': np.std(inference_times) if inference_times else 0
            }
            
            results[model_name] = model_results
            
            logger.info(f"âœ… {wrapper.spec['display_name']} å®Œæˆ - "
                       f"å‡†ç¡®ç‡: {accuracy:.3f}, "
                       f"å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time:.2f}s")
                       
        return results
        
    def analyze_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå¯¹æ¯”ç»“æœ"""
        logger.info("ğŸ“Š åˆ†æå®éªŒç»“æœ...")
        
        if not results:
            logger.warning("âš ï¸ æ²¡æœ‰ç»“æœå¯åˆ†æ")
            return {}
            
        analysis = {
            'performance_ranking': [],
            'efficiency_analysis': {},
            'category_analysis': {},
            'h4_hypothesis_validation': {}
        }
        
        # 1. æ€§èƒ½æ’å
        performance_data = []
        for model_name, result in results.items():
            metrics = result['performance_metrics']
            model_info = result['model_info']
            
            performance_data.append((
                model_info['display_name'],
                model_name,
                metrics['accuracy'],
                metrics['average_confidence'],
                metrics['average_inference_time']
            ))
            
        # æŒ‰å‡†ç¡®ç‡æ’åº
        performance_data.sort(key=lambda x: x[2], reverse=True)
        analysis['performance_ranking'] = performance_data
        
        # 2. æ•ˆç‡åˆ†æ
        for model_name, result in results.items():
            metrics = result['performance_metrics']
            model_info = result['model_info']
            
            # æ•ˆç‡åˆ†æ•° = å‡†ç¡®ç‡ / (æ¨ç†æ—¶é—´ * æ¨¡å‹å¤§å°)
            efficiency_score = metrics['accuracy'] / (
                metrics['average_inference_time'] * model_info['model_size_gb'] / 10 + 1e-6
            )
            
            analysis['efficiency_analysis'][model_name] = {
                'display_name': model_info['display_name'],
                'accuracy': metrics['accuracy'],
                'inference_time': metrics['average_inference_time'],
                'model_size': model_info['model_size_gb'],
                'efficiency_score': efficiency_score,
                'theoretical_reasoning': model_info['reasoning_score']
            }
            
        # 3. åˆ†ç±»åˆ«åˆ†æ
        categories = set()
        for model_name, result in results.items():
            for test_result in result['test_results']:
                categories.add(test_result['category'])
                
        for category in categories:
            analysis['category_analysis'][category] = {}
            
            for model_name, result in results.items():
                model_info = result['model_info']
                category_results = [tr for tr in result['test_results'] 
                                  if tr['category'] == category]
                
                if category_results:
                    category_accuracy = sum(tr['correct'] for tr in category_results) / len(category_results)
                    analysis['category_analysis'][category][model_name] = {
                        'display_name': model_info['display_name'],
                        'accuracy': category_accuracy,
                        'sample_count': len(category_results)
                    }
                    
        # 4. H4å‡è®¾éªŒè¯
        llama_models = [name for name in results.keys() if 'llama' in name.lower()]
        
        if llama_models:
            # é€‰æ‹©æœ€ä½³çš„Llamaæ¨¡å‹
            best_llama = None
            best_llama_accuracy = 0
            
            for llama_model in llama_models:
                accuracy = results[llama_model]['performance_metrics']['accuracy']
                if accuracy > best_llama_accuracy:
                    best_llama_accuracy = accuracy
                    best_llama = llama_model
                    
            # æ‰¾åˆ°æœ€ä½³Llamaæ¨¡å‹çš„æ’å
            llama_rank = None
            for i, (display_name, model_name, accuracy, _, _) in enumerate(performance_data):
                if model_name == best_llama:
                    llama_rank = i + 1
                    break
                    
            # è®¡ç®—è¯æ®
            evidence_score = 0
            evidence_details = {}
            
            # è¯æ®1: æ’åå‰2
            top_ranking = llama_rank <= 2 if llama_rank else False
            evidence_details['top_ranking'] = top_ranking
            if top_ranking:
                evidence_score += 1
                
            # è¯æ®2: å‡†ç¡®ç‡ä¼˜åŠ¿ 
            accuracy_advantage = best_llama_accuracy > 0.6  # é˜ˆå€¼å¯è°ƒæ•´
            evidence_details['accuracy_advantage'] = accuracy_advantage
            if accuracy_advantage:
                evidence_score += 1
                
            # è¯æ®3: æ•ˆç‡å¹³è¡¡
            if best_llama in analysis['efficiency_analysis']:
                llama_efficiency = analysis['efficiency_analysis'][best_llama]['efficiency_score']
                efficiency_balance = llama_efficiency > np.mean([
                    data['efficiency_score'] for data in analysis['efficiency_analysis'].values()
                ])
                evidence_details['efficiency_balance'] = efficiency_balance
                if efficiency_balance:
                    evidence_score += 1
                    
            # è¯æ®4: å¤šåœºæ™¯è¡¨ç°
            consistent_performance = True
            if analysis['category_analysis']:
                llama_category_scores = []
                for category_data in analysis['category_analysis'].values():
                    if best_llama in category_data:
                        llama_category_scores.append(category_data[best_llama]['accuracy'])
                        
                if llama_category_scores:
                    # æ£€æŸ¥æ˜¯å¦åœ¨æ‰€æœ‰ç±»åˆ«ä¸­éƒ½æœ‰åˆç†è¡¨ç°
                    min_category_accuracy = min(llama_category_scores)
                    consistent_performance = min_category_accuracy > 0.3
                    
            evidence_details['consistent_performance'] = consistent_performance
            if consistent_performance:
                evidence_score += 1
                
            analysis['h4_hypothesis_validation'] = {
                'best_llama_model': best_llama,
                'best_llama_display_name': results[best_llama]['model_info']['display_name'] if best_llama else None,
                'llama_rank': llama_rank,
                'llama_accuracy': best_llama_accuracy,
                'evidence_score': evidence_score,
                'max_evidence': 4,
                'evidence_details': evidence_details,
                'hypothesis_supported': evidence_score >= 2,
                'total_models_tested': len(results)
            }
            
        return analysis
        
    def create_visualizations(self, results: Dict[str, Any], analysis: Dict[str, Any]):
        """åˆ›å»ºå¯è§†åŒ–"""
        logger.info("ğŸ“Š åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        
        if not results or not analysis:
            logger.warning("âš ï¸ æ— æ•°æ®è¿›è¡Œå¯è§†åŒ–")
            return
            
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Real LLM Models Comparison for Recommendation Tasks - H4 Validation', 
                    fontsize=16, fontweight='bold')
        
        # 1. å‡†ç¡®ç‡å¯¹æ¯”
        if 'performance_ranking' in analysis:
            rankings = analysis['performance_ranking']
            display_names = [item[0] for item in rankings]
            accuracies = [item[2] for item in rankings]
            
            colors = ['gold' if 'llama' in name.lower() else 'skyblue' for name in display_names]
            bars = axes[0, 0].bar(display_names, accuracies, color=colors, alpha=0.8)
            
            axes[0, 0].set_title('Model Accuracy Comparison')
            axes[0, 0].set_ylabel('Accuracy')
            axes[0, 0].tick_params(axis='x', rotation=45)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, acc in zip(bars, accuracies):
                axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
                               
        # 2. æ¨ç†æ—¶é—´å¯¹æ¯”
        if 'efficiency_analysis' in analysis:
            eff_data = analysis['efficiency_analysis']
            models = [data['display_name'] for data in eff_data.values()]
            inference_times = [data['inference_time'] for data in eff_data.values()]
            
            colors = ['orange' if 'llama' in name.lower() else 'lightcoral' for name in models]
            bars = axes[0, 1].bar(models, inference_times, color=colors, alpha=0.8)
            
            axes[0, 1].set_title('Average Inference Time')
            axes[0, 1].set_ylabel('Time (seconds)')
            axes[0, 1].tick_params(axis='x', rotation=45)
            
            for bar, time_val in zip(bars, inference_times):
                axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                               f'{time_val:.1f}s', ha='center', va='bottom')
                               
        # 3. æ•ˆç‡vsæ€§èƒ½æ•£ç‚¹å›¾
        if 'efficiency_analysis' in analysis:
            eff_data = analysis['efficiency_analysis']
            models = list(eff_data.keys())
            
            x_values = [eff_data[model]['efficiency_score'] for model in models]
            y_values = [eff_data[model]['accuracy'] for model in models]
            labels = [eff_data[model]['display_name'] for model in models]
            
            colors = ['red' if 'llama' in label.lower() else 'blue' for label in labels]
            sizes = [120 if 'llama' in label.lower() else 80 for label in labels]
            
            axes[0, 2].scatter(x_values, y_values, c=colors, s=sizes, alpha=0.7)
            
            for i, label in enumerate(labels):
                axes[0, 2].annotate(label, (x_values[i], y_values[i]),
                                   xytext=(5, 5), textcoords='offset points', fontsize=9)
                                   
            axes[0, 2].set_title('Efficiency vs Performance')
            axes[0, 2].set_xlabel('Efficiency Score')
            axes[0, 2].set_ylabel('Accuracy')
            axes[0, 2].grid(True, alpha=0.3)
            
        # 4. åˆ†ç±»åˆ«æ€§èƒ½
        if 'category_analysis' in analysis:
            categories = list(analysis['category_analysis'].keys())
            
            if categories:
                # å‡†å¤‡æ•°æ®
                models_in_categories = set()
                for category_data in analysis['category_analysis'].values():
                    models_in_categories.update(category_data.keys())
                    
                model_names = list(models_in_categories)
                category_matrix = []
                
                for model in model_names:
                    row = []
                    for category in categories:
                        if model in analysis['category_analysis'][category]:
                            row.append(analysis['category_analysis'][category][model]['accuracy'])
                        else:
                            row.append(0)
                    category_matrix.append(row)
                    
                # åˆ›å»ºçƒ­åŠ›å›¾
                im = axes[1, 0].imshow(category_matrix, cmap='RdYlBu_r', aspect='auto')
                
                # è®¾ç½®æ ‡ç­¾
                display_names = [analysis['efficiency_analysis'][model]['display_name'] 
                               if model in analysis['efficiency_analysis'] else model 
                               for model in model_names]
                
                axes[1, 0].set_xticks(range(len(categories)))
                axes[1, 0].set_xticklabels(categories, rotation=45, ha='right')
                axes[1, 0].set_yticks(range(len(model_names)))
                axes[1, 0].set_yticklabels(display_names)
                axes[1, 0].set_title('Performance by Category')
                
                # æ·»åŠ é¢œè‰²æ¡
                plt.colorbar(im, ax=axes[1, 0], fraction=0.046, pad=0.04)
                
                # æ·»åŠ æ•°å€¼æ ‡æ³¨
                for i in range(len(model_names)):
                    for j in range(len(categories)):
                        text = axes[1, 0].text(j, i, f'{category_matrix[i][j]:.2f}',
                                             ha="center", va="center", color="black", fontsize=8)
                                             
        # 5. H4å‡è®¾éªŒè¯æ€»ç»“
        axes[1, 1].axis('off')
        
        if 'h4_hypothesis_validation' in analysis:
            h4_data = analysis['h4_hypothesis_validation']
            
            summary_text = f"""
H4 Hypothesis Validation Summary:

Best Llama Model: {h4_data.get('best_llama_display_name', 'N/A')}
Performance Ranking: #{h4_data.get('llama_rank', 'N/A')}
Accuracy Achievement: {h4_data.get('llama_accuracy', 0):.3f}

Evidence Analysis:
â€¢ Top Ranking (â‰¤2): {'âœ…' if h4_data.get('evidence_details', {}).get('top_ranking', False) else 'âŒ'}
â€¢ Accuracy Advantage: {'âœ…' if h4_data.get('evidence_details', {}).get('accuracy_advantage', False) else 'âŒ'}
â€¢ Efficiency Balance: {'âœ…' if h4_data.get('evidence_details', {}).get('efficiency_balance', False) else 'âŒ'}
â€¢ Consistent Performance: {'âœ…' if h4_data.get('evidence_details', {}).get('consistent_performance', False) else 'âŒ'}

Evidence Score: {h4_data.get('evidence_score', 0)}/{h4_data.get('max_evidence', 4)}

Conclusion: {'âœ… H4 HYPOTHESIS SUPPORTED' if h4_data.get('hypothesis_supported', False) else 'âŒ H4 HYPOTHESIS NOT SUPPORTED'}

Models Tested: {h4_data.get('total_models_tested', 0)}
"""
            
            axes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes,
                           fontsize=11, verticalalignment='top', fontfamily='monospace',
                           bbox=dict(boxstyle="round,pad=0.5", facecolor='lightyellow', alpha=0.8))
                           
        # 6. æ¨¡å‹å¤§å°vsæ€§èƒ½
        if 'efficiency_analysis' in analysis:
            eff_data = analysis['efficiency_analysis']
            
            model_sizes = [data['model_size'] for data in eff_data.values()]
            accuracies = [data['accuracy'] for data in eff_data.values()]
            labels = [data['display_name'] for data in eff_data.values()]
            
            colors = ['green' if 'llama' in label.lower() else 'purple' for label in labels]
            sizes = [100 if 'llama' in label.lower() else 60 for label in labels]
            
            axes[1, 2].scatter(model_sizes, accuracies, c=colors, s=sizes, alpha=0.7)
            
            for i, label in enumerate(labels):
                axes[1, 2].annotate(f'{label}\n({model_sizes[i]:.1f}GB)', 
                                   (model_sizes[i], accuracies[i]),
                                   xytext=(5, 5), textcoords='offset points', fontsize=8)
                                   
            axes[1, 2].set_title('Model Size vs Performance')
            axes[1, 2].set_xlabel('Model Size (GB)')
            axes[1, 2].set_ylabel('Accuracy')
            axes[1, 2].grid(True, alpha=0.3)
            
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plot_file = self.results_dir / f'real_llm_comparison_{self.timestamp}.png'
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        logger.info(f"âœ… å¯è§†åŒ–ä¿å­˜è‡³: {plot_file}")
        
        plt.show()
        
    def save_results(self, results: Dict[str, Any], analysis: Dict[str, Any]):
        """ä¿å­˜ç»“æœ"""
        logger.info("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        
        final_results = {
            'timestamp': self.timestamp,
            'config': {
                'num_test_cases': self.config.num_test_cases,
                'models_tested': self.selected_models,
                'random_seed': self.config.random_seed
            },
            'raw_results': results,
            'analysis': analysis
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.results_dir / f'real_llm_comparison_results_{self.timestamp}.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False, default=str)
            
        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_report(final_results)
        report_file = self.results_dir / f'real_H4_validation_report_{self.timestamp}.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        logger.info(f"âœ… ç»“æœä¿å­˜è‡³: {results_file}")
        logger.info(f"âœ… æŠ¥å‘Šä¿å­˜è‡³: {report_file}")
        
    def _generate_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        analysis = results.get('analysis', {})
        h4_data = analysis.get('h4_hypothesis_validation', {})
        
        report = f"""# H4å‡è®¾éªŒè¯æŠ¥å‘Š: çœŸå®LLMæ¨¡å‹æ¨èæ€§èƒ½å¯¹æ¯”

**å®éªŒæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**å®éªŒç±»å‹**: çœŸå®LLMæ¨¡å‹å¯¹æ¯”  
**å‡è®¾é™ˆè¿°**: Llama3åœ¨æ¨èä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–LLMæ¨¡å‹

## ğŸ“‹ å®éªŒæ¦‚è¿°

æœ¬å®éªŒä½¿ç”¨Ollamaè°ƒç”¨çœŸå®çš„LLMæ¨¡å‹ï¼Œåœ¨å®é™…æ¨èåœºæ™¯ä¸­éªŒè¯H4å‡è®¾ã€‚

### å®éªŒé…ç½®
- **æµ‹è¯•æ¡ˆä¾‹æ•°**: {results['config']['num_test_cases']}
- **å‚ä¸æ¨¡å‹**: {', '.join([model.split(':')[0].title() for model in results['config']['models_tested']])}
- **æµ‹è¯•åœºæ™¯**: ç”µå½±æ¨èã€å›¾ä¹¦æ¨èã€å•†å“æ¨è
- **è¯„ä¼°æŒ‡æ ‡**: å‡†ç¡®ç‡ã€æ¨ç†æ—¶é—´ã€ç½®ä¿¡åº¦

## ğŸ† å®éªŒç»“æœ

### æ€§èƒ½æ’å
"""
        
        if 'performance_ranking' in analysis:
            for i, (display_name, model_name, accuracy, confidence, inference_time) in enumerate(analysis['performance_ranking'], 1):
                emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
                report += f"{emoji} **{display_name}**: å‡†ç¡®ç‡ {accuracy:.3f}, ç½®ä¿¡åº¦ {confidence:.1f}, æ¨ç†æ—¶é—´ {inference_time:.2f}s\n"
                
        report += f"""

### H4å‡è®¾éªŒè¯ç»“æœ

**æœ€ä½³Llamaæ¨¡å‹**: {h4_data.get('best_llama_display_name', 'N/A')}  
**æ€§èƒ½æ’å**: #{h4_data.get('llama_rank', 'N/A')}  
**å‡†ç¡®ç‡**: {h4_data.get('llama_accuracy', 0):.3f}  

**è¯æ®åˆ†æ**:
1. **é¡¶çº§æ’å** (å‰2å): {'âœ… è¾¾æˆ' if h4_data.get('evidence_details', {}).get('top_ranking', False) else 'âŒ æœªè¾¾æˆ'}
2. **å‡†ç¡®ç‡ä¼˜åŠ¿** (>60%): {'âœ… è¾¾æˆ' if h4_data.get('evidence_details', {}).get('accuracy_advantage', False) else 'âŒ æœªè¾¾æˆ'}
3. **æ•ˆç‡å¹³è¡¡**: {'âœ… ä¼˜äºå¹³å‡' if h4_data.get('evidence_details', {}).get('efficiency_balance', False) else 'âŒ ä½äºå¹³å‡'}
4. **ä¸€è‡´æ€§è¡¨ç°**: {'âœ… å„åœºæ™¯ç¨³å®š' if h4_data.get('evidence_details', {}).get('consistent_performance', False) else 'âŒ è¡¨ç°ä¸ç¨³å®š'}

**ç»¼åˆè¯„åˆ†**: {h4_data.get('evidence_score', 0)}/{h4_data.get('max_evidence', 4)}

## ğŸ“Š H4å‡è®¾éªŒè¯ç»“è®º

### {"âœ… **å‡è®¾å¾—åˆ°æ”¯æŒ**" if h4_data.get('hypothesis_supported', False) else "âŒ **å‡è®¾æœªå¾—åˆ°å……åˆ†æ”¯æŒ**"}

"""
        
        if h4_data.get('hypothesis_supported', False):
            report += f"""**æ”¯æŒç†ç”±**:
- Llamaæ¨¡å‹åœ¨{h4_data.get('total_models_tested', 0)}ä¸ªæµ‹è¯•æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²
- åœ¨çœŸå®æ¨èåœºæ™¯ä¸­å±•ç°äº†{h4_data.get('evidence_score', 0)}é¡¹å…³é”®ä¼˜åŠ¿
- å…¼é¡¾äº†æ€§èƒ½ã€æ•ˆç‡å’Œä¸€è‡´æ€§çš„å¤šç»´åº¦è¦æ±‚
"""
        else:
            report += """**ä¸æ”¯æŒåŸå› **:
- æ€§èƒ½æ’åæœªè¾¾åˆ°é¢„æœŸé¢†å…ˆåœ°ä½
- åœ¨æ•ˆç‡æˆ–ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ä¸è¶³
- éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ä»¥è¾¾åˆ°å‡è®¾è¦æ±‚
"""
        
        report += f"""

## ğŸ” è¯¦ç»†åˆ†æ

### å„æ¨¡å‹è¡¨ç°ç‰¹ç‚¹
"""
        
        if 'efficiency_analysis' in analysis:
            for model_name, data in analysis['efficiency_analysis'].items():
                report += f"""
**{data['display_name']}**:
- å‡†ç¡®ç‡: {data['accuracy']:.3f}
- å¹³å‡æ¨ç†æ—¶é—´: {data['inference_time']:.2f}ç§’
- æ¨¡å‹å¤§å°: {data['model_size']:.1f}GB
- æ•ˆç‡åˆ†æ•°: {data['efficiency_score']:.3f}
- ç†è®ºæ¨ç†èƒ½åŠ›: {data['theoretical_reasoning']:.2f}
"""
        
        report += f"""

### åˆ†åœºæ™¯è¡¨ç°åˆ†æ
"""
        
        if 'category_analysis' in analysis:
            for category, category_data in analysis['category_analysis'].items():
                report += f"\n**{category}**:\n"
                for model_name, model_data in category_data.items():
                    report += f"- {model_data['display_name']}: {model_data['accuracy']:.3f} ({model_data['sample_count']}ä¸ªæ ·æœ¬)\n"
                    
        report += f"""

## ğŸ¯ å®é™…åº”ç”¨å»ºè®®

### æ¨¡å‹é€‰æ‹©å»ºè®®
"""
        
        if 'performance_ranking' in analysis and analysis['performance_ranking']:
            best_model = analysis['performance_ranking'][0]
            report += f"""
**æ¨èæ¨¡å‹**: {best_model[0]}
- åœ¨å‡†ç¡®ç‡æ–¹é¢è¡¨ç°æœ€ä¼˜: {best_model[2]:.3f}
- æ¨ç†æ—¶é—´: {best_model[4]:.2f}ç§’
- é€‚ç”¨åœºæ™¯: å¯¹å‡†ç¡®ç‡è¦æ±‚è¾ƒé«˜çš„æ¨èç³»ç»Ÿ
"""
        
        if h4_data.get('hypothesis_supported', False):
            report += f"""
**Llama3ä¼˜åŠ¿**:
- åœ¨å¤šä¸ªè¯„ä¼°ç»´åº¦ä¸Šå±•ç°å‡ºå¹³è¡¡çš„æ€§èƒ½
- ç‰¹åˆ«é€‚åˆéœ€è¦é«˜è´¨é‡æ¨ç†çš„æ¨èåœºæ™¯
- æ¨¡å‹å¤§å°å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡è¾ƒå¥½
"""
        
        report += f"""

### éƒ¨ç½²è€ƒè™‘å› ç´ 
1. **æ€§èƒ½éœ€æ±‚**: æ ¹æ®ä¸šåŠ¡å¯¹å‡†ç¡®ç‡çš„å…·ä½“è¦æ±‚é€‰æ‹©æ¨¡å‹
2. **èµ„æºé™åˆ¶**: è€ƒè™‘GPUå†…å­˜å’Œæ¨ç†å»¶è¿Ÿçš„çº¦æŸ
3. **åœºæ™¯é€‚é…**: ä¸åŒæ¨èåœºæ™¯å¯èƒ½éœ€è¦ä¸åŒçš„æ¨¡å‹ç­–ç•¥

## ğŸš§ å®éªŒå±€é™æ€§

1. **æµ‹è¯•è§„æ¨¡**: åŸºäº{results['config']['num_test_cases']}ä¸ªæµ‹è¯•æ¡ˆä¾‹çš„æœ‰é™è¯„ä¼°
2. **åœºæ™¯è¦†ç›–**: ä¸»è¦è¦†ç›–ç”µå½±ã€å›¾ä¹¦ã€å•†å“ä¸‰ä¸ªé¢†åŸŸ
3. **è¯„ä¼°æ ‡å‡†**: ä½¿ç”¨é¢„å®šä¹‰çš„ground truthï¼Œå¯èƒ½å­˜åœ¨ä¸»è§‚æ€§
4. **æ¨¡å‹ç‰ˆæœ¬**: åŸºäºå½“å‰å¯ç”¨çš„Ollamaæ¨¡å‹ç‰ˆæœ¬

## ğŸ“š åç»­å·¥ä½œå»ºè®®

1. **æ‰©å¤§æµ‹è¯•è§„æ¨¡**: å¢åŠ æµ‹è¯•æ¡ˆä¾‹æ•°é‡å’Œåœºæ™¯ç±»å‹
2. **çœŸå®æ•°æ®éªŒè¯**: åœ¨å®é™…ç”¨æˆ·æ•°æ®ä¸Šè¿›è¡ŒA/Bæµ‹è¯•
3. **æˆæœ¬æ•ˆç›Šåˆ†æ**: è¯¦ç»†åˆ†æä¸åŒæ¨¡å‹çš„ROI
4. **ä¸“ä¸šåŒ–å¾®è°ƒ**: ä¸ºç‰¹å®šæ¨èåœºæ™¯å¾®è°ƒæ¨¡å‹

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**å®éªŒç‰ˆæœ¬**: çœŸå®LLMæ¨¡å‹å¯¹æ¯”v1.0
"""
        
        return report
        
    def run_complete_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        logger.info("ğŸš€ å¼€å§‹çœŸå®LLMæ¨¡å‹å¯¹æ¯”å®éªŒ...")
        
        # 1. æ¨¡å‹å¯¹æ¯”
        results = self.run_model_comparison()
        
        if not results:
            logger.error("âŒ å®éªŒå¤±è´¥ï¼Œæ²¡æœ‰è·å¾—ç»“æœ")
            return None
            
        # 2. ç»“æœåˆ†æ  
        analysis = self.analyze_results(results)
        
        # 3. åˆ›å»ºå¯è§†åŒ–
        self.create_visualizations(results, analysis)
        
        # 4. ä¿å­˜ç»“æœ
        self.save_results(results, analysis)
        
        logger.info("âœ… çœŸå®LLMæ¨¡å‹å¯¹æ¯”å®éªŒå®Œæˆï¼")
        
        return results, analysis

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ”¬ å¼€å§‹çœŸå®LLMæ¨¡å‹H4å‡è®¾éªŒè¯å®éªŒ...")
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    
    # åˆ›å»ºå¯¹æ¯”å™¨
    comparator = RealLLMComparator()
    
    # è¿è¡Œå®éªŒ
    results, analysis = comparator.run_complete_experiment()
    
    if results:
        logger.info("ğŸ‰ å®éªŒæˆåŠŸå®Œæˆï¼")
        
        # æ˜¾ç¤ºå…³é”®ç»“æœ
        if 'h4_hypothesis_validation' in analysis:
            h4_data = analysis['h4_hypothesis_validation']
            logger.info(f"ğŸ“Š H4å‡è®¾éªŒè¯ç»“æœ: {'âœ… æ”¯æŒ' if h4_data.get('hypothesis_supported', False) else 'âŒ ä¸æ”¯æŒ'}")
            logger.info(f"ğŸ† æœ€ä½³Llamaæ¨¡å‹: {h4_data.get('best_llama_display_name', 'N/A')}")
            logger.info(f"ğŸ“ˆ è¯æ®è¯„åˆ†: {h4_data.get('evidence_score', 0)}/{h4_data.get('max_evidence', 4)}")
    else:
        logger.error("âŒ å®éªŒå¤±è´¥")

if __name__ == "__main__":
    main()
