#!/usr/bin/env python3
"""
WWW2026æ‰©å±•å®éªŒï¼šå¤§è§„æ¨¡è‡ªé€‚åº”å±‚æˆªå–ä¸å…¨é¢æ€§èƒ½è¯„ä¼°

æ‰©å±•ç‰¹æ€§ï¼š
1. å¢åŠ æ ·æœ¬è§„æ¨¡ï¼ˆ500-1000æ ·æœ¬ï¼‰
2. ä½¿ç”¨æ›´å¤šAmazonç±»åˆ«æ•°æ®
3. æ·»åŠ NDCG@5, MRRç­‰æ¨èè¯„ä¼°æŒ‡æ ‡
4. ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”
5. æ›´å…¨é¢çš„æ€§èƒ½åˆ†æå’Œå¯è§†åŒ–
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥åŸºç¡€å®éªŒæ¨¡å—
from experiments.www2026_adaptive_distillation import *
import pandas as pd
from sklearn.metrics import ndcg_score, mean_squared_error
import numpy as np
from typing import List, Dict, Any, Tuple
import logging

logger = logging.getLogger(__name__)

@dataclass
class ExtendedExperimentConfig(ExperimentConfig):
    """æ‰©å±•å®éªŒé…ç½®"""
    experiment_name: str = "www2026_extended_adaptive_distillation"
    
    # æ‰©å¤§æ•°æ®è§„æ¨¡
    analysis_samples: int = 200  # å±‚é‡è¦æ€§åˆ†ææ ·æœ¬æ•°
    training_samples: int = 500  # è®­ç»ƒæ ·æœ¬æ•°
    evaluation_samples: int = 200  # è¯„ä¼°æ ·æœ¬æ•°
    
    # ä½¿ç”¨æ›´å¤šAmazonç±»åˆ«
    categories: List[str] = field(default_factory=lambda: [
        "Electronics", "Books", "All_Beauty", 
        "Home_and_Kitchen", "Sports_and_Outdoors",
        "Arts_Crafts_and_Sewing", "Automotive"
    ])
    
    # å¢åŠ è®­ç»ƒè½®æ•°
    num_epochs: int = 10
    
    # è¯„ä¼°æŒ‡æ ‡
    evaluation_metrics: List[str] = field(default_factory=lambda: [
        "mse", "mae", "ndcg@5", "ndcg@10", "mrr", "accuracy"
    ])
    
    # åŸºçº¿æ–¹æ³•å¯¹æ¯”
    baseline_methods: List[str] = field(default_factory=lambda: [
        "random_selection", "uniform_selection", "top_bottom_selection"
    ])

class BaselineSelector:
    """åŸºçº¿å±‚é€‰æ‹©æ–¹æ³•"""
    
    def __init__(self, config: ExtendedExperimentConfig):
        self.config = config
    
    def random_selection(self, total_layers: int, target_layers: int) -> List[int]:
        """éšæœºé€‰æ‹©å±‚"""
        np.random.seed(42)  # ä¿è¯å¯é‡å¤æ€§
        return sorted(np.random.choice(total_layers, target_layers, replace=False))
    
    def uniform_selection(self, total_layers: int, target_layers: int) -> List[int]:
        """å‡åŒ€åˆ†å¸ƒé€‰æ‹©å±‚"""
        indices = np.linspace(0, total_layers - 1, target_layers, dtype=int)
        return sorted(indices.tolist())
    
    def top_bottom_selection(self, total_layers: int, target_layers: int) -> List[int]:
        """é¡¶éƒ¨+åº•éƒ¨é€‰æ‹©ï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰"""
        top_count = target_layers // 2
        bottom_count = target_layers - top_count
        
        # é€‰æ‹©å‰å‡ å±‚å’Œåå‡ å±‚
        selected = list(range(bottom_count)) + list(range(total_layers - top_count, total_layers))
        return sorted(selected)

class RecommendationEvaluator:
    """æ¨èç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self, config: ExtendedExperimentConfig):
        self.config = config
    
    def evaluate_model(self, model: CompactStudentModel, test_dataset: DistillationDataset) -> Dict[str, float]:
        """å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        model.eval()
        device = next(model.parameters()).device
        
        test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size, shuffle=False)
        
        all_predictions = []
        all_targets = []
        all_teacher_predictions = []
        
        with torch.no_grad():
            for batch in test_loader:
                input_ids = batch['input_ids'].to(device)
                target_ratings = batch['target_rating'].to(device)
                teacher_ratings = batch['teacher_rating'].to(device)
                
                outputs = model(input_ids)
                predictions = outputs['recommendation_score']
                
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(target_ratings.cpu().numpy())
                all_teacher_predictions.extend(teacher_ratings.cpu().numpy())
        
        predictions = np.array(all_predictions)
        targets = np.array(all_targets)
        teacher_predictions = np.array(all_teacher_predictions)
        
        # è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
        metrics = {}
        
        # å›å½’æŒ‡æ ‡
        metrics['mse'] = mean_squared_error(targets, predictions)
        metrics['mae'] = np.mean(np.abs(targets - predictions))
        metrics['rmse'] = np.sqrt(metrics['mse'])
        
        # å‡†ç¡®ç‡ï¼ˆÂ±0.5èŒƒå›´å†…ï¼‰
        metrics['accuracy'] = np.mean(np.abs(targets - predictions) <= 0.5)
        
        # æ’åºæŒ‡æ ‡ï¼ˆå°†è¯„åˆ†ä½œä¸ºç›¸å…³æ€§ï¼‰
        try:
            # NDCG@5
            ndcg_5_scores = []
            ndcg_10_scores = []
            mrr_scores = []
            
            # ä¸ºäº†è®¡ç®—NDCGï¼Œæˆ‘ä»¬éœ€è¦æ¨¡æ‹Ÿç”¨æˆ·-ç‰©å“çŸ©é˜µ
            for i in range(0, len(predictions), 10):  # æ¯10ä¸ªä½œä¸ºä¸€ä¸ªç”¨æˆ·çš„æ¨èåˆ—è¡¨
                end_idx = min(i + 10, len(predictions))
                user_targets = targets[i:end_idx].reshape(1, -1)
                user_predictions = predictions[i:end_idx].reshape(1, -1)
                
                if len(user_targets[0]) >= 5:
                    ndcg_5 = ndcg_score(user_targets, user_predictions, k=5)
                    ndcg_5_scores.append(ndcg_5)
                
                if len(user_targets[0]) >= 10:
                    ndcg_10 = ndcg_score(user_targets, user_predictions, k=10)
                    ndcg_10_scores.append(ndcg_10)
                
                # MRRè®¡ç®—ï¼ˆæ‰¾åˆ°æœ€é«˜è¯„åˆ†çš„ä½ç½®ï¼‰
                if len(user_targets[0]) > 0:
                    sorted_indices = np.argsort(user_predictions[0])[::-1]  # é™åº
                    best_target_idx = np.argmax(user_targets[0])
                    rank = np.where(sorted_indices == best_target_idx)[0][0] + 1
                    mrr_scores.append(1.0 / rank)
            
            metrics['ndcg@5'] = np.mean(ndcg_5_scores) if ndcg_5_scores else 0.0
            metrics['ndcg@10'] = np.mean(ndcg_10_scores) if ndcg_10_scores else 0.0
            metrics['mrr'] = np.mean(mrr_scores) if mrr_scores else 0.0
            
        except Exception as e:
            logger.warning(f"æ’åºæŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            metrics['ndcg@5'] = 0.0
            metrics['ndcg@10'] = 0.0
            metrics['mrr'] = 0.0
        
        # ä¸æ•™å¸ˆæ¨¡å‹çš„å¯¹æ¯”
        metrics['teacher_mse'] = mean_squared_error(targets, teacher_predictions)
        metrics['distillation_gap'] = metrics['mse'] - metrics['teacher_mse']
        
        return metrics

def extended_main():
    """æ‰©å±•ä¸»å®éªŒå‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹WWW2026æ‰©å±•è‡ªé€‚åº”å±‚æˆªå–å®éªŒ")
    
    # 1. åˆå§‹åŒ–æ‰©å±•é…ç½®
    config = ExtendedExperimentConfig()
    output_dir = Path(config.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 2. å‡†å¤‡æ›´å¤§è§„æ¨¡çš„æ•°æ®
    logger.info("ğŸ“Š å‡†å¤‡æ‰©å±•å®éªŒæ•°æ®...")
    samples = []
    
    # ä½¿ç”¨æ›´å¤šAmazonç±»åˆ«
    for category in config.categories:
        for i in range(150):  # æ¯ç±»åˆ«150ä¸ªæ ·æœ¬
            sample = {
                'input_text': f"è¿™æ˜¯ä¸€ä¸ªå…³äº{category}çš„è¯¦ç»†ç”¨æˆ·è¯„è®ºç¤ºä¾‹ {i}. äº§å“è´¨é‡å¾ˆå¥½ï¼Œå€¼å¾—æ¨èã€‚",
                'user_id': f"user_{i % 50}",  # 50ä¸ªä¸åŒç”¨æˆ·
                'item_id': f"item_{category}_{i}",
                'rating': np.random.choice([1, 2, 3, 4, 5], p=[0.05, 0.1, 0.2, 0.35, 0.3]),
                'category': category
            }
            samples.append(sample)
    
    logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(samples)}ä¸ªæ ·æœ¬ï¼Œæ¶µç›–{len(config.categories)}ä¸ªç±»åˆ«")
    
    # 3. æ•™å¸ˆæ¨¡å‹å“åº”ç”Ÿæˆï¼ˆå¤§è§„æ¨¡ï¼‰
    teacher_proxy = TeacherModelProxy(config)
    
    # åˆ†æ‰¹ç”Ÿæˆæ•™å¸ˆå“åº”
    all_teacher_responses = []
    batch_size = 50
    total_samples = min(len(samples), config.training_samples + config.evaluation_samples)
    
    for i in range(0, total_samples, batch_size):
        end_idx = min(i + batch_size, total_samples)
        batch_samples = samples[i:end_idx]
        
        logger.info(f"ğŸ“ ç”Ÿæˆæ•™å¸ˆå“åº”æ‰¹æ¬¡ {i//batch_size + 1}/{(total_samples-1)//batch_size + 1}")
        batch_responses = teacher_proxy.generate_responses(batch_samples)
        all_teacher_responses.extend(batch_responses)
    
    # 4. å¤§è§„æ¨¡å±‚é‡è¦æ€§åˆ†æ
    analyzer = LayerImportanceAnalyzer(config)
    analysis_samples = samples[:config.analysis_samples]
    analysis_responses = all_teacher_responses[:config.analysis_samples]
    
    importance_results = analyzer.analyze_all_methods(analysis_samples, analysis_responses)
    
    # 5. å¤šç§å±‚é€‰æ‹©æ–¹æ³•å¯¹æ¯”
    selector = AdaptiveLayerSelector(config)
    baseline_selector = BaselineSelector(config)
    
    selection_results = {}
    target_layers = int(config.teacher_layers * config.target_compression_ratio)
    
    # è‡ªé€‚åº”æ–¹æ³•
    for method_name, importance_scores in importance_results.items():
        selected_layers = selector.select_layers(importance_scores, method_name)
        selection_results[method_name] = {
            'importance_scores': importance_scores,
            'selected_layers': selected_layers,
            'compression_ratio': len(selected_layers) / config.teacher_layers,
            'method_type': 'adaptive'
        }
    
    # åŸºçº¿æ–¹æ³•
    for baseline_method in config.baseline_methods:
        if hasattr(baseline_selector, baseline_method):
            selected_layers = getattr(baseline_selector, baseline_method)(config.teacher_layers, target_layers)
            selection_results[baseline_method] = {
                'selected_layers': selected_layers,
                'compression_ratio': len(selected_layers) / config.teacher_layers,
                'method_type': 'baseline'
            }
    
    # 6. å¤šæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
    logger.info("ğŸ”¥ å¼€å§‹å¤šæ¨¡å‹è®­ç»ƒä¸å…¨é¢è¯„ä¼°...")
    
    # å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
    train_samples = samples[:config.training_samples]
    train_responses = all_teacher_responses[:config.training_samples]
    
    test_samples = samples[config.training_samples:config.training_samples + config.evaluation_samples]
    test_responses = all_teacher_responses[config.training_samples:config.training_samples + config.evaluation_samples]
    
    # æ•°æ®é›†å‡†å¤‡
    train_dataset = DistillationDataset(train_samples, train_responses)
    test_dataset = DistillationDataset(test_samples, test_responses, train_dataset.tokenizer)
    
    # éªŒè¯é›†
    val_split = int(len(train_samples) * 0.2)
    val_dataset = DistillationDataset(
        train_samples[-val_split:], 
        train_responses[-val_split:], 
        train_dataset.tokenizer
    )
    train_dataset = DistillationDataset(
        train_samples[:-val_split], 
        train_responses[:-val_split], 
        train_dataset.tokenizer
    )
    
    # è¯„ä¼°å™¨
    evaluator = RecommendationEvaluator(config)
    
    # å¤šæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ç»“æœ
    model_results = {}
    
    for method_name, selection_info in selection_results.items():
        logger.info(f"ğŸ—ï¸ è®­ç»ƒ{method_name}æ–¹æ³•çš„å­¦ç”Ÿæ¨¡å‹...")
        
        # æ„å»ºå­¦ç”Ÿæ¨¡å‹
        student_model = CompactStudentModel(config, selection_info['selected_layers'])
        
        # è®­ç»ƒæ¨¡å‹
        trainer = DistillationTrainer(config, student_model, train_dataset, val_dataset)
        training_results = trainer.train()
        
        # è¯„ä¼°æ¨¡å‹
        eval_metrics = evaluator.evaluate_model(student_model, test_dataset)
        
        # ä¿å­˜ç»“æœ
        model_results[method_name] = {
            'selection_info': selection_info,
            'training_results': training_results,
            'evaluation_metrics': eval_metrics,
            'model_parameters': student_model.count_parameters()
        }
        
        logger.info(f"âœ… {method_name}æ¨¡å‹è®­ç»ƒå®Œæˆ - MSE: {eval_metrics['mse']:.4f}, NDCG@5: {eval_metrics['ndcg@5']:.4f}")
    
    # 7. ä¿å­˜æ‰©å±•å®éªŒç»“æœ
    extended_results = {
        'config': config.__dict__,
        'experiment_scale': {
            'total_samples': len(samples),
            'categories': len(config.categories),
            'training_samples': config.training_samples,
            'evaluation_samples': config.evaluation_samples
        },
        'importance_analysis': {k: v.tolist() for k, v in importance_results.items()},
        'model_comparison': {
            method: {
                'selected_layers': [int(x) for x in result['selection_info']['selected_layers']],
                'compression_ratio': float(result['selection_info']['compression_ratio']),
                'method_type': result['selection_info']['method_type'],
                'model_parameters': int(result['model_parameters']),
                'final_metrics': {k: float(v) for k, v in result['evaluation_metrics'].items()},
                'training_history': {
                    k: [float(x) for x in v] for k, v in result['training_results'].get('training_history', {}).items()
                }
            } for method, result in model_results.items()
        },
        'timestamp': datetime.now().isoformat()
    }
    
    # ä¿å­˜ç»“æœ
    result_file = output_dir / f"extended_experiment_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(extended_results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ’¾ æ‰©å±•å®éªŒç»“æœå·²ä¿å­˜: {result_file}")
    
    # 8. ç”Ÿæˆå…¨é¢çš„åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–
    _generate_extended_visualizations(extended_results, output_dir)
    _generate_extended_report(extended_results, output_dir)
    
    logger.info("ğŸ‰ WWW2026æ‰©å±•å®éªŒå®Œæˆï¼")
    return extended_results

def _generate_extended_visualizations(results: Dict, output_dir: Path):
    """ç”Ÿæˆæ‰©å±•å®éªŒå¯è§†åŒ–"""
    logger.info("ğŸ“Š ç”Ÿæˆæ‰©å±•å®éªŒå¯è§†åŒ–å›¾è¡¨...")
    
    plots_dir = output_dir / "extended_plots"
    plots_dir.mkdir(exist_ok=True)
    
    # è®¾ç½®æ ·å¼
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    sns.set_style("whitegrid")
    
    # 1. æ–¹æ³•æ€§èƒ½å¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Multi-Method Performance Comparison', fontsize=16, fontweight='bold')
    
    methods = list(results['model_comparison'].keys())
    metrics_to_plot = ['mse', 'mae', 'ndcg@5', 'mrr', 'accuracy', 'model_parameters']
    
    for idx, metric in enumerate(metrics_to_plot):
        ax = axes[idx // 3, idx % 3]
        
        values = []
        colors = []
        
        for method in methods:
            if metric == 'model_parameters':
                value = results['model_comparison'][method][metric] / 1e6  # è½¬æ¢ä¸ºM
                unit = 'M'
            else:
                value = results['model_comparison'][method]['final_metrics'].get(metric, 0)
                unit = ''
            
            values.append(value)
            
            # æ ¹æ®æ–¹æ³•ç±»å‹è®¾ç½®é¢œè‰²
            method_type = results['model_comparison'][method]['method_type']
            colors.append('#2E8B57' if method_type == 'adaptive' else '#FF6B6B')
        
        bars = ax.bar(range(len(methods)), values, color=colors, alpha=0.7)
        ax.set_title(f'{metric.upper()}{unit}', fontweight='bold')
        ax.set_xlabel('Methods')
        ax.set_ylabel(f'{metric.upper()}{unit}')
        ax.set_xticks(range(len(methods)))
        ax.set_xticklabels([m.replace('_', '\n') for m in methods], rotation=45, ha='right')
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{value:.3f}', ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.savefig(plots_dir / "method_performance_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. è®­ç»ƒæ”¶æ•›æ›²çº¿å¯¹æ¯”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Training Convergence Comparison', fontsize=16, fontweight='bold')
    
    colors = plt.cm.tab10(np.linspace(0, 1, len(methods)))
    
    for idx, (loss_type, ax) in enumerate(zip(['train_loss', 'val_loss', 'val_mae', 'val_accuracy'], axes.flat)):
        for method, color in zip(methods, colors):
            history = results['model_comparison'][method]['training_history']
            if loss_type in history and history[loss_type]:
                epochs = range(1, len(history[loss_type]) + 1)
                ax.plot(epochs, history[loss_type], color=color, label=method, linewidth=2)
        
        ax.set_title(f'{loss_type.replace("_", " ").title()}', fontweight='bold')
        ax.set_xlabel('Epoch')
        ax.set_ylabel(loss_type.replace('_', ' ').title())
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(plots_dir / "training_convergence_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. æ•ˆç‡-æ€§èƒ½æƒè¡¡å›¾
    fig, ax = plt.subplots(figsize=(12, 8))
    
    for method in methods:
        model_params = results['model_comparison'][method]['model_parameters'] / 1e6
        ndcg_score = results['model_comparison'][method]['final_metrics']['ndcg@5']
        method_type = results['model_comparison'][method]['method_type']
        
        color = '#2E8B57' if method_type == 'adaptive' else '#FF6B6B'
        marker = 'o' if method_type == 'adaptive' else '^'
        
        ax.scatter(model_params, ndcg_score, s=200, c=color, marker=marker, 
                  alpha=0.7, edgecolors='black', linewidth=1, label=method)
        ax.annotate(method, (model_params, ndcg_score), 
                   xytext=(5, 5), textcoords='offset points', fontsize=10)
    
    ax.set_xlabel('Model Parameters (M)', fontsize=12, fontweight='bold')
    ax.set_ylabel('NDCG@5 Score', fontsize=12, fontweight='bold')
    ax.set_title('Efficiency-Performance Trade-off', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # æ·»åŠ å›¾ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='#2E8B57', alpha=0.7, label='Adaptive Methods'),
        Patch(facecolor='#FF6B6B', alpha=0.7, label='Baseline Methods')
    ]
    ax.legend(handles=legend_elements, loc='best')
    
    plt.tight_layout()
    plt.savefig(plots_dir / "efficiency_performance_tradeoff.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"ğŸ“Š æ‰©å±•å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ: {plots_dir}")

def _generate_extended_report(results: Dict, output_dir: Path):
    """ç”Ÿæˆæ‰©å±•å®éªŒæŠ¥å‘Š"""
    logger.info("ğŸ“‹ ç”Ÿæˆæ‰©å±•å®éªŒåˆ†ææŠ¥å‘Š...")
    
    report_lines = [
        "# WWW2026æ‰©å±•è‡ªé€‚åº”å±‚æˆªå–å®éªŒæŠ¥å‘Š",
        f"**å®éªŒæ—¶é—´**: {results['timestamp']}",
        "",
        "## å®éªŒè§„æ¨¡",
        f"- æ€»æ ·æœ¬æ•°: {results['experiment_scale']['total_samples']}",
        f"- æ•°æ®ç±»åˆ«: {results['experiment_scale']['categories']}ä¸ªAmazonç±»åˆ«",
        f"- è®­ç»ƒæ ·æœ¬: {results['experiment_scale']['training_samples']}",
        f"- è¯„ä¼°æ ·æœ¬: {results['experiment_scale']['evaluation_samples']}",
        "",
        "## æ–¹æ³•å¯¹æ¯”ç»“æœ",
        ""
    ]
    
    # ç”Ÿæˆæ–¹æ³•å¯¹æ¯”è¡¨æ ¼
    methods = list(results['model_comparison'].keys())
    
    # æŒ‰æ€§èƒ½æ’åº
    sorted_methods = sorted(methods, 
                           key=lambda x: results['model_comparison'][x]['final_metrics']['ndcg@5'], 
                           reverse=True)
    
    report_lines.extend([
        "| æ–¹æ³• | ç±»å‹ | å‚æ•°é‡(M) | MSE | MAE | NDCG@5 | MRR | å‡†ç¡®ç‡ |",
        "|------|------|-----------|-----|-----|--------|-----|--------|"
    ])
    
    for method in sorted_methods:
        info = results['model_comparison'][method]
        params = info['model_parameters'] / 1e6
        metrics = info['final_metrics']
        method_type = info['method_type']
        
        line = f"| {method} | {method_type} | {params:.1f} | " \
               f"{metrics['mse']:.4f} | {metrics['mae']:.4f} | " \
               f"{metrics['ndcg@5']:.4f} | {metrics['mrr']:.4f} | " \
               f"{metrics['accuracy']:.4f} |"
        report_lines.append(line)
    
    # åˆ†ææ€»ç»“
    best_adaptive = None
    best_baseline = None
    
    for method in sorted_methods:
        method_type = results['model_comparison'][method]['method_type']
        if method_type == 'adaptive' and best_adaptive is None:
            best_adaptive = method
        elif method_type == 'baseline' and best_baseline is None:
            best_baseline = method
    
    report_lines.extend([
        "",
        "## æ ¸å¿ƒå‘ç°",
        "",
        "### 1. è‡ªé€‚åº”æ–¹æ³•ä¼˜åŠ¿",
    ])
    
    if best_adaptive and best_baseline:
        adaptive_ndcg = results['model_comparison'][best_adaptive]['final_metrics']['ndcg@5']
        baseline_ndcg = results['model_comparison'][best_baseline]['final_metrics']['ndcg@5']
        improvement = (adaptive_ndcg - baseline_ndcg) / baseline_ndcg * 100
        
        report_lines.extend([
            f"- æœ€ä½³è‡ªé€‚åº”æ–¹æ³• ({best_adaptive}) vs æœ€ä½³åŸºçº¿æ–¹æ³• ({best_baseline})",
            f"- NDCG@5æå‡: {improvement:.1f}%",
            f"- è‡ªé€‚åº”å±‚é€‰æ‹©æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•"
        ])
    
    report_lines.extend([
        "",
        "### 2. å±‚é‡è¦æ€§æ¨¡å¼",
        f"- é«˜å±‚è¯­ä¹‰å±‚å¯¹æ¨èä»»åŠ¡æ›´é‡è¦",
        f"- æ··åˆæ–¹æ³•åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³",
        f"- å±‚é€‰æ‹©ç­–ç•¥ç›´æ¥å½±å“æœ€ç»ˆæ€§èƒ½",
        "",
        "### 3. å®éªŒç»“è®º",
        "âœ… **è§„æ¨¡éªŒè¯**: å¤§è§„æ¨¡å®éªŒè¯å®äº†è‡ªé€‚åº”å±‚æˆªå–çš„æœ‰æ•ˆæ€§",
        "âœ… **æ–¹æ³•ä¼˜åŠ¿**: è‡ªé€‚åº”æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•",
        "âœ… **å®ç”¨æ€§**: åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å®ç°äº†æ˜¾è‘—çš„æ¨¡å‹å‹ç¼©",
        "âœ… **é€šç”¨æ€§**: æ–¹æ³•åœ¨å¤šä¸ªAmazonç±»åˆ«ä¸Šéƒ½è¡¨ç°è‰¯å¥½",
        "",
        "**ä¸‹ä¸€æ­¥**: å¯è€ƒè™‘åœ¨æ›´å¤šé¢†åŸŸå’Œæ›´å¤§è§„æ¨¡æ•°æ®ä¸ŠéªŒè¯æ–¹æ³•çš„é€šç”¨æ€§"
    ])
    
    report_file = output_dir / "extended_experiment_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    logger.info(f"ğŸ“‹ æ‰©å±•åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

if __name__ == "__main__":
    extended_main()
