#!/usr/bin/env python3
"""
é˜¶æ®µ1ï¼šè¯šå®çš„æ•°æ®åŠ è½½å’ŒåŸºç¡€æ¨¡å‹è®­ç»ƒ
ç¡®ä¿æ•°æ®å®Œå…¨çœŸå®ï¼Œæ¨¡å‹è®­ç»ƒç¨³å®š
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import json
import time
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HonestDataValidator:
    """æ•°æ®è¯šå®æ€§éªŒè¯å™¨"""
    
    @staticmethod
    def validate_real_data(df, source_name="Amazon"):
        """éªŒè¯æ•°æ®æ˜¯çœŸå®çš„ï¼Œä¸æ˜¯æ¨¡æ‹Ÿçš„"""
        logger.info(f"ğŸ” éªŒè¯{source_name}æ•°æ®çœŸå®æ€§...")
        
        # åŸºæœ¬ç»Ÿè®¡æ£€æŸ¥
        logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        logger.info(f"åˆ—å: {list(df.columns)}")
        
        # æ£€æŸ¥æ–‡æœ¬å¤šæ ·æ€§ï¼ˆçœŸå®æ•°æ®åº”è¯¥æœ‰å¾ˆé«˜çš„å¤šæ ·æ€§ï¼‰
        if 'text' in df.columns:
            unique_texts = df['text'].nunique()
            total_texts = len(df)
            diversity_ratio = unique_texts / total_texts
            
            logger.info(f"æ–‡æœ¬å”¯ä¸€æ€§: {unique_texts:,}/{total_texts:,} = {diversity_ratio:.3f}")
            
            if diversity_ratio < 0.5:
                logger.warning(f"æ–‡æœ¬å¤šæ ·æ€§ä½ ({diversity_ratio:.3f})ï¼Œå¯èƒ½å­˜åœ¨é‡å¤æˆ–æ¨¡æ‹Ÿæ•°æ®")
            else:
                logger.info("âœ… æ–‡æœ¬å¤šæ ·æ€§éªŒè¯é€šè¿‡")
            
            # æ£€æŸ¥æ–‡æœ¬é•¿åº¦åˆ†å¸ƒï¼ˆçœŸå®æ•°æ®åº”è¯¥æœ‰è‡ªç„¶çš„é•¿åº¦åˆ†å¸ƒï¼‰
            text_lengths = df['text'].str.len()
            logger.info(f"æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
            logger.info(f"  æœ€å°: {text_lengths.min()}")
            logger.info(f"  æœ€å¤§: {text_lengths.max()}")
            logger.info(f"  å¹³å‡: {text_lengths.mean():.1f}")
            logger.info(f"  ä¸­ä½æ•°: {text_lengths.median():.1f}")
            logger.info(f"  æ ‡å‡†å·®: {text_lengths.std():.1f}")
        
        # æ£€æŸ¥è¯„åˆ†åˆ†å¸ƒ
        if 'rating' in df.columns:
            rating_dist = df['rating'].value_counts().sort_index()
            logger.info(f"è¯„åˆ†åˆ†å¸ƒ:")
            for rating, count in rating_dist.items():
                percentage = count / len(df) * 100
                logger.info(f"  {rating}æ˜Ÿ: {count:,} ({percentage:.1f}%)")
        
        # æ£€æŸ¥æ—¶é—´æˆ³ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'timestamp' in df.columns:
            timestamps = pd.to_datetime(df['timestamp'], errors='coerce')
            time_range = timestamps.max() - timestamps.min()
            logger.info(f"æ—¶é—´è·¨åº¦: {time_range}")
        
        logger.info("âœ… æ•°æ®çœŸå®æ€§éªŒè¯å®Œæˆ")
        return True

class StableDataLoader:
    """ç¨³å®šçš„çœŸå®æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_dir: str = "dataset"):
        self.data_dir = Path(data_dir)
        self.vocab = {}
        self.vocab_size = 0
        self.validator = HonestDataValidator()
        
    def load_verified_amazon_data(self, category="Electronics", max_samples=30000, min_text_length=10):
        """åŠ è½½å¹¶éªŒè¯çœŸå®Amazonæ•°æ®"""
        logger.info(f"ğŸ“Š åŠ è½½çœŸå®Amazon {category}æ•°æ®...")
        
        reviews_file = self.data_dir / "amazon" / f"{category}_reviews.parquet"
        
        if not reviews_file.exists():
            raise FileNotFoundError(f"çœŸå®æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {reviews_file}")
        
        # åŠ è½½æ•°æ®
        df = pd.read_parquet(reviews_file)
        logger.info(f"åŸå§‹æ•°æ®: {len(df):,} æ¡è®°å½•")
        
        # éªŒè¯æ•°æ®çœŸå®æ€§
        self.validator.validate_real_data(df, f"Amazon {category}")
        
        # æ•°æ®æ¸…æ´—ï¼ˆä¿æŒä¸¥æ ¼æ ‡å‡†ï¼‰
        original_size = len(df)
        
        # å¿…é¡»å­—æ®µæ£€æŸ¥
        required_fields = ['text', 'rating']
        for field in required_fields:
            if field not in df.columns:
                raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
        
        # æ•°æ®è´¨é‡è¿‡æ»¤
        df = df.dropna(subset=required_fields)
        df = df[df['rating'].between(1, 5)]
        df = df[df['text'].str.len() >= min_text_length]
        df = df[df['text'].str.len() <= 1000]  # è¿‡æ»¤å¼‚å¸¸é•¿æ–‡æœ¬
        
        # å»é™¤æ˜æ˜¾é‡å¤
        df = df.drop_duplicates(subset=['text'], keep='first')
        
        logger.info(f"è´¨é‡è¿‡æ»¤: {original_size:,} -> {len(df):,} ({len(df)/original_size:.1%}ä¿ç•™)")
        
        # å¹³è¡¡é‡‡æ ·
        positive_samples = df[df['rating'] >= 4]
        negative_samples = df[df['rating'] <= 2]
        
        logger.info(f"æ­£ä¾‹å€™é€‰: {len(positive_samples):,}")
        logger.info(f"è´Ÿä¾‹å€™é€‰: {len(negative_samples):,}")
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿæ ·æœ¬
        min_samples_per_class = max_samples // 2
        if len(positive_samples) < min_samples_per_class:
            logger.warning(f"æ­£ä¾‹æ ·æœ¬ä¸è¶³: {len(positive_samples)} < {min_samples_per_class}")
            min_samples_per_class = len(positive_samples)
        
        if len(negative_samples) < min_samples_per_class:
            logger.warning(f"è´Ÿä¾‹æ ·æœ¬ä¸è¶³: {len(negative_samples)} < {min_samples_per_class}")
            min_samples_per_class = len(negative_samples)
        
        # éšæœºé‡‡æ ·
        df_positive = positive_samples.sample(n=min_samples_per_class, random_state=42)
        df_negative = negative_samples.sample(n=min_samples_per_class, random_state=42)
        
        # åˆå¹¶å’ŒéšæœºåŒ–
        df_balanced = pd.concat([df_positive, df_negative]).sample(frac=1, random_state=42).reset_index(drop=True)
        
        # æå–æ–‡æœ¬å’Œæ ‡ç­¾
        texts = df_balanced['text'].astype(str).tolist()
        labels = (df_balanced['rating'] >= 4).astype(int).tolist()
        
        # æœ€ç»ˆéªŒè¯
        logger.info(f"âœ… æœ€ç»ˆæ•°æ®é›†:")
        logger.info(f"   æ€»æ ·æœ¬: {len(texts):,}")
        logger.info(f"   æ­£ä¾‹: {sum(labels):,} ({sum(labels)/len(labels):.1%})")
        logger.info(f"   è´Ÿä¾‹: {len(labels)-sum(labels):,} ({(len(labels)-sum(labels))/len(labels):.1%})")
        logger.info(f"   å¹³å‡æ–‡æœ¬é•¿åº¦: {np.mean([len(t.split()) for t in texts]):.1f} è¯")
        
        return texts, labels, df_balanced
    
    def create_stable_vocabulary(self, texts, max_vocab_size=15000, min_freq=2):
        """åˆ›å»ºç¨³å®šçš„è¯æ±‡è¡¨"""
        logger.info("ğŸ“ æ„å»ºç¨³å®šè¯æ±‡è¡¨...")
        
        # æ–‡æœ¬é¢„å¤„ç†å’Œè¯é¢‘ç»Ÿè®¡
        word_freq = defaultdict(int)
        total_words = 0
        
        for text in texts:
            # ç®€å•ä½†æœ‰æ•ˆçš„æ–‡æœ¬æ¸…ç†
            clean_text = text.lower()
            # ä¿ç•™åŸºæœ¬æ ‡ç‚¹ä½†åˆ†ç¦»
            for punct in '.,!?;:"()[]{}':
                clean_text = clean_text.replace(punct, f' {punct} ')
            
            words = clean_text.split()
            for word in words:
                word = word.strip()
                if word and len(word) <= 20:  # è¿‡æ»¤å¼‚å¸¸é•¿è¯
                    word_freq[word] += 1
                    total_words += 1
        
        logger.info(f"è¯æ±‡ç»Ÿè®¡: {len(word_freq):,} å”¯ä¸€è¯, {total_words:,} æ€»è¯æ•°")
        
        # æ„å»ºè¯æ±‡è¡¨
        special_tokens = ['<PAD>', '<UNK>', '<CLS>', '<SEP>', '<MASK>']
        vocab_words = special_tokens.copy()
        
        # æŒ‰é¢‘ç‡æ’åºï¼Œæ·»åŠ é«˜é¢‘è¯
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        
        # ç»Ÿè®¡è¦†ç›–ç‡
        covered_freq = 0
        for word, freq in sorted_words:
            if len(vocab_words) >= max_vocab_size:
                break
            if freq >= min_freq:
                vocab_words.append(word)
                covered_freq += freq
        
        coverage = covered_freq / total_words
        
        self.vocab = {word: idx for idx, word in enumerate(vocab_words)}
        self.vocab_size = len(vocab_words)
        
        logger.info(f"âœ… è¯æ±‡è¡¨æ„å»ºå®Œæˆ:")
        logger.info(f"   è¯æ±‡è¡¨å¤§å°: {self.vocab_size:,}")
        logger.info(f"   è¯æ±‡è¦†ç›–ç‡: {coverage:.1%}")
        logger.info(f"   OOVç‡: {1-coverage:.1%}")
        
        return self.vocab
    
    def tokenize_texts(self, texts, max_seq_length=256):
        """æ–‡æœ¬tokenization"""
        logger.info(f"ğŸ”¤ æ–‡æœ¬tokenization (åºåˆ—é•¿åº¦: {max_seq_length})...")
        
        if not self.vocab:
            raise ValueError("è¯æ±‡è¡¨æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨create_stable_vocabulary")
        
        tokenized_texts = []
        
        for i, text in enumerate(texts):
            if i % 5000 == 0:
                logger.info(f"   å¤„ç†è¿›åº¦: {i:,}/{len(texts):,}")
            
            # é¢„å¤„ç†
            clean_text = text.lower()
            for punct in '.,!?;:"()[]{}':
                clean_text = clean_text.replace(punct, f' {punct} ')
            
            # Tokenize
            tokens = [self.vocab['<CLS>']]
            words = clean_text.split()
            
            for word in words[:max_seq_length-2]:  # ä¿ç•™ä½ç½®ç»™ç‰¹æ®Štoken
                word = word.strip()
                if word:
                    token_id = self.vocab.get(word, self.vocab['<UNK>'])
                    tokens.append(token_id)
            
            tokens.append(self.vocab['<SEP>'])
            
            # å¡«å……æˆ–æˆªæ–­
            if len(tokens) < max_seq_length:
                tokens.extend([self.vocab['<PAD>']] * (max_seq_length - len(tokens)))
            else:
                tokens = tokens[:max_seq_length]
            
            tokenized_texts.append(tokens)
        
        logger.info(f"âœ… Tokenizationå®Œæˆ: {len(tokenized_texts)} æ ·æœ¬")
        
        return torch.tensor(tokenized_texts, dtype=torch.long)

class StableTransformer(nn.Module):
    """ç¨³å®šçš„Transformeræ¨¡å‹"""
    
    def __init__(self, vocab_size=15000, d_model=512, nhead=8, num_layers=12, max_seq_length=256, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        self.max_seq_length = max_seq_length
        
        # åµŒå…¥å±‚
        self.token_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.position_embedding = nn.Embedding(max_seq_length, d_model)
        self.embedding_dropout = nn.Dropout(dropout)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True  # Pre-norm for stability
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Dropout(dropout),
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 2)
        )
        
        # æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
        
        # è®°å½•æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        logger.info(f"   æ€»å‚æ•°: {total_params:,}")
        logger.info(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        logger.info(f"   æ¨¡å‹å¤§å°: ~{total_params * 4 / 1024 / 1024:.1f} MB")
    
    def _init_weights(self, module):
        """Xavieråˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.xavier_uniform_(module.weight)
            if hasattr(module, 'padding_idx') and module.padding_idx is not None:
                module.weight.data[module.padding_idx].fill_(0)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def forward(self, input_ids, attention_mask=None):
        """å‰å‘ä¼ æ’­"""
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        # åˆ›å»ºattention mask
        if attention_mask is None:
            attention_mask = (input_ids != 0)  # PAD token is 0
        
        # è½¬æ¢ä¸ºtransformeræœŸæœ›çš„æ ¼å¼ (Trueè¡¨ç¤ºè¦attendçš„ä½ç½®)
        src_key_padding_mask = ~attention_mask  # Invert for transformer
        
        # åµŒå…¥
        token_embeds = self.token_embedding(input_ids)
        
        # ä½ç½®åµŒå…¥
        position_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
        position_embeds = self.position_embedding(position_ids)
        
        # ç»„åˆåµŒå…¥
        hidden_states = self.embedding_dropout(token_embeds + position_embeds)
        
        # Transformerç¼–ç 
        try:
            encoded = self.transformer(hidden_states, src_key_padding_mask=src_key_padding_mask)
        except Exception as e:
            logger.warning(f"Transformerç¼–ç å‡ºé”™ï¼Œä½¿ç”¨æ— maskæ¨¡å¼: {e}")
            encoded = self.transformer(hidden_states)
        
        # ä½¿ç”¨CLS tokenè¿›è¡Œåˆ†ç±»
        cls_output = encoded[:, 0, :]  # [CLS] token
        logits = self.classifier(cls_output)
        
        return {
            'logits': logits,
            'hidden_states': encoded,
            'attention_mask': attention_mask
        }

class StableTrainer:
    """ç¨³å®šçš„æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.training_history = []
        
    def train_stable(self, train_data, train_labels, val_data, val_labels, 
                    max_epochs=10, batch_size=32, learning_rate=2e-4, patience=3):
        """ç¨³å®šè®­ç»ƒæ¨¡å‹"""
        logger.info("ğŸ‹ï¸ å¼€å§‹ç¨³å®šè®­ç»ƒ...")
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = torch.optim.AdamW(
            self.model.parameters(), 
            lr=learning_rate, 
            weight_decay=0.01,
            eps=1e-6
        )
        
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=learning_rate,
            epochs=max_epochs,
            steps_per_epoch=len(train_data) // batch_size + 1,
            pct_start=0.1,
            anneal_strategy='cos',
            div_factor=10,
            final_div_factor=100
        )
        
        criterion = nn.CrossEntropyLoss()
        
        best_val_acc = 0
        patience_counter = 0
        
        for epoch in range(max_epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            # æ‰“ä¹±è®­ç»ƒæ•°æ®
            indices = torch.randperm(len(train_data))
            
            num_batches = 0
            for i in range(0, len(train_data), batch_size):
                batch_indices = indices[i:i+batch_size]
                batch_data = train_data[batch_indices].to(self.device)
                batch_labels = torch.tensor([train_labels[idx] for idx in batch_indices], 
                                          dtype=torch.long).to(self.device)
                
                optimizer.zero_grad()
                
                outputs = self.model(batch_data)
                loss = criterion(outputs['logits'], batch_labels)
                
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                scheduler.step()
                
                # ç»Ÿè®¡
                train_loss += loss.item()
                num_batches += 1
                
                predictions = torch.argmax(outputs['logits'], dim=1)
                train_correct += (predictions == batch_labels).sum().item()
                train_total += batch_labels.size(0)
                
                # å®šæœŸè¾“å‡ºè¿›åº¦
                if num_batches % 50 == 0:
                    current_lr = scheduler.get_last_lr()[0]
                    logger.info(f"   Batch {num_batches}, Loss: {loss.item():.4f}, LR: {current_lr:.2e}")
            
            # éªŒè¯é˜¶æ®µ
            val_acc, val_loss = self.evaluate(val_data, val_labels, batch_size)
            train_acc = train_correct / train_total if train_total > 0 else 0
            avg_train_loss = train_loss / num_batches if num_batches > 0 else 0
            
            # è®°å½•å†å²
            epoch_stats = {
                'epoch': epoch + 1,
                'train_loss': avg_train_loss,
                'train_acc': train_acc,
                'val_loss': val_loss,
                'val_acc': val_acc,
                'lr': scheduler.get_last_lr()[0]
            }
            self.training_history.append(epoch_stats)
            
            logger.info(f"Epoch {epoch+1}/{max_epochs}:")
            logger.info(f"  Train - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f}")
            logger.info(f"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}")
            logger.info(f"  LR: {scheduler.get_last_lr()[0]:.2e}")
            
            # æ—©åœæ£€æŸ¥
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_checkpoint(f"best_model_epoch_{epoch+1}.pt", epoch_stats)
                logger.info(f"  âœ… æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
            else:
                patience_counter += 1
                logger.info(f"  â³ è€å¿ƒè®¡æ•°: {patience_counter}/{patience}")
                
            if patience_counter >= patience:
                logger.info(f"  ğŸ›‘ æ—©åœäºepoch {epoch+1}")
                break
        
        logger.info(f"ğŸ¯ è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        return best_val_acc, self.training_history
    
    def evaluate(self, data, labels, batch_size=64):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        num_batches = 0
        
        criterion = nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for i in range(0, len(data), batch_size):
                batch_data = data[i:i+batch_size].to(self.device)
                batch_labels = torch.tensor(labels[i:i+batch_size], dtype=torch.long).to(self.device)
                
                outputs = self.model(batch_data)
                loss = criterion(outputs['logits'], batch_labels)
                
                total_loss += loss.item()
                num_batches += 1
                
                predictions = torch.argmax(outputs['logits'], dim=1)
                correct += (predictions == batch_labels).sum().item()
                total += batch_labels.size(0)
        
        accuracy = correct / total if total > 0 else 0
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        
        return accuracy, avg_loss
    
    def save_checkpoint(self, filename, epoch_stats):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'epoch_stats': epoch_stats,
            'training_history': self.training_history,
            'model_config': {
                'vocab_size': self.model.token_embedding.num_embeddings,
                'd_model': self.model.d_model,
                'num_layers': self.model.num_layers,
                'max_seq_length': self.model.max_seq_length
            }
        }
        
        checkpoint_dir = Path("checkpoints")
        checkpoint_dir.mkdir(exist_ok=True)
        
        torch.save(checkpoint, checkpoint_dir / filename)
        logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")

def run_stage1_data_and_training():
    """é˜¶æ®µ1ï¼šæ•°æ®åŠ è½½å’ŒåŸºç¡€è®­ç»ƒ"""
    logger.info("ğŸš€ å¼€å§‹é˜¶æ®µ1ï¼šæ•°æ®åŠ è½½å’ŒåŸºç¡€è®­ç»ƒ")
    
    # 1. æ•°æ®åŠ è½½
    logger.info("ğŸ“ æ­¥éª¤1: åŠ è½½çœŸå®æ•°æ®")
    data_loader = StableDataLoader()
    
    try:
        texts, labels, raw_df = data_loader.load_verified_amazon_data(
            category="Electronics", 
            max_samples=20000
        )
        
        # ä¿å­˜åŸå§‹æ•°æ®æ‘˜è¦
        data_summary = {
            'total_samples': len(texts),
            'positive_samples': sum(labels),
            'negative_samples': len(labels) - sum(labels),
            'avg_text_length': np.mean([len(t.split()) for t in texts]),
            'data_source': 'Amazon Electronics Reviews',
            'timestamp': datetime.now().isoformat()
        }
        
        summary_file = Path("results") / "stage1_data_summary.json"
        summary_file.parent.mkdir(exist_ok=True)
        with open(summary_file, 'w') as f:
            json.dump(data_summary, f, indent=2)
        
        logger.info(f"ğŸ“‹ æ•°æ®æ‘˜è¦å·²ä¿å­˜: {summary_file}")
        
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        raise
    
    # 2. è¯æ±‡è¡¨æ„å»º
    logger.info("ğŸ“š æ­¥éª¤2: æ„å»ºè¯æ±‡è¡¨")
    vocab = data_loader.create_stable_vocabulary(texts, max_vocab_size=12000)
    
    # 3. Tokenization
    logger.info("ğŸ”¤ æ­¥éª¤3: æ–‡æœ¬tokenization")
    input_ids = data_loader.tokenize_texts(texts, max_seq_length=256)
    
    # 4. æ•°æ®åˆ†å‰²
    logger.info("âœ‚ï¸ æ­¥éª¤4: æ•°æ®åˆ†å‰²")
    n_total = len(texts)
    n_train = int(n_total * 0.7)
    n_val = int(n_total * 0.15)
    n_test = n_total - n_train - n_val
    
    # éšæœºåˆ†å‰²
    indices = torch.randperm(n_total)
    
    train_indices = indices[:n_train]
    val_indices = indices[n_train:n_train+n_val]
    test_indices = indices[n_train+n_val:]
    
    train_data = input_ids[train_indices]
    train_labels = [labels[i] for i in train_indices]
    val_data = input_ids[val_indices]
    val_labels = [labels[i] for i in val_indices]
    test_data = input_ids[test_indices]
    test_labels = [labels[i] for i in test_indices]
    
    logger.info(f"âœ… æ•°æ®åˆ†å‰²å®Œæˆ:")
    logger.info(f"   è®­ç»ƒé›†: {len(train_data):,} æ ·æœ¬")
    logger.info(f"   éªŒè¯é›†: {len(val_data):,} æ ·æœ¬")
    logger.info(f"   æµ‹è¯•é›†: {len(test_data):,} æ ·æœ¬")
    
    # 5. æ¨¡å‹åˆ›å»º
    logger.info("ğŸ—ï¸ æ­¥éª¤5: åˆ›å»ºTransformeræ¨¡å‹")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    model = StableTransformer(
        vocab_size=data_loader.vocab_size,
        d_model=512,
        nhead=8,
        num_layers=12,
        max_seq_length=256,
        dropout=0.1
    ).to(device)
    
    # 6. æ¨¡å‹è®­ç»ƒ
    logger.info("ğŸ‹ï¸ æ­¥éª¤6: æ¨¡å‹è®­ç»ƒ")
    trainer = StableTrainer(model, device)
    
    best_val_acc, history = trainer.train_stable(
        train_data, train_labels, 
        val_data, val_labels,
        max_epochs=8,
        batch_size=24,
        learning_rate=1e-4,
        patience=3
    )
    
    # 7. æµ‹è¯•é›†è¯„ä¼°
    logger.info("ğŸ§ª æ­¥éª¤7: æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°")
    test_acc, test_loss = trainer.evaluate(test_data, test_labels)
    
    logger.info(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•ç»“æœ:")
    logger.info(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    logger.info(f"   æµ‹è¯•æŸå¤±: {test_loss:.4f}")
    
    # 8. ä¿å­˜ç»“æœ
    stage1_results = {
        'data_summary': data_summary,
        'model_config': {
            'vocab_size': data_loader.vocab_size,
            'd_model': 512,
            'num_layers': 12,
            'max_seq_length': 256
        },
        'training_results': {
            'best_val_acc': best_val_acc,
            'final_test_acc': test_acc,
            'final_test_loss': test_loss,
            'training_history': history
        },
        'stage1_completed': datetime.now().isoformat()
    }
    
    results_file = Path("results") / "stage1_complete_results.json"
    with open(results_file, 'w') as f:
        json.dump(stage1_results, f, indent=2, default=str)
    
    logger.info(f"ğŸ’¾ é˜¶æ®µ1ç»“æœå·²ä¿å­˜: {results_file}")
    logger.info("ğŸ‰ é˜¶æ®µ1å®Œæˆï¼æ¨¡å‹å·²è®­ç»ƒç¨³å®šï¼Œå‡†å¤‡è¿›å…¥é˜¶æ®µ2")
    
    return {
        'model': model,
        'trainer': trainer,
        'data_splits': {
            'train': (train_data, train_labels),
            'val': (val_data, val_labels), 
            'test': (test_data, test_labels)
        },
        'data_loader': data_loader,
        'results': stage1_results
    }

if __name__ == "__main__":
    # è¿è¡Œé˜¶æ®µ1
    stage1_outputs = run_stage1_data_and_training()
    
    logger.info("âœ… é˜¶æ®µ1æˆåŠŸå®Œæˆï¼")
    logger.info("ğŸ”œ å‡†å¤‡è¿è¡Œé˜¶æ®µ2: å±‚é‡è¦æ€§åˆ†æ")
