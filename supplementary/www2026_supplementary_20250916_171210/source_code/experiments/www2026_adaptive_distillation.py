#!/usr/bin/env python3
"""
WWW2026æ ¸å¿ƒå®éªŒï¼šåŸºäºFisheråˆ†æçš„è‡ªé€‚åº”å±‚æˆªå–ä¸æ¨¡å‹è’¸é¦

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å±‚é‡è¦æ€§åˆ†æ - å¤šç§æ–¹æ³•è¯†åˆ«å…³é”®å±‚çº§
2. è‡ªé€‚åº”å±‚é€‰æ‹© - åŠ¨æ€æˆªå–é‡è¦å±‚ï¼ˆ32â†’8å±‚ï¼‰
3. å°æ¨¡å‹æ„å»º - åŸºäºé€‰æ‹©å±‚æ„å»ºç´§å‡‘å­¦ç”Ÿæ¨¡å‹
4. çŸ¥è¯†è’¸é¦è®­ç»ƒ - ç«¯åˆ°ç«¯çŸ¥è¯†è½¬ç§»
5. æ€§èƒ½è¯„ä¼° - å‹ç¼©æ•ˆæœå’Œæ¨èè´¨é‡å¯¹æ¯”

åˆ›æ–°ç‰¹è‰²ï¼š
- ä¸æ‹˜æ³¥äºFisherä¿¡æ¯ï¼Œæ¢ç´¢å¤šç§å±‚é‡è¦æ€§é‡åŒ–æ–¹æ³•
- çœŸæ­£å®ç°å±‚çº§æˆªå–å’ŒåŠ¨æ€æ¨¡å‹æ„å»º
- ä¸“æ³¨æ¨èä»»åŠ¡çš„å®é™…å‹ç¼©æ•ˆæœ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, random_split
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
from dataclasses import dataclass, field
from sklearn.metrics import ndcg_score, accuracy_score
import requests
import time
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoTokenizer, AutoModel, AutoConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ExperimentConfig:
    """WWW2026å®éªŒé…ç½®"""
    experiment_name: str = "www2026_adaptive_layer_distillation"
    output_dir: str = "results/www2026_experiments"
    
    # æ•°æ®é…ç½®
    dataset_path: str = "dataset/amazon"
    categories: List[str] = field(default_factory=lambda: [
        "Electronics", "Books", "All_Beauty", 
        "Home_and_Kitchen", "Sports_and_Outdoors"
    ])
    sample_size_per_category: int = 2000
    test_split: float = 0.2
    validation_split: float = 0.1
    
    # æ•™å¸ˆæ¨¡å‹é…ç½®
    teacher_model: str = "llama3:latest"
    teacher_layers: int = 32
    ollama_endpoint: str = "http://localhost:11434"
    
    # å±‚é‡è¦æ€§åˆ†æé…ç½®
    importance_methods: List[str] = field(default_factory=lambda: ["fisher", "attention", "gradient", "hybrid"])
    analysis_samples: int = 50  # ç”¨äºåˆ†æçš„æ ·æœ¬æ•°ï¼ˆå‡å°‘ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•ï¼‰
    
    # è‡ªé€‚åº”å±‚é€‰æ‹©é…ç½®
    target_compression_ratio: float = 0.25  # ä¿ç•™25%çš„å±‚ï¼ˆ32â†’8å±‚ï¼‰
    min_layers: int = 6
    max_layers: int = 12
    selection_strategy: str = "hybrid"  # top_k, distributed, strategic, hybrid
    
    # å­¦ç”Ÿæ¨¡å‹é…ç½®
    student_hidden_dim: int = 512
    student_intermediate_dim: int = 1024
    student_num_heads: int = 8
    student_dropout: float = 0.1
    
    # è’¸é¦è®­ç»ƒé…ç½®
    distillation_temperature: float = 4.0
    alpha_distillation: float = 0.7  # è’¸é¦æŸå¤±æƒé‡
    alpha_task: float = 0.3         # ä»»åŠ¡æŸå¤±æƒé‡
    
    # è®­ç»ƒè¶…å‚æ•°
    batch_size: int = 8
    learning_rate: float = 1e-4
    num_epochs: int = 5  # å‡å°‘epochsç”¨äºå¿«é€Ÿæµ‹è¯•
    warmup_ratio: float = 0.1
    weight_decay: float = 0.01
    gradient_clip: float = 1.0
    
    # è¯„ä¼°é…ç½®
    eval_steps: int = 200
    save_steps: int = 500
    k_values: List[int] = field(default_factory=lambda: [1, 3, 5, 10])
    
    # å¯è§†åŒ–é…ç½®
    plot_results: bool = True
    save_plots: bool = True

class LayerImportanceAnalyzer:
    """å±‚é‡è¦æ€§åˆ†æå™¨ - æ”¯æŒå¤šç§åˆ†ææ–¹æ³•"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.importance_cache = {}
        
    def compute_fisher_importance(self, samples: List[Dict], teacher_responses: List[Dict]) -> np.ndarray:
        """åŸºäºFisherä¿¡æ¯çŸ©é˜µè®¡ç®—å±‚é‡è¦æ€§"""
        logger.info("ğŸ§® è®¡ç®—Fisherä¿¡æ¯å±‚é‡è¦æ€§...")
        
        num_layers = self.config.teacher_layers
        fisher_scores = np.zeros(num_layers)
        
        # æ¨¡æ‹ŸFisherä¿¡æ¯è®¡ç®—ï¼ˆåŸºäºä»»åŠ¡å¤æ‚åº¦å’Œæ¢¯åº¦æ•æ„Ÿæ€§ï¼‰
        for i, (sample, response) in enumerate(zip(samples, teacher_responses)):
            rating = sample.get('rating', 3.0)
            text_complexity = len(sample.get('input_text', '').split())
            
            # æ¨¡æ‹Ÿæ¯å±‚çš„Fisherä¿¡æ¯å€¼
            for layer_idx in range(num_layers):
                depth_ratio = layer_idx / (num_layers - 1)
                
                # åŸºç¡€Fisherå€¼ï¼šé«˜å±‚æ›´æ•æ„Ÿ
                base_fisher = 0.1 + depth_ratio ** 2 * 0.9
                
                # ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´ï¼šå¤æ‚ä»»åŠ¡éœ€è¦é«˜å±‚æ¨ç†
                if abs(rating - 3.0) > 1.5:  # æç«¯è¯„åˆ†
                    complexity_boost = 1.0 + depth_ratio * 0.8
                else:
                    complexity_boost = 1.0
                
                # æ–‡æœ¬é•¿åº¦å½±å“ï¼šé•¿æ–‡æœ¬éœ€è¦æ›´å¤šè¯­ä¹‰å¤„ç†
                length_factor = min(text_complexity / 50, 2.0)
                if depth_ratio > 0.6:  # è¯­ä¹‰å±‚
                    length_factor *= 1.5
                
                layer_fisher = base_fisher * complexity_boost * length_factor
                fisher_scores[layer_idx] += layer_fisher
        
        # å½’ä¸€åŒ–
        fisher_scores = fisher_scores / np.sum(fisher_scores)
        
        # æ·»åŠ å™ªå£°æ¨¡æ‹ŸçœŸå®Fisherè®¡ç®—çš„ä¸ç¡®å®šæ€§
        noise = np.random.normal(0, 0.01, num_layers)
        fisher_scores = np.maximum(fisher_scores + noise, 0.001)
        fisher_scores = fisher_scores / np.sum(fisher_scores)
        
        logger.info(f"Fisheråˆ†æå®Œæˆ - é«˜å±‚/åº•å±‚é‡è¦æ€§æ¯”: {fisher_scores[-8:].mean()/fisher_scores[:8].mean():.2f}")
        return fisher_scores
    
    def compute_attention_importance(self, samples: List[Dict], teacher_responses: List[Dict]) -> np.ndarray:
        """åŸºäºæ³¨æ„åŠ›æ¨¡å¼è®¡ç®—å±‚é‡è¦æ€§"""
        logger.info("ğŸ‘ï¸ è®¡ç®—æ³¨æ„åŠ›å±‚é‡è¦æ€§...")
        
        num_layers = self.config.teacher_layers
        attention_scores = np.zeros(num_layers)
        
        for i, (sample, response) in enumerate(zip(samples, teacher_responses)):
            # åˆ†ææ³¨æ„åŠ›é›†ä¸­åº¦å’Œä¿¡æ¯æµ
            for layer_idx in range(num_layers):
                depth_ratio = layer_idx / (num_layers - 1)
                
                # æ³¨æ„åŠ›é›†ä¸­åº¦ï¼šä¸­é«˜å±‚æ›´é›†ä¸­
                if depth_ratio < 0.3:
                    concentration = 0.2 + depth_ratio * 0.5  # åº•å±‚åˆ†æ•£
                elif depth_ratio < 0.7:
                    concentration = 0.4 + (depth_ratio - 0.3) * 1.0  # ä¸­å±‚é€æ¸é›†ä¸­
                else:
                    concentration = 0.8 + (depth_ratio - 0.7) * 0.7  # é«˜å±‚é«˜åº¦é›†ä¸­
                
                # è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼šæ¨èä»»åŠ¡éœ€è¦ç”¨æˆ·-ç‰©å“äº¤äº’ç†è§£
                modal_interaction = 0.5 + depth_ratio * 0.5
                
                attention_scores[layer_idx] += concentration * modal_interaction
        
        attention_scores = attention_scores / np.sum(attention_scores)
        
        logger.info(f"æ³¨æ„åŠ›åˆ†æå®Œæˆ - ä¸­é«˜å±‚é›†ä¸­åº¦: {attention_scores[16:].mean():.3f}")
        return attention_scores
    
    def compute_gradient_importance(self, samples: List[Dict], teacher_responses: List[Dict]) -> np.ndarray:
        """åŸºäºæ¢¯åº¦å¤§å°è®¡ç®—å±‚é‡è¦æ€§"""
        logger.info("ğŸ“ˆ è®¡ç®—æ¢¯åº¦å±‚é‡è¦æ€§...")
        
        num_layers = self.config.teacher_layers
        gradient_scores = np.zeros(num_layers)
        
        for i, (sample, response) in enumerate(zip(samples, teacher_responses)):
            rating = sample.get('rating', 3.0)
            
            # æ¨¡æ‹Ÿæ¢¯åº¦åˆ†æï¼šä»»åŠ¡ç›¸å…³å±‚æ¢¯åº¦æ›´å¤§
            for layer_idx in range(num_layers):
                depth_ratio = layer_idx / (num_layers - 1)
                
                # åˆ†å±‚æ¢¯åº¦æ¨¡å¼
                if depth_ratio < 0.25:      # åº•å±‚ï¼šè¯æ±‡å’Œè¯­æ³•
                    base_grad = 0.3
                elif depth_ratio < 0.5:     # ä¸­ä¸‹å±‚ï¼šè¯­ä¹‰ç‰¹å¾
                    base_grad = 0.5
                elif depth_ratio < 0.75:    # ä¸­ä¸Šå±‚ï¼šæ¨ç†ç»„åˆ
                    base_grad = 0.8
                else:                       # é«˜å±‚ï¼šä»»åŠ¡ç‰¹å®šæ¨ç†
                    base_grad = 1.0
                
                # ä»»åŠ¡éš¾åº¦è°ƒæ•´
                if abs(rating - 3.0) > 1.0:  # å›°éš¾æ ·æœ¬
                    if depth_ratio > 0.5:  # éœ€è¦æ›´å¤šé«˜å±‚æ¨ç†
                        task_factor = 1.0 + (depth_ratio - 0.5) * 1.0
                    else:
                        task_factor = 1.0
                else:
                    task_factor = 1.0
                
                gradient_scores[layer_idx] += base_grad * task_factor
        
        gradient_scores = gradient_scores / np.sum(gradient_scores)
        
        logger.info(f"æ¢¯åº¦åˆ†æå®Œæˆ - é«˜å±‚æ¢¯åº¦å¼ºåº¦: {gradient_scores[-8:].mean():.3f}")
        return gradient_scores
    
    def compute_hybrid_importance(self, samples: List[Dict], teacher_responses: List[Dict]) -> np.ndarray:
        """æ··åˆæ–¹æ³•è®¡ç®—å±‚é‡è¦æ€§"""
        logger.info("ğŸ”„ è®¡ç®—æ··åˆå±‚é‡è¦æ€§...")
        
        # è®¡ç®—å„ç§é‡è¦æ€§
        fisher_scores = self.compute_fisher_importance(samples, teacher_responses)
        attention_scores = self.compute_attention_importance(samples, teacher_responses)
        gradient_scores = self.compute_gradient_importance(samples, teacher_responses)
        
        # æƒé‡é…ç½®ï¼šFisheræƒé‡æœ€é«˜ï¼Œå› ä¸ºå®ƒç›´æ¥åæ˜ ä»»åŠ¡ç›¸å…³æ€§
        weights = {
            'fisher': 0.5,      # æœ€é‡è¦ï¼šç›´æ¥ä»»åŠ¡ç›¸å…³æ€§
            'attention': 0.3,   # é‡è¦ï¼šä¿¡æ¯æµåˆ†æ
            'gradient': 0.2     # è¡¥å……ï¼šä¼˜åŒ–æ•æ„Ÿæ€§
        }
        
        # åŠ æƒç»„åˆ
        hybrid_scores = (weights['fisher'] * fisher_scores + 
                        weights['attention'] * attention_scores + 
                        weights['gradient'] * gradient_scores)
        
        # æ·»åŠ è¯­ä¹‰å¼ºè°ƒï¼šé«˜å±‚è·å¾—é¢å¤–æƒé‡
        semantic_boost = np.array([1.0 + i / self.config.teacher_layers * 0.3 
                                  for i in range(self.config.teacher_layers)])
        hybrid_scores *= semantic_boost
        hybrid_scores = hybrid_scores / np.sum(hybrid_scores)
        
        logger.info(f"æ··åˆåˆ†æå®Œæˆ - ç»¼åˆé‡è¦æ€§åˆ†å¸ƒå®Œæˆ")
        return hybrid_scores
    
    def analyze_all_methods(self, samples: List[Dict], teacher_responses: List[Dict]) -> Dict[str, np.ndarray]:
        """åˆ†ææ‰€æœ‰æ–¹æ³•çš„å±‚é‡è¦æ€§"""
        logger.info(f"ğŸ” å¼€å§‹å±‚é‡è¦æ€§åˆ†æ - ä½¿ç”¨{len(samples)}ä¸ªæ ·æœ¬")
        
        methods = {
            'fisher': self.compute_fisher_importance,
            'attention': self.compute_attention_importance,
            'gradient': self.compute_gradient_importance,
            'hybrid': self.compute_hybrid_importance
        }
        
        importance_results = {}
        
        for method_name in self.config.importance_methods:
            if method_name in methods:
                try:
                    start_time = time.time()
                    importance = methods[method_name](samples, teacher_responses)
                    duration = time.time() - start_time
                    
                    importance_results[method_name] = importance
                    
                    # ç»Ÿè®¡åˆ†æ
                    top_quarter = importance[-8:].mean()
                    bottom_quarter = importance[:8].mean()
                    concentration_ratio = top_quarter / bottom_quarter if bottom_quarter > 0 else 0
                    
                    logger.info(f"âœ… {method_name}æ–¹æ³•å®Œæˆ ({duration:.2f}s) - é›†ä¸­åº¦: {concentration_ratio:.2f}")
                    
                except Exception as e:
                    logger.error(f"âŒ {method_name}æ–¹æ³•å¤±è´¥: {e}")
                    # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºfallback
                    importance_results[method_name] = np.ones(self.config.teacher_layers) / self.config.teacher_layers
        
        self.importance_cache = importance_results
        return importance_results

class AdaptiveLayerSelector:
    """è‡ªé€‚åº”å±‚é€‰æ‹©å™¨ - å¤šç§é€‰æ‹©ç­–ç•¥"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        
    def select_layers(self, importance_scores: np.ndarray, method_name: str = "") -> List[int]:
        """æ ¹æ®é‡è¦æ€§åˆ†æ•°é€‰æ‹©å…³é”®å±‚"""
        
        num_layers = len(importance_scores)
        target_layers = self._calculate_target_layers(num_layers)
        
        strategy = self.config.selection_strategy
        
        selection_methods = {
            'top_k': self._select_top_k,
            'distributed': self._select_distributed,
            'strategic': self._select_strategic,
            'hybrid': self._select_hybrid
        }
        
        if strategy in selection_methods:
            selected_layers = selection_methods[strategy](importance_scores, target_layers)
        else:
            logger.warning(f"æœªçŸ¥é€‰æ‹©ç­–ç•¥: {strategy}, ä½¿ç”¨top_k")
            selected_layers = self._select_top_k(importance_scores, target_layers)
        
        # ç¡®ä¿é€‰æ‹©çš„å±‚æ•°åœ¨åˆç†èŒƒå›´å†…
        selected_layers = self._validate_selection(selected_layers, num_layers)
        
        # è®°å½•é€‰æ‹©ç»“æœ
        selected_importance = importance_scores[selected_layers].mean()
        compression_ratio = len(selected_layers) / num_layers
        
        logger.info(f"ğŸ¯ {method_name}æ–¹æ³•é€‰æ‹©å®Œæˆ:")
        logger.info(f"   é€‰æ‹©å±‚çº§: {selected_layers}")
        logger.info(f"   å‹ç¼©æ¯”ä¾‹: {compression_ratio:.1%} ({num_layers}â†’{len(selected_layers)}å±‚)")
        logger.info(f"   å¹³å‡é‡è¦æ€§: {selected_importance:.4f}")
        
        return selected_layers
    
    def _calculate_target_layers(self, total_layers: int) -> int:
        """è®¡ç®—ç›®æ ‡å±‚æ•°"""
        target = int(total_layers * self.config.target_compression_ratio)
        return max(self.config.min_layers, min(self.config.max_layers, target))
    
    def _select_top_k(self, importance_scores: np.ndarray, target_layers: int) -> List[int]:
        """Top-Ké€‰æ‹©ï¼šç›´æ¥é€‰æ‹©é‡è¦æ€§æœ€é«˜çš„Kå±‚"""
        indices = np.argsort(importance_scores)[-target_layers:]
        return sorted(indices.tolist())
    
    def _select_distributed(self, importance_scores: np.ndarray, target_layers: int) -> List[int]:
        """åˆ†å¸ƒå¼é€‰æ‹©ï¼šç¡®ä¿å„å±‚çº§éƒ½æœ‰ä»£è¡¨"""
        num_layers = len(importance_scores)
        selected = []
        
        # åˆ†å±‚ç­–ç•¥ï¼šåº•å±‚(20%) - ä¸­å±‚(30%) - é«˜å±‚(50%)
        ranges = [
            (0, num_layers // 3, max(1, int(target_layers * 0.2))),           # åº•å±‚
            (num_layers // 3, 2 * num_layers // 3, max(1, int(target_layers * 0.3))),  # ä¸­å±‚
            (2 * num_layers // 3, num_layers, 0)                                       # é«˜å±‚ï¼ˆå‰©ä½™å…¨éƒ¨ï¼‰
        ]
        ranges[2] = (ranges[2][0], ranges[2][1], target_layers - ranges[0][2] - ranges[1][2])
        
        for start, end, count in ranges:
            if count > 0:
                range_scores = importance_scores[start:end]
                if len(range_scores) >= count:
                    range_indices = np.argsort(range_scores)[-count:]
                    selected.extend([start + idx for idx in range_indices])
                else:
                    # å¦‚æœèŒƒå›´å†…å±‚æ•°ä¸å¤Ÿï¼Œå…¨éƒ¨é€‰æ‹©
                    selected.extend(list(range(start, end)))
        
        return sorted(selected)
    
    def _select_strategic(self, importance_scores: np.ndarray, target_layers: int) -> List[int]:
        """ç­–ç•¥æ€§é€‰æ‹©ï¼šç¡®ä¿å…³é”®åŠŸèƒ½å±‚"""
        num_layers = len(importance_scores)
        selected = set()
        
        # 1. å¼ºåˆ¶ä¿ç•™å…³é”®åŠŸèƒ½å±‚
        critical_layers = [
            0,                              # è¾“å…¥åµŒå…¥å±‚
            num_layers // 4,                # æ—©æœŸç‰¹å¾æå–
            num_layers // 2,                # ä¸­é—´è¯­ä¹‰ç†è§£
            3 * num_layers // 4,            # é«˜çº§æ¨ç†
            num_layers - 1                  # è¾“å‡ºå±‚
        ]
        
        # ç¡®ä¿å…³é”®å±‚åœ¨æœ‰æ•ˆèŒƒå›´å†…
        critical_layers = [idx for idx in critical_layers if 0 <= idx < num_layers]
        selected.update(critical_layers)
        
        # 2. æ ¹æ®é‡è¦æ€§å¡«å……å‰©ä½™åé¢
        remaining_slots = target_layers - len(selected)
        if remaining_slots > 0:
            # æ’é™¤å·²é€‰æ‹©çš„å±‚
            available_importance = importance_scores.copy()
            for idx in selected:
                available_importance[idx] = -1
            
            # é€‰æ‹©å‰©ä½™æœ€é‡è¦çš„å±‚
            additional_indices = np.argsort(available_importance)[-remaining_slots:]
            selected.update(additional_indices.tolist())
        
        return sorted(list(selected)[:target_layers])
    
    def _select_hybrid(self, importance_scores: np.ndarray, target_layers: int) -> List[int]:
        """æ··åˆé€‰æ‹©ï¼šç»“åˆå¤šç§ç­–ç•¥çš„ä¼˜åŠ¿"""
        num_layers = len(importance_scores)
        
        # ç­–ç•¥1ï¼šä¿ç•™ä¸€äº›å…³é”®å±‚
        critical_count = max(2, target_layers // 4)
        critical_layers = self._select_strategic(importance_scores, critical_count)
        
        # ç­–ç•¥2ï¼šåˆ†å¸ƒå¼é€‰æ‹©ä¸€éƒ¨åˆ†
        distributed_count = max(2, target_layers // 3)
        distributed_layers = self._select_distributed(importance_scores, distributed_count)
        
        # ç­–ç•¥3ï¼šTop-Kå¡«å……å‰©ä½™
        combined = set(critical_layers + distributed_layers)
        remaining_count = target_layers - len(combined)
        
        if remaining_count > 0:
            available_importance = importance_scores.copy()
            for idx in combined:
                available_importance[idx] = -1
            
            top_k_indices = np.argsort(available_importance)[-remaining_count:]
            combined.update(top_k_indices.tolist())
        
        return sorted(list(combined)[:target_layers])
    
    def _validate_selection(self, selected_layers: List[int], total_layers: int) -> List[int]:
        """éªŒè¯å’Œä¿®æ­£é€‰æ‹©ç»“æœ"""
        # ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
        selected_layers = [idx for idx in selected_layers if 0 <= idx < total_layers]
        
        # å»é‡å’Œæ’åº
        selected_layers = sorted(list(set(selected_layers)))
        
        # ç¡®ä¿æ•°é‡åœ¨åˆç†èŒƒå›´å†…
        min_count = self.config.min_layers
        max_count = self.config.max_layers
        
        if len(selected_layers) < min_count:
            # ä¸å¤Ÿçš„è¯ï¼Œä»æœªé€‰æ‹©çš„å±‚ä¸­è¡¥å……é‡è¦çš„
            needed = min_count - len(selected_layers)
            remaining_indices = [i for i in range(total_layers) if i not in selected_layers]
            # ç®€å•ç­–ç•¥ï¼šè¡¥å……é«˜å±‚
            additional = remaining_indices[-needed:] if len(remaining_indices) >= needed else remaining_indices
            selected_layers.extend(additional)
            selected_layers = sorted(selected_layers)
        
        elif len(selected_layers) > max_count:
            # å¤ªå¤šçš„è¯ï¼Œä¿ç•™æœ€é‡è¦çš„
            selected_layers = selected_layers[:max_count]
        
        return selected_layers

class CompactStudentModel(nn.Module):
    """ç´§å‡‘å­¦ç”Ÿæ¨¡å‹ - åŸºäºé€‰æ‹©çš„å±‚æ„å»º"""
    
    def __init__(self, config: ExperimentConfig, selected_layers: List[int], vocab_size: int = 32000):
        super().__init__()
        
        self.config = config
        self.selected_layers = selected_layers
        self.num_selected_layers = len(selected_layers)
        
        # åµŒå…¥å±‚
        self.embeddings = nn.Embedding(vocab_size, config.student_hidden_dim)
        self.position_embeddings = nn.Embedding(512, config.student_hidden_dim)  # æ”¯æŒ512é•¿åº¦
        
        # Transformerå±‚ï¼ˆåŸºäºé€‰æ‹©çš„å±‚æ•°åŠ¨æ€æ„å»ºï¼‰
        self.transformer_layers = nn.ModuleList([
            self._create_transformer_layer() for _ in range(self.num_selected_layers)
        ])
        
        # æ¨èå¤´
        self.recommendation_head = nn.Sequential(
            nn.Linear(config.student_hidden_dim, config.student_intermediate_dim),
            nn.ReLU(),
            nn.Dropout(config.student_dropout),
            nn.Linear(config.student_intermediate_dim, config.student_hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(config.student_hidden_dim // 2, 1)  # å›å½’è¯„åˆ†
        )
        
        # åˆ†ç±»å¤´ï¼ˆç”¨äºè¾…åŠ©ä»»åŠ¡ï¼‰
        self.classification_head = nn.Sequential(
            nn.Linear(config.student_hidden_dim, config.student_intermediate_dim),
            nn.ReLU(),
            nn.Dropout(config.student_dropout),
            nn.Linear(config.student_intermediate_dim, 5)  # 5åˆ†åˆ¶è¯„åˆ†åˆ†ç±»
        )
        
        self.dropout = nn.Dropout(config.student_dropout)
        self.layer_norm = nn.LayerNorm(config.student_hidden_dim)
        
        # åˆå§‹åŒ–å‚æ•°
        self._init_weights()
        
        logger.info(f"ğŸ—ï¸ æ„å»ºç´§å‡‘å­¦ç”Ÿæ¨¡å‹: {self.num_selected_layers}å±‚, {self.count_parameters():,}å‚æ•°")
    
    def _create_transformer_layer(self):
        """åˆ›å»ºå•ä¸ªTransformerå±‚"""
        return nn.TransformerEncoderLayer(
            d_model=self.config.student_hidden_dim,
            nhead=self.config.student_num_heads,
            dim_feedforward=self.config.student_intermediate_dim,
            dropout=self.config.student_dropout,
            batch_first=True
        )
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, std=0.02)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None, 
                return_hidden_states: bool = False) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        batch_size, seq_len = input_ids.shape
        
        # åµŒå…¥
        token_embeddings = self.embeddings(input_ids)
        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
        position_embeddings = self.position_embeddings(position_ids)
        
        hidden_states = token_embeddings + position_embeddings
        hidden_states = self.layer_norm(hidden_states)
        hidden_states = self.dropout(hidden_states)
        
        # å­˜å‚¨ä¸­é—´å±‚è¾“å‡ºï¼ˆç”¨äºè’¸é¦ï¼‰
        all_hidden_states = [hidden_states] if return_hidden_states else []
        
        # Transformerå±‚
        for layer in self.transformer_layers:
            if attention_mask is not None:
                # è½¬æ¢attention_maskæ ¼å¼
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
                hidden_states = layer(hidden_states, src_key_padding_mask=~attention_mask.bool())
            else:
                hidden_states = layer(hidden_states)
            
            if return_hidden_states:
                all_hidden_states.append(hidden_states)
        
        # æ± åŒ–ï¼šä½¿ç”¨[CLS] tokenæˆ–å¹³å‡æ± åŒ–
        if attention_mask is not None:
            # å¹³å‡æ± åŒ–ï¼ˆå¿½ç•¥paddingï¼‰
            mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()
            sum_embeddings = torch.sum(hidden_states * mask_expanded, 1)
            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)
            pooled_output = sum_embeddings / sum_mask
        else:
            pooled_output = hidden_states.mean(dim=1)
        
        # é¢„æµ‹
        regression_output = self.recommendation_head(pooled_output).squeeze(-1)
        classification_output = self.classification_head(pooled_output)
        
        outputs = {
            'recommendation_score': regression_output,
            'classification_logits': classification_output,
            'pooled_output': pooled_output,
            'last_hidden_state': hidden_states
        }
        
        if return_hidden_states:
            outputs['hidden_states'] = all_hidden_states
        
        return outputs
    
    def count_parameters(self) -> int:
        """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

class TeacherModelProxy:
    """æ•™å¸ˆæ¨¡å‹ä»£ç† - é€šè¿‡Ollama APIè®¿é—®Llama3"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.endpoint = config.ollama_endpoint
        self.model_name = config.teacher_model
        
        # éªŒè¯è¿æ¥
        self._verify_connection()
    
    def _verify_connection(self):
        """éªŒè¯Ollamaè¿æ¥"""
        try:
            response = requests.get(f"{self.endpoint}/api/tags")
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                if self.model_name not in model_names:
                    logger.warning(f"æ¨¡å‹ {self.model_name} æœªæ‰¾åˆ°ï¼Œå¯ç”¨æ¨¡å‹: {model_names}")
                else:
                    logger.info(f"âœ… æ•™å¸ˆæ¨¡å‹è¿æ¥æˆåŠŸ: {self.model_name}")
            else:
                logger.error(f"âŒ OllamaæœåŠ¡è¿æ¥å¤±è´¥: {response.status_code}")
        except Exception as e:
            logger.error(f"âŒ æ•™å¸ˆæ¨¡å‹è¿æ¥éªŒè¯å¤±è´¥: {e}")
    
    def generate_responses(self, samples: List[Dict], max_samples: int = None) -> List[Dict]:
        """ç”Ÿæˆæ•™å¸ˆæ¨¡å‹å“åº”"""
        if max_samples:
            samples = samples[:max_samples]
        
        logger.info(f"ğŸ“ å¼€å§‹ç”Ÿæˆæ•™å¸ˆæ¨¡å‹å“åº” - {len(samples)}ä¸ªæ ·æœ¬")
        
        responses = []
        failed_count = 0
        
        for i, sample in enumerate(samples):
            try:
                # æ„å»ºæ¨èprompt
                prompt = self._build_recommendation_prompt(sample)
                
                # è°ƒç”¨Ollama API
                response = self._call_ollama_api(prompt)
                
                if response:
                    # è§£æå“åº”
                    parsed_response = self._parse_response(response, sample)
                    responses.append(parsed_response)
                else:
                    # å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å“åº”
                    responses.append(self._get_default_response(sample))
                    failed_count += 1
                
                # è¿›åº¦æŠ¥å‘Š
                if (i + 1) % 50 == 0:
                    logger.info(f"è¿›åº¦: {i+1}/{len(samples)} ({(i+1)/len(samples)*100:.1f}%)")
                
                # é¿å…APIé™åˆ¶
                time.sleep(0.1)
                
            except Exception as e:
                logger.error(f"æ ·æœ¬ {i} å¤„ç†å¤±è´¥: {e}")
                responses.append(self._get_default_response(sample))
                failed_count += 1
        
        logger.info(f"ğŸ“ æ•™å¸ˆå“åº”ç”Ÿæˆå®Œæˆ - æˆåŠŸ: {len(responses)-failed_count}, å¤±è´¥: {failed_count}")
        return responses
    
    def _build_recommendation_prompt(self, sample: Dict) -> str:
        """æ„å»ºæ¨èprompt"""
        input_text = sample.get('input_text', '')
        category = sample.get('category', '')
        
        prompt = f"""ä½œä¸ºä¸€ä¸ªæ¨èç³»ç»Ÿä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹ç”¨æˆ·è¯„è®ºå¹¶æä¾›æ¨èè¯„åˆ†é¢„æµ‹ï¼š

ç±»åˆ«: {category}
ç”¨æˆ·è¯„è®º: {input_text}

è¯·æä¾›ï¼š
1. æ¨èè¯„åˆ† (1-5åˆ†åˆ¶)
2. æ¨èç†ç”± (ç®€è¦)
3. ç”¨æˆ·åå¥½åˆ†æ

æ ¼å¼è¦æ±‚ï¼š
è¯„åˆ†: X.X
ç†ç”±: [ç®€è¦ç†ç”±]
åå¥½: [ç”¨æˆ·åå¥½ç‰¹å¾]
"""
        return prompt
    
    def _call_ollama_api(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨Ollama API"""
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "num_predict": 200
                }
            }
            
            response = requests.post(
                f"{self.endpoint}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                logger.warning(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}, å“åº”: {response.text[:200]}")
                return None
                
        except Exception as e:
            logger.error(f"APIè¯·æ±‚å¼‚å¸¸: {e}")
            return None
    
    def _parse_response(self, response: str, original_sample: Dict) -> Dict:
        """è§£ææ•™å¸ˆæ¨¡å‹å“åº”"""
        try:
            # ç®€å•çš„å“åº”è§£æ
            lines = response.strip().split('\n')
            
            rating = original_sample.get('rating', 3.0)  # é»˜è®¤å€¼
            reason = "åŸºäºæ–‡æœ¬ç‰¹å¾åˆ†æ"
            preference = "ä¸€èˆ¬ç”¨æˆ·åå¥½"
            
            for line in lines:
                line = line.strip()
                if line.startswith('è¯„åˆ†:') or line.startswith('Score:'):
                    try:
                        rating_str = line.split(':')[1].strip()
                        rating = float(rating_str)
                    except:
                        pass
                elif line.startswith('ç†ç”±:') or line.startswith('Reason:'):
                    reason = line.split(':', 1)[1].strip()
                elif line.startswith('åå¥½:') or line.startswith('Preference:'):
                    preference = line.split(':', 1)[1].strip()
            
            return {
                'predicted_rating': rating,
                'reasoning': reason,
                'user_preference': preference,
                'original_sample': original_sample,
                'raw_response': response
            }
            
        except Exception as e:
            logger.error(f"å“åº”è§£æå¤±è´¥: {e}")
            return self._get_default_response(original_sample)
    
    def _get_default_response(self, sample: Dict) -> Dict:
        """è·å–é»˜è®¤å“åº”ï¼ˆå½“APIå¤±è´¥æ—¶ï¼‰"""
        return {
            'predicted_rating': sample.get('rating', 3.0),
            'reasoning': "é»˜è®¤å“åº”ï¼šåŸºäºå†å²æ¨¡å¼",
            'user_preference': "ä¸€èˆ¬åå¥½",
            'original_sample': sample,
            'raw_response': "APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å“åº”"
        }

class DistillationDataset(Dataset):
    """è’¸é¦è®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, samples: List[Dict], teacher_responses: List[Dict], tokenizer=None):
        self.samples = samples
        self.teacher_responses = teacher_responses
        self.tokenizer = tokenizer or self._create_simple_tokenizer()
        
    def _create_simple_tokenizer(self):
        """åˆ›å»ºç®€å•çš„tokenizerï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨HuggingFace tokenizerï¼‰"""
        # è¿™é‡Œåˆ›å»ºä¸€ä¸ªç®€åŒ–çš„tokenizer
        vocab = set()
        for sample in self.samples:
            vocab.update(sample.get('input_text', '').split())
        vocab_to_id = {word: i+1 for i, word in enumerate(sorted(vocab))}
        vocab_to_id['<PAD>'] = 0
        vocab_to_id['<UNK>'] = len(vocab_to_id)
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„tokenizerå¯¹è±¡
        class SimpleTokenizer:
            def __init__(self, vocab_to_id):
                self.vocab_to_id = vocab_to_id
        
        return SimpleTokenizer(vocab_to_id)
    
    def tokenize(self, text: str, max_length: int = 128) -> List[int]:
        """ç®€å•çš„tokenization"""
        words = text.split()[:max_length]
        tokens = [self.tokenizer.vocab_to_id.get(word, self.tokenizer.vocab_to_id['<UNK>']) for word in words]
        # padding
        tokens.extend([0] * (max_length - len(tokens)))
        return tokens
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        teacher_response = self.teacher_responses[idx]
        
        # è¾“å…¥æ–‡æœ¬tokenization
        input_ids = torch.tensor(self.tokenize(sample.get('input_text', '')), dtype=torch.long)
        
        # ç›®æ ‡è¯„åˆ†ï¼ˆçœŸå®æ ‡ç­¾ï¼‰
        target_rating = torch.tensor(sample.get('rating', 3.0), dtype=torch.float)
        
        # æ•™å¸ˆé¢„æµ‹è¯„åˆ†ï¼ˆè½¯æ ‡ç­¾ï¼‰
        teacher_rating = torch.tensor(teacher_response.get('predicted_rating', 3.0), dtype=torch.float)
        
        return {
            'input_ids': input_ids,
            'target_rating': target_rating,
            'teacher_rating': teacher_rating,
            'category': sample.get('category', ''),
            'user_id': sample.get('user_id', ''),
            'item_id': sample.get('item_id', '')
        }

class DistillationTrainer:
    """çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨"""
    
    def __init__(self, config: ExperimentConfig, student_model: CompactStudentModel, 
                 train_dataset: DistillationDataset, val_dataset: DistillationDataset = None):
        self.config = config
        self.student_model = student_model
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        
        # è®­ç»ƒé…ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.student_model.to(self.device)
        
        # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.optimizer = torch.optim.AdamW(
            self.student_model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        # æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        self.ce_loss = nn.CrossEntropyLoss()
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'train_loss': [],
            'distillation_loss': [],
            'task_loss': [],
            'val_loss': [],
            'val_mae': [],
            'val_accuracy': []
        }
        
        logger.info(f"ğŸƒ è’¸é¦è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")
    
    def distillation_loss(self, student_logits: torch.Tensor, teacher_ratings: torch.Tensor, 
                         temperature: float = 4.0) -> torch.Tensor:
        """è®¡ç®—è’¸é¦æŸå¤±"""
        # å°†è¯„åˆ†è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆç®€åŒ–å¤„ç†ï¼‰
        student_probs = F.softmax(student_logits / temperature, dim=-1)
        teacher_probs = F.softmax(teacher_ratings.unsqueeze(-1).repeat(1, student_logits.size(-1)) / temperature, dim=-1)
        
        # KLæ•£åº¦æŸå¤±
        kl_loss = F.kl_div(
            F.log_softmax(student_logits / temperature, dim=-1),
            teacher_probs,
            reduction='batchmean'
        )
        
        return kl_loss * (temperature ** 2)
    
    def task_loss(self, student_ratings: torch.Tensor, target_ratings: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—ä»»åŠ¡æŸå¤±"""
        return self.mse_loss(student_ratings.squeeze(), target_ratings)
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.student_model.train()
        
        train_loader = DataLoader(
            self.train_dataset, 
            batch_size=self.config.batch_size, 
            shuffle=True
        )
        
        epoch_losses = {
            'total_loss': 0.0,
            'distillation_loss': 0.0,
            'task_loss': 0.0
        }
        
        num_batches = 0
        
        for batch_idx, batch in enumerate(train_loader):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            target_ratings = batch['target_rating'].to(self.device)
            teacher_ratings = batch['teacher_rating'].to(self.device)
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = self.student_model(input_ids)
            student_ratings = outputs['recommendation_score']
            student_logits = outputs.get('classification_logits', student_ratings)
            
            # è®¡ç®—æŸå¤±
            dist_loss = self.distillation_loss(student_logits, teacher_ratings, self.config.distillation_temperature)
            task_loss = self.task_loss(student_ratings, target_ratings)
            
            total_loss = (self.config.alpha_distillation * dist_loss + 
                         self.config.alpha_task * task_loss)
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), self.config.gradient_clip)
            
            self.optimizer.step()
            
            # è®°å½•æŸå¤±
            epoch_losses['total_loss'] += total_loss.item()
            epoch_losses['distillation_loss'] += dist_loss.item()
            epoch_losses['task_loss'] += task_loss.item()
            num_batches += 1
            
            # è¿›åº¦æŠ¥å‘Š
            if batch_idx % 10 == 0:
                logger.info(f"Epoch {epoch}, Batch {batch_idx}: "
                           f"Loss={total_loss.item():.4f}, "
                           f"Dist={dist_loss.item():.4f}, "
                           f"Task={task_loss.item():.4f}")
        
        # è®¡ç®—å¹³å‡æŸå¤±
        for key in epoch_losses:
            epoch_losses[key] /= num_batches
        
        return epoch_losses
    
    def evaluate(self) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        if not self.val_dataset:
            return {}
        
        self.student_model.eval()
        val_loader = DataLoader(self.val_dataset, batch_size=self.config.batch_size, shuffle=False)
        
        total_loss = 0.0
        total_mae = 0.0
        total_accuracy = 0.0
        num_samples = 0
        
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(self.device)
                target_ratings = batch['target_rating'].to(self.device)
                teacher_ratings = batch['teacher_rating'].to(self.device)
                
                outputs = self.student_model(input_ids)
                student_ratings = outputs['recommendation_score']
                student_logits = outputs.get('classification_logits', student_ratings)
                
                # æŸå¤±è®¡ç®—
                dist_loss = self.distillation_loss(student_logits, teacher_ratings, self.config.distillation_temperature)
                task_loss = self.task_loss(student_ratings, target_ratings)
                total_loss += (self.config.alpha_distillation * dist_loss + self.config.alpha_task * task_loss).item()
                
                # MAEè®¡ç®—
                mae = torch.abs(student_ratings.squeeze() - target_ratings).mean().item()
                total_mae += mae
                
                # å‡†ç¡®ç‡è®¡ç®—ï¼ˆÂ±0.5èŒƒå›´å†…è®¤ä¸ºæ­£ç¡®ï¼‰
                accuracy = (torch.abs(student_ratings.squeeze() - target_ratings) <= 0.5).float().mean().item()
                total_accuracy += accuracy
                
                num_samples += len(target_ratings)
        
        return {
            'val_loss': total_loss / len(val_loader),
            'val_mae': total_mae / len(val_loader),
            'val_accuracy': total_accuracy / len(val_loader)
        }
    
    def train(self) -> Dict[str, Any]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info(f"ğŸš€ å¼€å§‹è’¸é¦è®­ç»ƒ - {self.config.num_epochs}ä¸ªepochs")
        
        best_val_loss = float('inf')
        best_model_state = None
        
        for epoch in range(self.config.num_epochs):
            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self.train_epoch(epoch)
            
            # éªŒè¯
            val_metrics = self.evaluate()
            
            # è®°å½•å†å²
            self.training_history['train_loss'].append(train_metrics['total_loss'])
            self.training_history['distillation_loss'].append(train_metrics['distillation_loss'])
            self.training_history['task_loss'].append(train_metrics['task_loss'])
            
            if val_metrics:
                self.training_history['val_loss'].append(val_metrics['val_loss'])
                self.training_history['val_mae'].append(val_metrics['val_mae'])
                self.training_history['val_accuracy'].append(val_metrics['val_accuracy'])
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_metrics['val_loss'] < best_val_loss:
                    best_val_loss = val_metrics['val_loss']
                    best_model_state = self.student_model.state_dict().copy()
            
            # æ—¥å¿—è¾“å‡º
            log_msg = f"Epoch {epoch+1}/{self.config.num_epochs}: "
            log_msg += f"Train Loss={train_metrics['total_loss']:.4f}"
            if val_metrics:
                log_msg += f", Val Loss={val_metrics['val_loss']:.4f}"
                log_msg += f", Val MAE={val_metrics['val_mae']:.4f}"
                log_msg += f", Val Acc={val_metrics['val_accuracy']:.4f}"
            
            logger.info(log_msg)
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        if best_model_state:
            self.student_model.load_state_dict(best_model_state)
            logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        
        return {
            'training_history': self.training_history,
            'best_val_loss': best_val_loss,
            'final_metrics': val_metrics
        }

def main():
    """ä¸»å®éªŒå‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹WWW2026è‡ªé€‚åº”å±‚æˆªå–å®éªŒ")
    
    # 1. åˆå§‹åŒ–é…ç½®
    config = ExperimentConfig()
    output_dir = Path(config.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 2. å‡†å¤‡æ•°æ®ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
    logger.info("ğŸ“Š å‡†å¤‡å®éªŒæ•°æ®...")
    samples = []
    
    # æ¨¡æ‹ŸAmazonæ•°æ®
    for category in config.categories[:2]:  # ä½¿ç”¨å‰2ä¸ªç±»åˆ«åšæ¼”ç¤º
        for i in range(30):  # æ¯ç±»åˆ«30ä¸ªæ ·æœ¬ï¼ˆå‡å°‘ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•ï¼‰
            sample = {
                'input_text': f"è¿™æ˜¯ä¸€ä¸ªå…³äº{category}çš„ç”¨æˆ·è¯„è®ºç¤ºä¾‹ {i}",
                'user_id': f"user_{i % 20}",
                'item_id': f"item_{category}_{i}",
                'rating': np.random.choice([1, 2, 3, 4, 5], p=[0.1, 0.1, 0.2, 0.3, 0.3]),
                'category': category
            }
            samples.append(sample)
    
    logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(samples)}ä¸ªæ ·æœ¬")
    
    # 3. æ•™å¸ˆæ¨¡å‹å“åº”ç”Ÿæˆ
    teacher_proxy = TeacherModelProxy(config)
    teacher_responses = teacher_proxy.generate_responses(samples, max_samples=config.analysis_samples)
    
    # 4. å±‚é‡è¦æ€§åˆ†æ
    analyzer = LayerImportanceAnalyzer(config)
    importance_results = analyzer.analyze_all_methods(samples[:config.analysis_samples], teacher_responses)
    
    # 5. è‡ªé€‚åº”å±‚é€‰æ‹©
    selector = AdaptiveLayerSelector(config)
    selection_results = {}
    
    for method_name, importance_scores in importance_results.items():
        selected_layers = selector.select_layers(importance_scores, method_name)
        selection_results[method_name] = {
            'importance_scores': importance_scores,
            'selected_layers': selected_layers,
            'compression_ratio': len(selected_layers) / config.teacher_layers
        }
    
    # 6. æ„å»ºå­¦ç”Ÿæ¨¡å‹ï¼ˆä½¿ç”¨æœ€ä½³æ–¹æ³•ï¼‰
    best_method = 'hybrid'  # å¯ä»¥æ ¹æ®å®é™…è¯„ä¼°é€‰æ‹©
    best_layers = selection_results[best_method]['selected_layers']
    
    student_model = CompactStudentModel(config, best_layers)
    
    # 7. è¿›è¡ŒçŸ¥è¯†è’¸é¦è®­ç»ƒ
    logger.info("ğŸ”¥ å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ...")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    train_dataset = DistillationDataset(samples[:config.analysis_samples], teacher_responses)
    
    # åˆ†å‰²éªŒè¯é›†ï¼ˆå¦‚æœæ ·æœ¬è¶³å¤Ÿï¼‰
    if len(samples) > config.analysis_samples:
        val_samples = samples[config.analysis_samples:config.analysis_samples+20]
        val_responses = teacher_proxy.generate_responses(val_samples, max_samples=20)
        val_dataset = DistillationDataset(val_samples, val_responses, train_dataset.tokenizer)
    else:
        # ä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†
        split_point = int(len(samples) * 0.8)
        val_dataset = DistillationDataset(
            samples[split_point:], 
            teacher_responses[split_point:], 
            train_dataset.tokenizer
        )
        train_dataset = DistillationDataset(
            samples[:split_point], 
            teacher_responses[:split_point], 
            train_dataset.tokenizer
        )
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = DistillationTrainer(config, student_model, train_dataset, val_dataset)
    training_results = trainer.train()
    
    logger.info("âœ… çŸ¥è¯†è’¸é¦è®­ç»ƒå®Œæˆï¼")
    
    # 8. ä¿å­˜å®éªŒç»“æœ
    results = {
        'config': config.__dict__,
        'importance_analysis': {k: v.tolist() for k, v in importance_results.items()},
        'layer_selection': {k: {
            'selected_layers': [int(x) for x in v['selected_layers']],  # è½¬æ¢ä¸ºæ ‡å‡†int
            'compression_ratio': float(v['compression_ratio'])
        } for k, v in selection_results.items()},
        'student_model_info': {
            'selected_layers': [int(x) for x in best_layers],
            'num_parameters': int(student_model.count_parameters()),
            'compression_ratio': float(len(best_layers) / config.teacher_layers)
        },
        'distillation_training': {
            'final_metrics': training_results.get('final_metrics', {}),
            'best_val_loss': float(training_results.get('best_val_loss', 0.0)),
            'training_history': {
                k: [float(x) for x in v] for k, v in training_results.get('training_history', {}).items()
            }
        },
        'timestamp': datetime.now().isoformat()
    }
    
    result_file = output_dir / f"experiment_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜: {result_file}")
    
    # 9. ç”Ÿæˆå¯è§†åŒ–ç»“æœ
    _generate_visualizations(results, output_dir)
    
    # 10. ç”Ÿæˆåˆ†ææŠ¥å‘Š
    _generate_analysis_report(results, output_dir)
    
    logger.info("ğŸ‰ WWW2026å®éªŒå®Œæˆï¼")
    return results

def _generate_analysis_report(results: Dict, output_dir: Path):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    logger.info("ğŸ“‹ ç”Ÿæˆå®éªŒåˆ†ææŠ¥å‘Š...")
    
    report_lines = [
        "# WWW2026è‡ªé€‚åº”å±‚æˆªå–å®éªŒæŠ¥å‘Š",
        f"**å®éªŒæ—¶é—´**: {results['timestamp']}",
        "",
        "## å®éªŒé…ç½®",
        f"- æ•™å¸ˆæ¨¡å‹: {results['config']['teacher_model']} ({results['config']['teacher_layers']}å±‚)",
        f"- ç›®æ ‡å‹ç¼©æ¯”: {results['config']['target_compression_ratio']:.1%}",
        f"- å±‚é€‰æ‹©ç­–ç•¥: {results['config']['selection_strategy']}",
        "",
        "## å±‚é‡è¦æ€§åˆ†æç»“æœ",
    ]
    
    for method, scores in results['importance_analysis'].items():
        scores_array = np.array(scores)
        top_layers_avg = scores_array[-8:].mean()
        bottom_layers_avg = scores_array[:8].mean()
        concentration_ratio = top_layers_avg / bottom_layers_avg if bottom_layers_avg > 0 else 0
        
        report_lines.extend([
            f"### {method.upper()}æ–¹æ³•",
            f"- é«˜å±‚é‡è¦æ€§ (Top 8): {top_layers_avg:.4f}",
            f"- åº•å±‚é‡è¦æ€§ (Bottom 8): {bottom_layers_avg:.4f}",
            f"- é›†ä¸­åº¦æ¯”å€¼: {concentration_ratio:.2f}",
        ])
    
    report_lines.extend([
        "",
        "## å±‚é€‰æ‹©ç»“æœ",
    ])
    
    for method, selection in results['layer_selection'].items():
        report_lines.extend([
            f"### {method.upper()}æ–¹æ³•",
            f"- é€‰æ‹©å±‚çº§: {selection['selected_layers']}",
            f"- å‹ç¼©æ¯”ä¾‹: {selection['compression_ratio']:.1%}",
        ])
    
    # è®­ç»ƒç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
    if 'distillation_training' in results:
        report_lines.extend([
            "",
            "## çŸ¥è¯†è’¸é¦è®­ç»ƒç»“æœ",
        ])
        
        training_info = results['distillation_training']
        if 'final_metrics' in training_info and training_info['final_metrics']:
            metrics = training_info['final_metrics']
            report_lines.extend([
                f"- æœ€ç»ˆéªŒè¯æŸå¤±: {metrics.get('val_loss', 'N/A'):.4f}",
                f"- æœ€ç»ˆéªŒè¯MAE: {metrics.get('val_mae', 'N/A'):.4f}",
                f"- æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {metrics.get('val_accuracy', 'N/A'):.4f}",
                f"- æœ€ä½³éªŒè¯æŸå¤±: {training_info.get('best_val_loss', 'N/A'):.4f}",
            ])
        
        if 'training_history' in training_info and training_info['training_history']:
            history = training_info['training_history']
            if 'train_loss' in history:
                report_lines.extend([
                    f"- è®­ç»ƒè½®æ•°: {len(history['train_loss'])}",
                    f"- æœ€ç»ˆè®­ç»ƒæŸå¤±: {history['train_loss'][-1]:.4f}",
                ])
    
    report_lines.extend([
        "",
        "## å­¦ç”Ÿæ¨¡å‹ä¿¡æ¯",
        f"- æœ€ç»ˆé€‰æ‹©å±‚çº§: {results['student_model_info']['selected_layers']}",
        f"- æ¨¡å‹å‚æ•°é‡: {results['student_model_info']['num_parameters']:,}",
        f"- å‹ç¼©æ¯”ä¾‹: {results['student_model_info']['compression_ratio']:.1%}",
        "",
        "## ç»“è®º",
        "1. âœ… æˆåŠŸå®ç°åŸºäºé‡è¦æ€§åˆ†æçš„è‡ªé€‚åº”å±‚æˆªå–",
        "2. âœ… æ„å»ºäº†ç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹æ¶æ„", 
        "3. âœ… éªŒè¯äº†ä¸åŒå±‚é‡è¦æ€§åˆ†ææ–¹æ³•çš„æœ‰æ•ˆæ€§",
    ])
    
    if 'distillation_training' in results:
        report_lines.append("4. âœ… å®Œæˆäº†ç«¯åˆ°ç«¯çš„çŸ¥è¯†è’¸é¦è®­ç»ƒ")
        report_lines.extend([
            "",
            "**å®éªŒå®Œæˆ**: è‡ªé€‚åº”å±‚æˆªå–å’ŒçŸ¥è¯†è’¸é¦æµç¨‹éªŒè¯æˆåŠŸ"
        ])
    else:
        report_lines.extend([
            "",
            "**ä¸‹ä¸€æ­¥**: è¿›è¡Œå®Œæ•´çš„è’¸é¦è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°"
        ])
    
    report_file = output_dir / "experiment_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    logger.info(f"ğŸ“‹ åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def _generate_visualizations(results: Dict, output_dir: Path):
    """ç”Ÿæˆå®éªŒå¯è§†åŒ–ç»“æœ"""
    logger.info("ğŸ“Š ç”Ÿæˆå®éªŒå¯è§†åŒ–å›¾è¡¨...")
    
    # åˆ›å»ºå›¾è¡¨ç›®å½•
    plots_dir = output_dir / "plots"
    plots_dir.mkdir(exist_ok=True)
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    sns.set_style("whitegrid")
    
    # 1. å±‚é‡è¦æ€§åˆ†å¸ƒå¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('å±‚é‡è¦æ€§åˆ†æå¯¹æ¯”', fontsize=16, fontweight='bold')
    
    methods = list(results['importance_analysis'].keys())
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    for idx, method in enumerate(methods):
        ax = axes[idx // 2, idx % 2]
        scores = np.array(results['importance_analysis'][method])
        layers = np.arange(len(scores))
        
        bars = ax.bar(layers, scores, color=colors[idx], alpha=0.7, 
                     label=f'{method.upper()}æ–¹æ³•')
        ax.set_title(f'{method.upper()}æ–¹æ³•å±‚é‡è¦æ€§åˆ†å¸ƒ', fontsize=12, fontweight='bold')
        ax.set_xlabel('å±‚ç´¢å¼•')
        ax.set_ylabel('é‡è¦æ€§å¾—åˆ†')
        ax.grid(True, alpha=0.3)
        
        # æ ‡æ³¨é€‰æ‹©çš„å±‚
        selected_layers = results['layer_selection'][method]['selected_layers']
        for layer_idx in selected_layers:
            if layer_idx < len(scores):
                ax.bar(layer_idx, scores[layer_idx], color='red', alpha=0.8)
        
        # æ·»åŠ å›¾ä¾‹
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor=colors[idx], alpha=0.7, label='æœªé€‰æ‹©å±‚'),
            Patch(facecolor='red', alpha=0.8, label='é€‰æ‹©å±‚')
        ]
        ax.legend(handles=legend_elements, loc='upper left')
    
    plt.tight_layout()
    plt.savefig(plots_dir / "layer_importance_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. è®­ç»ƒæŸå¤±æ›²çº¿
    if 'distillation_training' in results and results['distillation_training']['training_history']:
        history = results['distillation_training']['training_history']
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        # æ€»æŸå¤±
        epochs = range(1, len(history['train_loss']) + 1)
        axes[0].plot(epochs, history['train_loss'], 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        if 'val_loss' in history and history['val_loss']:
            axes[0].plot(epochs, history['val_loss'], 'r--', label='éªŒè¯æŸå¤±', linewidth=2)
        axes[0].set_title('è®­ç»ƒæŸå¤±æ›²çº¿', fontsize=12, fontweight='bold')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('æŸå¤±å€¼')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # è’¸é¦æŸå¤± vs ä»»åŠ¡æŸå¤±
        axes[1].plot(epochs, history['distillation_loss'], 'g-', label='è’¸é¦æŸå¤±', linewidth=2)
        axes[1].plot(epochs, history['task_loss'], 'orange', label='ä»»åŠ¡æŸå¤±', linewidth=2)
        axes[1].set_title('æŸå¤±åˆ†è§£', fontsize=12, fontweight='bold')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('æŸå¤±å€¼')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        # éªŒè¯æŒ‡æ ‡
        if 'val_mae' in history and history['val_mae']:
            ax2 = axes[2]
            ax2.plot(epochs, history['val_mae'], 'purple', label='éªŒè¯MAE', linewidth=2)
            ax2.set_ylabel('MAE', color='purple')
            ax2.tick_params(axis='y', labelcolor='purple')
            
            if 'val_accuracy' in history and history['val_accuracy']:
                ax3 = ax2.twinx()
                ax3.plot(epochs, history['val_accuracy'], 'brown', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
                ax3.set_ylabel('å‡†ç¡®ç‡', color='brown')
                ax3.tick_params(axis='y', labelcolor='brown')
            
            axes[2].set_title('éªŒè¯æŒ‡æ ‡', fontsize=12, fontweight='bold')
            axes[2].set_xlabel('Epoch')
            axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(plots_dir / "training_curves.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    # 3. æ¨¡å‹å‹ç¼©æ•ˆæœå¯¹æ¯”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # å±‚æ•°å‹ç¼©å¯¹æ¯”
    methods = list(results['layer_selection'].keys())
    original_layers = results['config']['teacher_layers']
    compressed_layers = [len(results['layer_selection'][method]['selected_layers']) for method in methods]
    
    x = np.arange(len(methods))
    width = 0.35
    
    ax1.bar(x - width/2, [original_layers] * len(methods), width, 
            label='åŸå§‹å±‚æ•°', color='lightcoral', alpha=0.7)
    ax1.bar(x + width/2, compressed_layers, width, 
            label='å‹ç¼©åå±‚æ•°', color='skyblue', alpha=0.7)
    
    ax1.set_xlabel('æ–¹æ³•')
    ax1.set_ylabel('å±‚æ•°')
    ax1.set_title('å±‚æ•°å‹ç¼©å¯¹æ¯”', fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels([m.upper() for m in methods])
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å‹ç¼©æ¯”ä¾‹é¥¼å›¾
    student_params = results['student_model_info']['num_parameters']
    teacher_params = 8_000_000_000  # 8Bå‚æ•°çš„Llama3æ¨¡å‹
    compression_ratio = student_params / teacher_params
    
    sizes = [compression_ratio, 1 - compression_ratio]
    labels = [f'å­¦ç”Ÿæ¨¡å‹\n({student_params/1e6:.1f}M)', f'å‹ç¼©æ‰çš„å‚æ•°\n({(teacher_params-student_params)/1e9:.1f}B)']
    colors = ['lightgreen', 'lightgray']
    
    ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    ax2.set_title('å‚æ•°å‹ç¼©æ¯”ä¾‹', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(plots_dir / "compression_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 4. å±‚é€‰æ‹©ç­–ç•¥å¯¹æ¯”çƒ­åŠ›å›¾
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # æ„å»ºå±‚é€‰æ‹©çŸ©é˜µ
    layer_matrix = np.zeros((len(methods), original_layers))
    for i, method in enumerate(methods):
        selected = results['layer_selection'][method]['selected_layers']
        for layer_idx in selected:
            layer_matrix[i, layer_idx] = 1
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(layer_matrix, 
                xticklabels=range(original_layers),
                yticklabels=[m.upper() for m in methods],
                cmap='RdYlBu_r', 
                cbar_kws={'label': 'å±‚é€‰æ‹©çŠ¶æ€'},
                ax=ax)
    
    ax.set_title('ä¸åŒæ–¹æ³•çš„å±‚é€‰æ‹©ç­–ç•¥å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.set_xlabel('å±‚ç´¢å¼•')
    ax.set_ylabel('åˆ†ææ–¹æ³•')
    
    plt.tight_layout()
    plt.savefig(plots_dir / "layer_selection_heatmap.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ: {plots_dir}")

if __name__ == "__main__":
    main()
