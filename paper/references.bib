@article{zhao2023llm4rec,
  title={A Survey on Large Language Models for Recommendation},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2305.19860},
  year={2023}
}

@inproceedings{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  booktitle={NIPS Deep Learning and Representation Learning Workshop},
  year={2015}
}

@inproceedings{rogers2020primer,
  title={A primer in BERTology: What we know about how BERT works},
  author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  booktitle={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={842--866},
  year={2020}
}

@inproceedings{zagoruyko2016attention,
  title={Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2015}
}

@inproceedings{sun2019patient,
  title={Patient knowledge distillation for BERT model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  pages={4323--4332},
  year={2019}
}

@inproceedings{jiao2019tinybert,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4163--4174},
  year={2020}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  booktitle={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017}
}

@inproceedings{turner2019blockwise,
  title={Blockwise parallel decoding for deep autoregressive models},
  author={Turner, Mitchell A and Wortsman, Mitchell and Dettmers, Tim and Schmidt, Ludwig},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{theis2018faster,
  title={Faster gaze prediction with dense networks and fisher pruning},
  author={Theis, Lucas and Korshunova, Iryna and Tejani, Alykhan and Husz{\'a}r, Ferenc},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={0--0},
  year={2018}
}

@article{li2023llm4rec,
  title={How Can Recommender Systems Benefit from Large Language Models: A Survey},
  author={Li, Jianghao and Zhang, Yanru and Fan, Yunjia and Hou, Yupeng and Ren, Pengjie and Tang, Zhaochun and Zhang, Zhumin and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2306.05817},
  year={2023}
}

@inproceedings{hou2022towards,
  title={Towards universal sequence representation learning for recommender systems},
  author={Hou, Yupeng and Mu, Shanlei and Zhao, Wayne Xin and Li, Yaliang and Ding, Bolin and Wen, Ji-Rong},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={585--593},
  year={2022}
}

@inproceedings{geng2022recommendation,
  title={Recommendation as language processing (rlp): A unified pretrain, personalized prompt \& predict paradigm (p5)},
  author={Geng, Shijie and Liu, Shuchang and Fu, Zuohui and Ge, Yingqiang and Zhang, Yongfeng},
  booktitle={Proceedings of the 16th ACM Conference on Recommender Systems},
  pages={299--315},
  year={2022}
}

@article{hou2024bridging,
  title={Bridging Language and Items for Retrieval and Recommendation},
  author={Hou, Yupeng and Li, Junjie and He, Zhankui and Yan, An and Ren, Xuemeng and Tang, Ruiming and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2403.03952},
  year={2024}
}

@inproceedings{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4171--4186},
  year={2019}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{wang2020minilm,
  title={MiniLM: Deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5776--5788},
  year={2020}
}

@inproceedings{tenney2019bert,
  title={BERT rediscovers the classical NLP pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4593--4601},
  year={2019}
}

@inproceedings{passban2021alp,
  title={ALP-KD: Attention-based layer projection for knowledge distillation},
  author={Passban, Peyman and Wu, Yimeng and Rezagholizadeh, Mehdi and Liu, Qun},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={3},
  pages={2643--2651},
  year={2021}
}

@inproceedings{lee2018snip,
  title={SNIP: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{wang2020picking,
  title={Picking winning tickets before training by preserving gradient flow},
  author={Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{zhang2021neural,
  title={Neural collaborative filtering with text feature enhancement for recommendation},
  author={Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={33},
  number={6},
  pages={2582--2596},
  year={2019}
}

@inproceedings{li2023chatgpt,
  title={Is ChatGPT a good recommender? A preliminary study},
  author={Li, Junling and Zhang, Jian and Chen, Lei and Wang, Yuxiang},
  booktitle={Proceedings of the 17th ACM Conference on Recommender Systems},
  pages={392--399},
  year={2023}
}

@article{dai2023uncovering,
  title={Uncovering ChatGPT's capabilities in recommender systems},
  author={Dai, Sunhao and Shao, Ning and Zhao, Haiyuan and Yu, Weijie and Si, Zihua and Xu, Chen and Sun, Zhongxin and Zhang, Xiao and Xu, Jun},
  journal={Proceedings of the 17th ACM Conference on Recommender Systems},
  pages={1--12},
  year={2023}
}

@inproceedings{bao2023tallrec,
  title={TALLRec: An effective and efficient tuning framework to align large language model with recommendation},
  author={Bao, Keqin and Zhang, Jizhi and Zhang, Yang and Wang, Wenjie and Feng, Fuli and He, Xiangnan},
  booktitle={Proceedings of the 17th ACM Conference on Recommender Systems},
  pages={1007--1014},
  year={2023}
}

@inproceedings{harper2015movielens,
  title={The MovieLens datasets: History and context},
  author={Harper, F Maxwell and Konstan, Joseph A},
  booktitle={ACM Transactions on Interactive Intelligent Systems},
  volume={5},
  number={4},
  pages={1--19},
  year={2015}
}

@article{kornblith2019similarity,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  journal={International Conference on Machine Learning},
  pages={3519--3529},
  year={2019}
}

@inproceedings{fan2019reducing,
  title={Reducing transformer depth on demand with structured dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3645--3650},
  year={2019}
}

@inproceedings{qin2022devil,
  title={The devil is in the details: On the pitfalls of token-level knowledge distillation},
  author={Qin, Yujia and Wang, Yankai and Liu, Zhiyuan and Sun, Maosong},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={3032--3043},
  year={2022}
}
