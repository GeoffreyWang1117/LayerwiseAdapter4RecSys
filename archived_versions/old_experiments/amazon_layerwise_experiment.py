"""
Layerwise Adapter Amazonå®éªŒ

å°†Fisherä¿¡æ¯åˆ†æå’Œlayerwiseé‡è¦æ€§åˆ†æåº”ç”¨åˆ°çœŸå®Amazonæ¨èåœºæ™¯ä¸­
"""

import sys
import os
sys.path.insert(0, os.path.abspath('.'))

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging
from datetime import datetime
from dataclasses import dataclass
import matplotlib.pyplot as plt
import seaborn as sns
import json
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import csr_matrix
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

@dataclass
class LayerwiseConfig:
    """Layerwiseå®éªŒé…ç½®"""
    # æ¨¡å‹å‚æ•°
    embedding_dim: int = 64
    hidden_dims: List[int] = None
    dropout_rate: float = 0.2
    
    # è®­ç»ƒå‚æ•°
    learning_rate: float = 0.001
    batch_size: int = 512
    max_epochs: int = 100
    patience: int = 10
    
    # Fisherä¿¡æ¯å‚æ•°
    fisher_sample_size: int = 1000
    importance_threshold: float = 0.1
    
    # æ•°æ®å‚æ•°
    max_users: int = 3000
    max_items: int = 2000
    test_ratio: float = 0.2
    
    def __post_init__(self):
        if self.hidden_dims is None:
            self.hidden_dims = [128, 64, 32]

class NeuralCollaborativeFiltering(nn.Module):
    """ç¥ç»ååŒè¿‡æ»¤æ¨¡å‹"""
    
    def __init__(self, n_users: int, n_items: int, config: LayerwiseConfig):
        super().__init__()
        self.config = config
        self.n_users = n_users
        self.n_items = n_items
        
        # åµŒå…¥å±‚
        self.user_embedding = nn.Embedding(n_users, config.embedding_dim)
        self.item_embedding = nn.Embedding(n_items, config.embedding_dim)
        
        # MLPå±‚
        input_dim = config.embedding_dim * 2
        self.layers = nn.ModuleList()
        
        prev_dim = input_dim
        for hidden_dim in config.hidden_dims:
            self.layers.append(nn.Linear(prev_dim, hidden_dim))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(config.dropout_rate))
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(prev_dim, 1)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, std=0.01)
            elif isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, user_ids: torch.Tensor, item_ids: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # è·å–åµŒå…¥
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        
        # æ‹¼æ¥ç‰¹å¾
        x = torch.cat([user_emb, item_emb], dim=-1)
        
        # MLPå±‚
        for layer in self.layers:
            x = layer(x)
        
        # è¾“å‡º
        output = self.output_layer(x)
        return torch.sigmoid(output) * 4 + 1  # æ˜ å°„åˆ°[1, 5]èŒƒå›´

class LayerwiseFisherAnalyzer:
    """Layerwise Fisherä¿¡æ¯åˆ†æå™¨"""
    
    def __init__(self, model: nn.Module, config: LayerwiseConfig):
        self.model = model
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # Fisherä¿¡æ¯å­˜å‚¨
        self.layer_fisher_info = {}
        self.layer_importance_scores = {}
        
    def compute_fisher_information(self, dataloader) -> Dict[str, torch.Tensor]:
        """è®¡ç®—æ¯å±‚çš„Fisherä¿¡æ¯çŸ©é˜µ"""
        self.logger.info("è®¡ç®—Fisherä¿¡æ¯çŸ©é˜µ...")
        
        self.model.eval()
        fisher_info = {}
        
        # åˆå§‹åŒ–Fisherä¿¡æ¯çŸ©é˜µ
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                fisher_info[name] = torch.zeros_like(param)
        
        n_samples = 0
        max_samples = self.config.fisher_sample_size
        
        for batch_idx, (user_ids, item_ids, ratings) in enumerate(dataloader):
            if n_samples >= max_samples:
                break
            
            user_ids = user_ids.to(self.device)
            item_ids = item_ids.to(self.device)
            ratings = ratings.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.model.zero_grad()
            outputs = self.model(user_ids, item_ids)
            loss = F.mse_loss(outputs.squeeze(), ratings)
            
            # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
            loss.backward()
            
            # ç´¯ç§¯æ¢¯åº¦å¹³æ–¹ï¼ˆFisherä¿¡æ¯çš„å¯¹è§’è¿‘ä¼¼ï¼‰
            for name, param in self.model.named_parameters():
                if param.grad is not None:
                    fisher_info[name] += param.grad.data ** 2
            
            n_samples += user_ids.size(0)
        
        # æ ‡å‡†åŒ–Fisherä¿¡æ¯
        for name in fisher_info:
            fisher_info[name] /= n_samples
        
        self.layer_fisher_info = fisher_info
        self.logger.info(f"Fisherä¿¡æ¯è®¡ç®—å®Œæˆï¼Œæ ·æœ¬æ•°: {n_samples}")
        
        return fisher_info
    
    def analyze_layer_importance(self) -> Dict[str, float]:
        """åˆ†ææ¯å±‚çš„é‡è¦æ€§"""
        self.logger.info("åˆ†æå±‚çº§é‡è¦æ€§...")
        
        layer_importance = {}
        
        # æŒ‰å±‚çº§åˆ†ç»„å‚æ•°
        layer_groups = self._group_parameters_by_layer()
        
        for layer_name, param_names in layer_groups.items():
            total_importance = 0.0
            param_count = 0
            
            for param_name in param_names:
                if param_name in self.layer_fisher_info:
                    fisher_tensor = self.layer_fisher_info[param_name]
                    # è®¡ç®—Fisherä¿¡æ¯çš„æ€»å’Œä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
                    if len(fisher_tensor.shape) == 1:
                        fisher_sum = fisher_tensor.sum()
                    else:
                        fisher_sum = fisher_tensor.sum()
                    total_importance += fisher_sum.item()
                    param_count += 1
            
            if param_count > 0:
                layer_importance[layer_name] = total_importance / param_count
            else:
                layer_importance[layer_name] = 0.0
        
        # æ ‡å‡†åŒ–é‡è¦æ€§åˆ†æ•°
        max_importance = max(layer_importance.values()) if layer_importance else 1.0
        if max_importance > 0:
            for layer_name in layer_importance:
                layer_importance[layer_name] /= max_importance
        
        self.layer_importance_scores = layer_importance
        
        self.logger.info("å±‚çº§é‡è¦æ€§åˆ†æå®Œæˆ")
        return layer_importance
    
    def _group_parameters_by_layer(self) -> Dict[str, List[str]]:
        """æŒ‰å±‚çº§åˆ†ç»„å‚æ•°"""
        layer_groups = {
            'embedding': [],
            'mlp_layers': [],
            'output': []
        }
        
        for name, _ in self.model.named_parameters():
            if 'embedding' in name:
                layer_groups['embedding'].append(name)
            elif 'output_layer' in name:
                layer_groups['output'].append(name)
            else:
                layer_groups['mlp_layers'].append(name)
        
        return layer_groups
    
    def identify_critical_layers(self, threshold: float = None) -> List[str]:
        """è¯†åˆ«å…³é”®å±‚"""
        if threshold is None:
            threshold = self.config.importance_threshold
        
        critical_layers = []
        for layer_name, importance in self.layer_importance_scores.items():
            if importance >= threshold:
                critical_layers.append(layer_name)
        
        self.logger.info(f"è¯†åˆ«å‡º {len(critical_layers)} ä¸ªå…³é”®å±‚")
        return critical_layers

class AmazonLayerwiseExperiment:
    """Amazon Layerwiseå®éªŒä¸»ç±»"""
    
    def __init__(self, category: str = 'All_Beauty', config: Optional[LayerwiseConfig] = None):
        self.category = category
        self.config = config or LayerwiseConfig()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # æ•°æ®ç¼–ç å™¨
        self.user_encoder = LabelEncoder()
        self.item_encoder = LabelEncoder()
        
        # æ¨¡å‹å’Œåˆ†æå™¨
        self.model = None
        self.fisher_analyzer = None
        
        # å®éªŒç»“æœ
        self.results = {}
        
    def load_and_preprocess_data(self, data_path: str = "dataset/amazon") -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        self.logger.info(f"åŠ è½½Amazon {self.category} æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        reviews_file = Path(data_path) / f"{self.category}_reviews.parquet"
        df = pd.read_parquet(reviews_file)
        
        # é¢„å¤„ç†
        df = df.rename(columns={'parent_asin': 'item_id'})
        df = df.dropna(subset=['user_id', 'item_id', 'rating'])
        df = df[(df['rating'] >= 1) & (df['rating'] <= 5)]
        
        # è¿‡æ»¤ä½é¢‘äº¤äº’
        user_counts = df['user_id'].value_counts()
        item_counts = df['item_id'].value_counts()
        
        valid_users = user_counts[user_counts >= 5].index
        valid_items = item_counts[item_counts >= 5].index
        
        df = df[df['user_id'].isin(valid_users) & df['item_id'].isin(valid_items)]
        
        # é™åˆ¶æ•°æ®è§„æ¨¡
        if len(df['user_id'].unique()) > self.config.max_users:
            top_users = df['user_id'].value_counts().head(self.config.max_users).index
            df = df[df['user_id'].isin(top_users)]
        
        if len(df['item_id'].unique()) > self.config.max_items:
            top_items = df['item_id'].value_counts().head(self.config.max_items).index
            df = df[df['item_id'].isin(top_items)]
        
        # ç¼–ç ç”¨æˆ·å’Œç‰©å“ID
        df['user_idx'] = self.user_encoder.fit_transform(df['user_id'])
        df['item_idx'] = self.item_encoder.fit_transform(df['item_id'])
        
        self.logger.info(f"é¢„å¤„ç†å®Œæˆ: {len(df):,} äº¤äº’, {df['user_idx'].nunique():,} ç”¨æˆ·, {df['item_idx'].nunique():,} ç‰©å“")
        
        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        from sklearn.model_selection import train_test_split
        train_df, test_df = train_test_split(df, test_size=self.config.test_ratio, random_state=42)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self._create_dataloader(train_df, shuffle=True)
        test_loader = self._create_dataloader(test_df, shuffle=False)
        
        return train_loader, test_loader
    
    def _create_dataloader(self, df: pd.DataFrame, shuffle: bool = True) -> torch.utils.data.DataLoader:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        dataset = torch.utils.data.TensorDataset(
            torch.LongTensor(df['user_idx'].values),
            torch.LongTensor(df['item_idx'].values),
            torch.FloatTensor(df['rating'].values)
        )
        
        return torch.utils.data.DataLoader(
            dataset, 
            batch_size=self.config.batch_size,
            shuffle=shuffle,
            num_workers=2
        )
    
    def train_model(self, train_loader, test_loader):
        """è®­ç»ƒç¥ç»ååŒè¿‡æ»¤æ¨¡å‹"""
        self.logger.info("å¼€å§‹è®­ç»ƒç¥ç»ååŒè¿‡æ»¤æ¨¡å‹...")
        
        n_users = len(self.user_encoder.classes_)
        n_items = len(self.item_encoder.classes_)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = NeuralCollaborativeFiltering(n_users, n_items, self.config).to(self.device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.learning_rate)
        criterion = nn.MSELoss()
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(self.config.max_epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            
            for user_ids, item_ids, ratings in train_loader:
                user_ids = user_ids.to(self.device)
                item_ids = item_ids.to(self.device)
                ratings = ratings.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(user_ids, item_ids)
                loss = criterion(outputs.squeeze(), ratings)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0.0
            
            with torch.no_grad():
                for user_ids, item_ids, ratings in test_loader:
                    user_ids = user_ids.to(self.device)
                    item_ids = item_ids.to(self.device)
                    ratings = ratings.to(self.device)
                    
                    outputs = self.model(user_ids, item_ids)
                    loss = criterion(outputs.squeeze(), ratings)
                    val_loss += loss.item()
            
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(test_loader)
            
            if epoch % 10 == 0:
                self.logger.info(f"Epoch {epoch}: Train Loss {avg_train_loss:.4f}, Val Loss {avg_val_loss:.4f}")
            
            # æ—©åœæœºåˆ¶
            if avg_val_loss < best_loss:
                best_loss = avg_val_loss
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= self.config.patience:
                    self.logger.info(f"æ—©åœäºepoch {epoch}")
                    break
        
        self.logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def run_layerwise_analysis(self, train_loader):
        """è¿è¡Œlayerwiseåˆ†æ"""
        self.logger.info("å¼€å§‹Layerwise Fisherä¿¡æ¯åˆ†æ...")
        
        # åˆå§‹åŒ–Fisheråˆ†æå™¨
        self.fisher_analyzer = LayerwiseFisherAnalyzer(self.model, self.config)
        
        # è®¡ç®—Fisherä¿¡æ¯
        fisher_info = self.fisher_analyzer.compute_fisher_information(train_loader)
        
        # åˆ†æå±‚çº§é‡è¦æ€§
        layer_importance = self.fisher_analyzer.analyze_layer_importance()
        
        # è¯†åˆ«å…³é”®å±‚
        critical_layers = self.fisher_analyzer.identify_critical_layers()
        
        # ä¿å­˜ç»“æœ
        self.results['fisher_info'] = {k: v.cpu().numpy().tolist() if isinstance(v, torch.Tensor) else v for k, v in fisher_info.items()}
        self.results['layer_importance'] = layer_importance
        self.results['critical_layers'] = critical_layers
        
        return layer_importance, critical_layers
    
    def evaluate_model(self, test_loader) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.logger.info("è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        self.model.eval()
        predictions = []
        ground_truth = []
        
        with torch.no_grad():
            for user_ids, item_ids, ratings in test_loader:
                user_ids = user_ids.to(self.device)
                item_ids = item_ids.to(self.device)
                
                outputs = self.model(user_ids, item_ids)
                predictions.extend(outputs.squeeze().cpu().numpy())
                ground_truth.extend(ratings.numpy())
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        mse = mean_squared_error(ground_truth, predictions)
        mae = mean_absolute_error(ground_truth, predictions)
        rmse = np.sqrt(mse)
        
        metrics = {
            'mse': mse,
            'mae': mae,
            'rmse': rmse,
            'test_samples': len(ground_truth)
        }
        
        self.results['performance'] = metrics
        self.logger.info(f"æ¨¡å‹æ€§èƒ½ - RMSE: {rmse:.4f}, MAE: {mae:.4f}")
        
        return metrics
    
    def save_results(self, save_dir: str = "results/amazon_layerwise"):
        """ä¿å­˜å®éªŒç»“æœ"""
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONç»“æœ
        results_file = save_path / f"amazon_layerwise_{self.category}_{timestamp}.json"
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        # ç”Ÿæˆå¯è§†åŒ–
        self._create_visualizations(save_path, timestamp)
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report(save_path, timestamp)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜è‡³: {save_path}")
    
    def _create_visualizations(self, save_path: Path, timestamp: str):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        # å±‚çº§é‡è¦æ€§å¯è§†åŒ–
        if 'layer_importance' in self.results:
            plt.figure(figsize=(10, 6))
            layers = list(self.results['layer_importance'].keys())
            importance = list(self.results['layer_importance'].values())
            
            plt.bar(layers, importance, alpha=0.7)
            plt.title(f'Layer Importance Analysis - {self.category}')
            plt.xlabel('Layer')
            plt.ylabel('Importance Score')
            plt.xticks(rotation=45)
            plt.tight_layout()
            
            plt.savefig(save_path / f"layer_importance_{self.category}_{timestamp}.png", dpi=300)
            plt.close()
    
    def _generate_report(self, save_path: Path, timestamp: str):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        report_file = save_path / f"report_{self.category}_{timestamp}.md"
        
        with open(report_file, 'w') as f:
            f.write(f"# Amazon Layerwise Analysis Report - {self.category}\n\n")
            f.write(f"**å®éªŒæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ¨¡å‹æ€§èƒ½
            if 'performance' in self.results:
                perf = self.results['performance']
                f.write("## æ¨¡å‹æ€§èƒ½\n\n")
                f.write(f"- RMSE: {perf['rmse']:.4f}\n")
                f.write(f"- MAE: {perf['mae']:.4f}\n")
                f.write(f"- æµ‹è¯•æ ·æœ¬æ•°: {perf['test_samples']:,}\n\n")
            
            # å±‚çº§é‡è¦æ€§
            if 'layer_importance' in self.results:
                f.write("## å±‚çº§é‡è¦æ€§åˆ†æ\n\n")
                for layer, importance in self.results['layer_importance'].items():
                    f.write(f"- {layer}: {importance:.4f}\n")
                f.write("\n")
            
            # å…³é”®å±‚
            if 'critical_layers' in self.results:
                f.write("## å…³é”®å±‚è¯†åˆ«\n\n")
                f.write(f"è¯†åˆ«å‡º {len(self.results['critical_layers'])} ä¸ªå…³é”®å±‚:\n")
                for layer in self.results['critical_layers']:
                    f.write(f"- {layer}\n")

def run_amazon_layerwise_experiment():
    """è¿è¡ŒAmazon Layerwiseå®éªŒ"""
    print("ğŸš€ å¼€å§‹Amazon Layerwiseå®éªŒ...")
    
    # å®éªŒé…ç½®
    config = LayerwiseConfig(
        embedding_dim=32,
        hidden_dims=[64, 32, 16],
        max_epochs=50,
        batch_size=256,
        max_users=2000,
        max_items=1500
    )
    
    # åˆå§‹åŒ–å®éªŒ
    experiment = AmazonLayerwiseExperiment('All_Beauty', config)
    
    # åŠ è½½æ•°æ®
    train_loader, test_loader = experiment.load_and_preprocess_data()
    
    # è®­ç»ƒæ¨¡å‹
    experiment.train_model(train_loader, test_loader)
    
    # Layerwiseåˆ†æ
    layer_importance, critical_layers = experiment.run_layerwise_analysis(train_loader)
    
    # è¯„ä¼°æ¨¡å‹
    metrics = experiment.evaluate_model(test_loader)
    
    # ä¿å­˜ç»“æœ
    experiment.save_results()
    
    # æ‰“å°ç»“æœæ‘˜è¦
    print("\nğŸ“Š å®éªŒç»“æœæ‘˜è¦:")
    print(f"æ¨¡å‹æ€§èƒ½ - RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}")
    print(f"å±‚çº§é‡è¦æ€§: {layer_importance}")
    print(f"å…³é”®å±‚: {critical_layers}")
    
    print("\nâœ… Amazon Layerwiseå®éªŒå®Œæˆ!")
    return experiment

if __name__ == "__main__":
    experiment = run_amazon_layerwise_experiment()
