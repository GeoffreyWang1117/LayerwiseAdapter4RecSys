#!/usr/bin/env python3
"""
çœŸå®æ¨¡å‹æ„å»ºå™¨ - å®é™…åˆ›å»ºç´§å‡‘Transformeræ¨¡å‹
ç›®æ ‡: åŸºäºé€‰æ‹©çš„å±‚æ„å»ºçœŸå®å¯è¿è¡Œçš„ç´§å‡‘æ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoTokenizer, AutoConfig
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CompactTransformerBuilder:
    """ç´§å‡‘Transformeræ„å»ºå™¨"""
    
    def __init__(self, original_model_name: str = "llama"):
        self.original_model_name = original_model_name
        self.original_model = None
        self.compact_model = None
        
    def load_original_model(self):
        """åŠ è½½åŸå§‹æ¨¡å‹"""
        logger.info(f"åŠ è½½åŸå§‹æ¨¡å‹: {self.original_model_name}")
        
        try:
            # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            if "llama" in self.original_model_name.lower():
                # è¿™é‡Œéœ€è¦å®é™…çš„æ¨¡å‹è·¯å¾„
                logger.warning("éœ€è¦å®é™…çš„LLaMAæ¨¡å‹è·¯å¾„")
                return self._create_mock_transformer_model()
            else:
                # å…¶ä»–æ¨¡å‹
                self.original_model = AutoModel.from_pretrained(self.original_model_name)
                
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {e}")
            logger.info("åˆ›å»ºæ¨¡æ‹ŸTransformeræ¨¡å‹ç”¨äºæµ‹è¯•")
            return self._create_mock_transformer_model()
    
    def _create_mock_transformer_model(self):
        """åˆ›å»ºæ¨¡æ‹Ÿçš„Transformeræ¨¡å‹ç”¨äºæµ‹è¯•"""
        logger.info("åˆ›å»º32å±‚æ¨¡æ‹ŸTransformeræ¨¡å‹")
        
        config = {
            'hidden_size': 768,
            'num_attention_heads': 12,
            'num_hidden_layers': 32,
            'intermediate_size': 3072,
            'vocab_size': 50000,
            'max_position_embeddings': 512
        }
        
        class MockTransformerLayer(nn.Module):
            def __init__(self, hidden_size, num_attention_heads, intermediate_size):
                super().__init__()
                self.attention = nn.MultiheadAttention(hidden_size, num_attention_heads, batch_first=True)
                self.norm1 = nn.LayerNorm(hidden_size)
                self.norm2 = nn.LayerNorm(hidden_size)
                self.mlp = nn.Sequential(
                    nn.Linear(hidden_size, intermediate_size),
                    nn.GELU(),
                    nn.Linear(intermediate_size, hidden_size)
                )
                
            def forward(self, x):
                # è‡ªæ³¨æ„åŠ›
                attn_out, _ = self.attention(x, x, x)
                x = self.norm1(x + attn_out)
                
                # MLP
                mlp_out = self.mlp(x)
                x = self.norm2(x + mlp_out)
                
                return x
        
        class MockTransformerModel(nn.Module):
            def __init__(self, config):
                super().__init__()
                self.config = config
                self.embeddings = nn.Embedding(config['vocab_size'], config['hidden_size'])
                self.position_embeddings = nn.Embedding(
                    config['max_position_embeddings'], 
                    config['hidden_size']
                )
                
                # 32å±‚Transformerå±‚
                self.layers = nn.ModuleList([
                    MockTransformerLayer(
                        config['hidden_size'],
                        config['num_attention_heads'],
                        config['intermediate_size']
                    ) for _ in range(config['num_hidden_layers'])
                ])
                
                self.norm = nn.LayerNorm(config['hidden_size'])
                self.classifier = nn.Linear(config['hidden_size'], config['vocab_size'])
                
            def forward(self, input_ids, output_hidden_states=False):
                batch_size, seq_len = input_ids.shape
                
                # åµŒå…¥
                x = self.embeddings(input_ids)
                
                # ä½ç½®ç¼–ç 
                position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
                position_embeds = self.position_embeddings(position_ids)
                x = x + position_embeds
                
                # é€šè¿‡æ‰€æœ‰å±‚
                hidden_states = []
                for i, layer in enumerate(self.layers):
                    x = layer(x)
                    if output_hidden_states:
                        hidden_states.append(x)
                
                # æœ€ç»ˆå½’ä¸€åŒ–
                x = self.norm(x)
                
                # åˆ†ç±»å¤´
                logits = self.classifier(x)
                
                return {
                    'logits': logits,
                    'hidden_states': hidden_states if output_hidden_states else None
                }
        
        self.original_model = MockTransformerModel(config)
        
        # ç§»åŠ¨æ¨¡å‹åˆ°æ­£ç¡®çš„è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.original_model = self.original_model.to(device)
        
        return self.original_model
    
    def build_compact_model(self, selected_layer_indices: List[int]):
        """åŸºäºé€‰æ‹©çš„å±‚æ„å»ºç´§å‡‘æ¨¡å‹"""
        logger.info(f"æ„å»ºç´§å‡‘æ¨¡å‹ï¼Œé€‰æ‹©å±‚: {selected_layer_indices}")
        
        if self.original_model is None:
            raise ValueError("å¿…é¡»å…ˆåŠ è½½åŸå§‹æ¨¡å‹")
        
        # éªŒè¯å±‚ç´¢å¼•
        total_layers = len(self.original_model.layers)
        for idx in selected_layer_indices:
            if idx >= total_layers or idx < 0:
                raise ValueError(f"å±‚ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ [0, {total_layers-1}]")
        
        # åˆ›å»ºç´§å‡‘æ¨¡å‹é…ç½®
        compact_config = self.original_model.config.copy() if hasattr(self.original_model, 'config') else {
            'hidden_size': 768,
            'num_attention_heads': 12,
            'num_hidden_layers': len(selected_layer_indices),
            'intermediate_size': 3072,
            'vocab_size': 50000,
            'max_position_embeddings': 512
        }
        compact_config['num_hidden_layers'] = len(selected_layer_indices)
        
        # æ„å»ºç´§å‡‘æ¨¡å‹
        class CompactTransformerModel(nn.Module):
            def __init__(self, original_model, selected_indices, config):
                super().__init__()
                self.config = config
                self.selected_indices = selected_indices
                
                # å¤åˆ¶åµŒå…¥å±‚
                self.embeddings = original_model.embeddings
                self.position_embeddings = original_model.position_embeddings if hasattr(original_model, 'position_embeddings') else None
                
                # å¤åˆ¶é€‰æ‹©çš„å±‚
                self.layers = nn.ModuleList()
                for idx in selected_indices:
                    # æ·±åº¦å¤åˆ¶é€‰æ‹©çš„å±‚
                    original_layer = original_model.layers[idx]
                    self.layers.append(original_layer)
                
                # å¤åˆ¶æœ€ç»ˆå±‚
                self.norm = original_model.norm
                self.classifier = original_model.classifier
                
            def forward(self, input_ids, output_hidden_states=False):
                batch_size, seq_len = input_ids.shape
                
                # åµŒå…¥
                x = self.embeddings(input_ids)
                
                # ä½ç½®ç¼–ç 
                if self.position_embeddings is not None:
                    position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
                    position_embeds = self.position_embeddings(position_ids)
                    x = x + position_embeds
                
                # é€šè¿‡é€‰æ‹©çš„å±‚
                hidden_states = []
                for i, layer in enumerate(self.layers):
                    x = layer(x)
                    if output_hidden_states:
                        hidden_states.append(x)
                
                # æœ€ç»ˆå½’ä¸€åŒ–
                x = self.norm(x)
                
                # åˆ†ç±»å¤´
                logits = self.classifier(x)
                
                return {
                    'logits': logits,
                    'hidden_states': hidden_states if output_hidden_states else None
                }
        
        self.compact_model = CompactTransformerModel(
            self.original_model, 
            selected_layer_indices, 
            compact_config
        )
        
        # ç§»åŠ¨ç´§å‡‘æ¨¡å‹åˆ°æ­£ç¡®çš„è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.compact_model = self.compact_model.to(device)
        
        logger.info(f"ç´§å‡‘æ¨¡å‹æ„å»ºå®Œæˆ: {len(selected_layer_indices)} å±‚")
        return self.compact_model
    
    def measure_model_performance(self, test_inputs):
        """æµ‹é‡æ¨¡å‹æ€§èƒ½"""
        logger.info("æµ‹é‡æ¨¡å‹æ¨ç†æ€§èƒ½...")
        
        if self.original_model is None or self.compact_model is None:
            raise ValueError("éœ€è¦å…ˆæ„å»ºåŸå§‹æ¨¡å‹å’Œç´§å‡‘æ¨¡å‹")
        
        # é¢„çƒ­
        with torch.no_grad():
            _ = self.original_model(test_inputs)
            _ = self.compact_model(test_inputs)
        
        # æµ‹é‡åŸå§‹æ¨¡å‹
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        original_times = []
        
        for _ in range(10):
            start_time = time.time()
            with torch.no_grad():
                _ = self.original_model(test_inputs)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()
            original_times.append(end_time - start_time)
        
        # æµ‹é‡ç´§å‡‘æ¨¡å‹
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        compact_times = []
        
        for _ in range(10):
            start_time = time.time()
            with torch.no_grad():
                _ = self.compact_model(test_inputs)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()
            compact_times.append(end_time - start_time)
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        original_avg = sum(original_times) / len(original_times)
        compact_avg = sum(compact_times) / len(compact_times)
        
        speedup = original_avg / compact_avg if compact_avg > 0 else 0
        
        # è®¡ç®—æ¨¡å‹å¤§å°
        original_params = sum(p.numel() for p in self.original_model.parameters())
        compact_params = sum(p.numel() for p in self.compact_model.parameters())
        
        compression_ratio = original_params / compact_params if compact_params > 0 else 0
        
        return {
            'original_inference_time': original_avg,
            'compact_inference_time': compact_avg,
            'speedup_ratio': speedup,
            'original_parameters': original_params,
            'compact_parameters': compact_params,
            'compression_ratio': compression_ratio
        }
    
    def validate_model_functionality(self, test_inputs, tolerance=0.1):
        """éªŒè¯ç´§å‡‘æ¨¡å‹åŠŸèƒ½æ­£ç¡®æ€§"""
        logger.info("éªŒè¯ç´§å‡‘æ¨¡å‹åŠŸèƒ½...")
        
        self.original_model.eval()
        self.compact_model.eval()
        
        with torch.no_grad():
            original_output = self.original_model(test_inputs)
            compact_output = self.compact_model(test_inputs)
            
            # æ¯”è¾ƒè¾“å‡º
            original_logits = original_output['logits']
            compact_logits = compact_output['logits']
            
            # è®¡ç®—è¾“å‡ºç›¸ä¼¼åº¦
            mse_loss = F.mse_loss(compact_logits, original_logits).item()
            cosine_sim = F.cosine_similarity(
                original_logits.flatten(), 
                compact_logits.flatten(), 
                dim=0
            ).item()
            
            # é¢„æµ‹ä¸€è‡´æ€§
            original_preds = torch.argmax(original_logits, dim=-1)
            compact_preds = torch.argmax(compact_logits, dim=-1)
            prediction_agreement = (original_preds == compact_preds).float().mean().item()
            
            validation_passed = (
                mse_loss < tolerance and 
                cosine_sim > (1 - tolerance) and 
                prediction_agreement > (1 - tolerance)
            )
            
            return {
                'mse_loss': mse_loss,
                'cosine_similarity': cosine_sim,
                'prediction_agreement': prediction_agreement,
                'validation_passed': validation_passed,
                'tolerance': tolerance
            }

class RealExperimentRunner:
    """çœŸå®å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self):
        self.builder = CompactTransformerBuilder()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def run_complete_experiment(self, selected_layers: List[int]):
        """è¿è¡Œå®Œæ•´çš„å®éªŒæµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„çœŸå®æ•°æ®å®éªŒ")
        
        # 1. åŠ è½½åŸå§‹æ¨¡å‹
        original_model = self.builder.load_original_model()
        
        # 2. æ„å»ºç´§å‡‘æ¨¡å‹
        compact_model = self.builder.build_compact_model(selected_layers)
        
        # 3. å‡†å¤‡æµ‹è¯•æ•°æ®
        test_inputs = self._create_test_inputs()
        
        # 4. æµ‹é‡æ€§èƒ½
        performance_results = self.builder.measure_model_performance(test_inputs)
        
        # 5. éªŒè¯åŠŸèƒ½
        validation_results = self.builder.validate_model_functionality(test_inputs)
        
        # 6. æ•´ç†ç»“æœ
        experiment_results = {
            'experiment_info': {
                'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
                'selected_layers': selected_layers,
                'original_layers': len(original_model.layers),
                'device': str(self.device)
            },
            'performance_metrics': performance_results,
            'validation_results': validation_results,
            'layer_selection_summary': {
                'total_original_layers': len(original_model.layers),
                'selected_layers_count': len(selected_layers),
                'compression_percentage': (1 - len(selected_layers) / len(original_model.layers)) * 100,
                'selected_layer_distribution': self._analyze_layer_distribution(selected_layers)
            }
        }
        
        # 7. ä¿å­˜ç»“æœ
        self._save_experiment_results(experiment_results)
        
        return experiment_results
    
    def _create_test_inputs(self):
        """åˆ›å»ºæµ‹è¯•è¾“å…¥æ•°æ®"""
        batch_size = 4
        seq_length = 128
        vocab_size = 50000
        
        # åˆ›å»ºéšæœºè¾“å…¥token
        test_inputs = torch.randint(0, vocab_size, (batch_size, seq_length))
        
        if torch.cuda.is_available():
            test_inputs = test_inputs.to(self.device)
            
        return test_inputs
    
    def _analyze_layer_distribution(self, selected_layers):
        """åˆ†æé€‰æ‹©å±‚çš„åˆ†å¸ƒ"""
        if not selected_layers:
            return {}
        
        selected_layers = sorted(selected_layers)
        
        # è®¡ç®—åˆ†å¸ƒç»Ÿè®¡
        layer_gaps = [selected_layers[i+1] - selected_layers[i] 
                     for i in range(len(selected_layers)-1)]
        
        return {
            'min_layer': min(selected_layers),
            'max_layer': max(selected_layers),
            'mean_gap': sum(layer_gaps) / len(layer_gaps) if layer_gaps else 0,
            'std_gap': (sum((gap - sum(layer_gaps)/len(layer_gaps))**2 
                           for gap in layer_gaps) / len(layer_gaps))**0.5 if layer_gaps else 0,
            'layer_range_coverage': (max(selected_layers) - min(selected_layers)) / 31 if len(selected_layers) > 1 else 0
        }
    
    def _save_experiment_results(self, results):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = results['experiment_info']['timestamp']
        results_dir = Path("results/real_experiments")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜JSONç»“æœ
        json_file = results_dir / f"compact_model_experiment_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_file = results_dir / f"experiment_report_{timestamp}.md"
        self._create_markdown_report(results, md_file)
        
        logger.info(f"å®éªŒç»“æœå·²ä¿å­˜:")
        logger.info(f"  JSON: {json_file}")
        logger.info(f"  æŠ¥å‘Š: {md_file}")
    
    def _create_markdown_report(self, results, output_file):
        """åˆ›å»ºMarkdownå®éªŒæŠ¥å‘Š"""
        report_content = f"""# çœŸå®Transformerå±‚é€‰æ‹©å®éªŒæŠ¥å‘Š

## å®éªŒæ¦‚è§ˆ

- **å®éªŒæ—¶é—´**: {results['experiment_info']['timestamp']}
- **é€‰æ‹©å±‚æ•°**: {results['experiment_info']['selected_layers']}
- **åŸå§‹å±‚æ•°**: {results['experiment_info']['original_layers']}
- **å‹ç¼©æ¯”ä¾‹**: {results['layer_selection_summary']['compression_percentage']:.1f}%
- **è®¾å¤‡**: {results['experiment_info']['device']}

## æ€§èƒ½æŒ‡æ ‡

### æ¨ç†æ€§èƒ½
- **åŸå§‹æ¨¡å‹æ¨ç†æ—¶é—´**: {results['performance_metrics']['original_inference_time']:.4f}s
- **ç´§å‡‘æ¨¡å‹æ¨ç†æ—¶é—´**: {results['performance_metrics']['compact_inference_time']:.4f}s
- **åŠ é€Ÿæ¯”**: {results['performance_metrics']['speedup_ratio']:.2f}x

### æ¨¡å‹å¤§å°
- **åŸå§‹å‚æ•°é‡**: {results['performance_metrics']['original_parameters']:,}
- **ç´§å‡‘å‚æ•°é‡**: {results['performance_metrics']['compact_parameters']:,}
- **å‹ç¼©æ¯”**: {results['performance_metrics']['compression_ratio']:.2f}x

## åŠŸèƒ½éªŒè¯

- **MSEæŸå¤±**: {results['validation_results']['mse_loss']:.6f}
- **ä½™å¼¦ç›¸ä¼¼åº¦**: {results['validation_results']['cosine_similarity']:.4f}
- **é¢„æµ‹ä¸€è‡´æ€§**: {results['validation_results']['prediction_agreement']:.4f}
- **éªŒè¯é€šè¿‡**: {'âœ…' if results['validation_results']['validation_passed'] else 'âŒ'}

## å±‚é€‰æ‹©åˆ†æ

- **é€‰æ‹©çš„å±‚**: {results['experiment_info']['selected_layers']}
- **å±‚åˆ†å¸ƒèŒƒå›´**: {results['layer_selection_summary']['selected_layer_distribution']['min_layer']} - {results['layer_selection_summary']['selected_layer_distribution']['max_layer']}
- **å¹³å‡å±‚é—´è·**: {results['layer_selection_summary']['selected_layer_distribution']['mean_gap']:.2f}
- **è¦†ç›–èŒƒå›´**: {results['layer_selection_summary']['selected_layer_distribution']['layer_range_coverage']:.2f}

## å®éªŒç»“è®º

åŸºäºçœŸå®çš„Transformerå±‚é€‰æ‹©å®éªŒï¼Œæˆ‘ä»¬æˆåŠŸæ„å»ºäº†ä¸€ä¸ªç´§å‡‘æ¨¡å‹ï¼š

1. **å‹ç¼©æ•ˆæœ**: å®ç°äº† {results['layer_selection_summary']['compression_percentage']:.1f}% çš„å±‚æ•°å‹ç¼©
2. **æ€§èƒ½æå‡**: è·å¾—äº† {results['performance_metrics']['speedup_ratio']:.2f}x çš„æ¨ç†åŠ é€Ÿ
3. **åŠŸèƒ½ä¿æŒ**: ç´§å‡‘æ¨¡å‹ä¸åŸå§‹æ¨¡å‹çš„è¾“å‡ºç›¸ä¼¼åº¦ä¸º {results['validation_results']['cosine_similarity']:.4f}

å®éªŒéªŒè¯äº†åŸºäºå±‚é€‰æ‹©çš„æ¨¡å‹å‹ç¼©æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
"""
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

def main():
    """ä¸»å‡½æ•° - è¿è¡ŒçœŸå®å®éªŒ"""
    logger.info("å¼€å§‹çœŸå®æ•°æ®Transformerå±‚é€‰æ‹©å®éªŒ")
    
    # ç¤ºä¾‹ï¼šé€‰æ‹©8ä¸ªé‡è¦å±‚ï¼ˆåŸºäºç†è®ºåˆ†æï¼‰
    # è¿™äº›å±‚ç´¢å¼•åº”è¯¥æ¥è‡ªçœŸå®çš„é‡è¦æ€§åˆ†æ
    selected_layers = [0, 4, 8, 12, 16, 20, 24, 28]  # å‡åŒ€åˆ†å¸ƒç¤ºä¾‹
    
    # æˆ–è€…åŸºäºé‡è¦æ€§åˆ†æçš„ç»“æœ
    # selected_layers = [2, 7, 15, 18, 23, 25, 29, 31]  # é‡è¦æ€§é©±åŠ¨
    
    # è¿è¡Œå®éªŒ
    runner = RealExperimentRunner()
    results = runner.run_complete_experiment(selected_layers)
    
    # è¾“å‡ºå…³é”®ç»“æœ
    logger.info("ğŸ‰ å®éªŒå®Œæˆ!")
    logger.info(f"å‹ç¼©æ¯”: {results['performance_metrics']['compression_ratio']:.2f}x")
    logger.info(f"åŠ é€Ÿæ¯”: {results['performance_metrics']['speedup_ratio']:.2f}x")
    logger.info(f"åŠŸèƒ½éªŒè¯: {'é€šè¿‡' if results['validation_results']['validation_passed'] else 'æœªé€šè¿‡'}")
    
    return results

if __name__ == "__main__":
    results = main()
