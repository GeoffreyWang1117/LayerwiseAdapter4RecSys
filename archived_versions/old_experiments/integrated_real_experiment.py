#!/usr/bin/env python3
"""
å®Œæ•´çš„çœŸå®æ•°æ®Transformerå±‚é€‰æ‹©å®éªŒ
æ•´åˆé‡è¦æ€§åˆ†æå’Œç´§å‡‘æ¨¡å‹æ„å»º
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging

# å¯¼å…¥æˆ‘ä»¬çš„åˆ†ææ¨¡å—
from real_layer_importance_analyzer import LayerImportanceIntegrator, create_synthetic_data_loader
from real_compact_model_builder import CompactTransformerBuilder, RealExperimentRunner

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntegratedExperimentRunner:
    """æ•´åˆå®éªŒè¿è¡Œå™¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_builder = CompactTransformerBuilder()
        self.original_model = None
        self.analysis_results = None
        
    def run_complete_pipeline(self, max_analysis_samples=500):
        """è¿è¡Œå®Œæ•´çš„å®éªŒæµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„çœŸå®æ•°æ®Transformerå±‚é€‰æ‹©å®éªŒ")
        
        results = {
            'experiment_info': {
                'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
                'device': str(self.device),
                'max_analysis_samples': max_analysis_samples
            }
        }
        
        # æ­¥éª¤1: åŠ è½½åŸå§‹æ¨¡å‹
        logger.info("ğŸ“‚ æ­¥éª¤1: åŠ è½½åŸå§‹æ¨¡å‹")
        self.original_model = self.model_builder.load_original_model()
        results['model_info'] = {
            'total_layers': len(self.original_model.layers),
            'total_parameters': sum(p.numel() for p in self.original_model.parameters())
        }
        
        # æ­¥éª¤2: åˆ›å»ºæ•°æ®
        logger.info("ğŸ“Š æ­¥éª¤2: å‡†å¤‡åˆ†ææ•°æ®")
        data_loader = create_synthetic_data_loader(
            batch_size=4,
            seq_length=64,
            vocab_size=50000,
            num_batches=max_analysis_samples // 4
        )
        
        # æ­¥éª¤3: å±‚é‡è¦æ€§åˆ†æ
        logger.info("ğŸ” æ­¥éª¤3: æ‰§è¡Œå±‚é‡è¦æ€§åˆ†æ")
        analyzer = LayerImportanceIntegrator(self.original_model, self.device)
        self.analysis_results = analyzer.comprehensive_layer_analysis(
            data_loader,
            max_samples=max_analysis_samples
        )
        
        # æ­¥éª¤4: é€‰æ‹©é‡è¦å±‚
        logger.info("ğŸ¯ æ­¥éª¤4: é€‰æ‹©é‡è¦å±‚")
        combined_scores = self.analysis_results['combined_importance']
        
        # æµ‹è¯•ä¸åŒçš„å±‚é€‰æ‹©ç­–ç•¥
        layer_selections = {
            'top_k_8': analyzer.select_important_layers(
                combined_scores, target_count=8, method='top_k'
            ),
            'top_k_12': analyzer.select_important_layers(
                combined_scores, target_count=12, method='top_k'
            ),
            'distributed_8': analyzer.select_important_layers(
                combined_scores, target_count=8, method='distributed_selection'
            ),
            'distributed_12': analyzer.select_important_layers(
                combined_scores, target_count=12, method='distributed_selection'
            )
        }
        
        results['layer_selections'] = layer_selections
        
        # æ­¥éª¤5: æ„å»ºå’Œè¯„ä¼°æ¯ä¸ªç´§å‡‘æ¨¡å‹
        logger.info("ğŸ—ï¸ æ­¥éª¤5: æ„å»ºå’Œè¯„ä¼°ç´§å‡‘æ¨¡å‹")
        model_evaluations = {}
        
        for selection_name, selected_layers in layer_selections.items():
            logger.info(f"  è¯„ä¼°é€‰æ‹©ç­–ç•¥: {selection_name}")
            logger.info(f"  é€‰æ‹©çš„å±‚: {selected_layers}")
            
            try:
                # æ„å»ºç´§å‡‘æ¨¡å‹
                compact_model = self.model_builder.build_compact_model(selected_layers)
                
                # å‡†å¤‡æµ‹è¯•æ•°æ®
                test_inputs = self._create_test_inputs()
                
                # æ€§èƒ½æµ‹è¯•
                performance_metrics = self.model_builder.measure_model_performance(test_inputs)
                
                # åŠŸèƒ½éªŒè¯
                validation_results = self.model_builder.validate_model_functionality(test_inputs)
                
                model_evaluations[selection_name] = {
                    'selected_layers': selected_layers,
                    'layer_count': len(selected_layers),
                    'compression_ratio': len(self.original_model.layers) / len(selected_layers),
                    'performance_metrics': performance_metrics,
                    'validation_results': validation_results,
                    'success': True
                }
                
                logger.info(f"    âœ… {selection_name}: å‹ç¼©æ¯” {model_evaluations[selection_name]['compression_ratio']:.2f}x, "
                           f"åŠ é€Ÿæ¯” {performance_metrics['speedup_ratio']:.2f}x")
                
            except Exception as e:
                logger.error(f"    âŒ {selection_name} è¯„ä¼°å¤±è´¥: {e}")
                model_evaluations[selection_name] = {
                    'selected_layers': selected_layers,
                    'error': str(e),
                    'success': False
                }
        
        results['model_evaluations'] = model_evaluations
        
        # æ­¥éª¤6: ç”Ÿæˆå¯è§†åŒ–å’ŒæŠ¥å‘Š
        logger.info("ğŸ“ˆ æ­¥éª¤6: ç”Ÿæˆå¯è§†åŒ–å’ŒæŠ¥å‘Š")
        output_dir = Path("results/integrated_experiment")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å±‚é‡è¦æ€§å¯è§†åŒ–
        viz_file = analyzer.create_analysis_visualization(
            self.analysis_results, 
            output_dir
        )
        
        # åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–
        self._create_comparison_visualization(model_evaluations, output_dir)
        
        # æ­¥éª¤7: ä¿å­˜å®Œæ•´ç»“æœ
        timestamp = results['experiment_info']['timestamp']
        results_file = output_dir / f"integrated_experiment_{timestamp}.json"
        
        # æ·»åŠ åˆ†æç»“æœåˆ°æœ€ç»ˆç»“æœä¸­
        results['layer_analysis'] = self.analysis_results
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºç»¼åˆæŠ¥å‘Š
        report_file = output_dir / f"comprehensive_report_{timestamp}.md"
        self._create_comprehensive_report(results, report_file)
        
        logger.info("ğŸ‰ å®Œæ•´å®éªŒæµç¨‹å®Œæˆ!")
        logger.info(f"ç»“æœä¿å­˜è‡³: {results_file}")
        logger.info(f"æŠ¥å‘Šä¿å­˜è‡³: {report_file}")
        
        # è¾“å‡ºå…³é”®ç»“æœæ‘˜è¦
        self._print_results_summary(model_evaluations)
        
        return results
    
    def _create_test_inputs(self):
        """åˆ›å»ºæµ‹è¯•è¾“å…¥"""
        batch_size = 4
        seq_length = 64
        vocab_size = 50000
        
        test_inputs = torch.randint(0, vocab_size, (batch_size, seq_length))
        return test_inputs.to(self.device)
    
    def _create_comparison_visualization(self, model_evaluations, output_dir):
        """åˆ›å»ºæ¨¡å‹å¯¹æ¯”å¯è§†åŒ–"""
        import matplotlib.pyplot as plt
        
        # å‡†å¤‡æ•°æ®
        methods = []
        compression_ratios = []
        speedup_ratios = []
        validation_scores = []
        
        for method, results in model_evaluations.items():
            if results.get('success', False):
                methods.append(method)
                compression_ratios.append(results['compression_ratio'])
                speedup_ratios.append(results['performance_metrics']['speedup_ratio'])
                validation_scores.append(results['validation_results']['cosine_similarity'])
        
        if not methods:
            logger.warning("æ²¡æœ‰æˆåŠŸçš„æ¨¡å‹è¯„ä¼°ç»“æœï¼Œè·³è¿‡å¯¹æ¯”å¯è§†åŒ–")
            return
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        fig.suptitle('Compact Model Comparison', fontsize=16)
        
        # å‹ç¼©æ¯”å¯¹æ¯”
        bars1 = axes[0].bar(methods, compression_ratios, alpha=0.7, color='blue')
        axes[0].set_title('Compression Ratio')
        axes[0].set_ylabel('Compression Ratio (x)')
        axes[0].tick_params(axis='x', rotation=45)
        
        # åŠ é€Ÿæ¯”å¯¹æ¯”
        bars2 = axes[1].bar(methods, speedup_ratios, alpha=0.7, color='green')
        axes[1].set_title('Speedup Ratio')
        axes[1].set_ylabel('Speedup Ratio (x)')
        axes[1].tick_params(axis='x', rotation=45)
        
        # éªŒè¯åˆ†æ•°å¯¹æ¯”
        bars3 = axes[2].bar(methods, validation_scores, alpha=0.7, color='red')
        axes[2].set_title('Validation Score (Cosine Similarity)')
        axes[2].set_ylabel('Cosine Similarity')
        axes[2].tick_params(axis='x', rotation=45)
        axes[2].axhline(y=0.9, color='orange', linestyle='--', label='Good Threshold')
        axes[2].legend()
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        comparison_file = output_dir / 'model_comparison.png'
        plt.savefig(comparison_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"å¯¹æ¯”å¯è§†åŒ–ä¿å­˜è‡³: {comparison_file}")
    
    def _create_comprehensive_report(self, results, output_file):
        """åˆ›å»ºç»¼åˆå®éªŒæŠ¥å‘Š"""
        timestamp = results['experiment_info']['timestamp']
        
        report_content = f"""# å®Œæ•´Transformerå±‚é€‰æ‹©å®éªŒæŠ¥å‘Š

## å®éªŒæ¦‚è§ˆ

- **å®éªŒæ—¶é—´**: {timestamp}
- **è®¾å¤‡**: {results['experiment_info']['device']}
- **åŸå§‹æ¨¡å‹å±‚æ•°**: {results['model_info']['total_layers']}
- **åŸå§‹æ¨¡å‹å‚æ•°**: {results['model_info']['total_parameters']:,}

## å±‚é‡è¦æ€§åˆ†æç»“æœ

"""
        
        # æ·»åŠ é‡è¦æ€§åˆ†ææ‘˜è¦
        if 'layer_analysis' in results:
            analysis = results['layer_analysis']
            
            report_content += f"""### åˆ†ææ–¹æ³•æˆåŠŸç‡
- Fisherä¿¡æ¯çŸ©é˜µ: {'âœ…' if analysis.get('fisher_information') else 'âŒ'}
- æ¢¯åº¦èŒƒæ•°åˆ†æ: {'âœ…' if analysis.get('gradient_norms') else 'âŒ'}  
- æ¿€æ´»ç»Ÿè®¡åˆ†æ: {'âœ…' if analysis.get('activation_statistics') else 'âŒ'}
- SHAPå€¼åˆ†æ: {'âœ…' if analysis.get('shap_values') else 'âŒ'}

### æœ€é‡è¦çš„å±‚ (Top 10)
"""
            
            combined_scores = analysis.get('combined_importance', {})
            if combined_scores:
                sorted_layers = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
                for i, (layer, score) in enumerate(sorted_layers[:10], 1):
                    report_content += f"{i}. å±‚ {layer}: {score:.4f}\n"
        
        report_content += f"""

## æ¨¡å‹æ„å»ºå’Œè¯„ä¼°ç»“æœ

"""
        
        # æ·»åŠ æ¯ä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœ
        successful_models = []
        for method, evaluation in results['model_evaluations'].items():
            if evaluation.get('success', False):
                successful_models.append((method, evaluation))
                
                report_content += f"""### {method}

- **é€‰æ‹©å±‚æ•°**: {evaluation['layer_count']} / {results['model_info']['total_layers']}
- **é€‰æ‹©çš„å±‚**: {evaluation['selected_layers']}
- **å‹ç¼©æ¯”**: {evaluation['compression_ratio']:.2f}x
- **æ¨ç†åŠ é€Ÿ**: {evaluation['performance_metrics']['speedup_ratio']:.2f}x
- **å‚æ•°å‹ç¼©**: {evaluation['performance_metrics']['compression_ratio']:.2f}x
- **åŠŸèƒ½éªŒè¯**: {'âœ…' if evaluation['validation_results']['validation_passed'] else 'âŒ'}
  - MSEæŸå¤±: {evaluation['validation_results']['mse_loss']:.6f}
  - ä½™å¼¦ç›¸ä¼¼åº¦: {evaluation['validation_results']['cosine_similarity']:.4f}
  - é¢„æµ‹ä¸€è‡´æ€§: {evaluation['validation_results']['prediction_agreement']:.4f}

"""
        
        # æ·»åŠ æœ€ä½³æ¨¡å‹æ¨è
        if successful_models:
            # æŒ‰ç»¼åˆæ€§èƒ½æ’åºï¼ˆè€ƒè™‘å‹ç¼©æ¯”ã€åŠ é€Ÿæ¯”å’ŒéªŒè¯åˆ†æ•°ï¼‰
            def score_model(model_data):
                metrics = model_data[1]['performance_metrics']
                validation = model_data[1]['validation_results']
                
                # ç»¼åˆè¯„åˆ†ï¼šå‹ç¼©æ¯” + åŠ é€Ÿæ¯” + éªŒè¯è´¨é‡
                score = (
                    metrics['compression_ratio'] * 0.3 +
                    metrics['speedup_ratio'] * 0.4 +
                    validation['cosine_similarity'] * 0.3
                )
                return score
            
            best_model = max(successful_models, key=score_model)
            
            report_content += f"""## æ¨èæ¨¡å‹

**æœ€ä½³ç»¼åˆæ€§èƒ½**: {best_model[0]}

æ­¤æ¨¡å‹åœ¨å‹ç¼©æ¯”ã€æ¨ç†é€Ÿåº¦å’ŒåŠŸèƒ½ä¿æŒæ–¹é¢è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼š
- å®ç°äº† {best_model[1]['compression_ratio']:.2f}x çš„æ¨¡å‹å‹ç¼©
- è·å¾—äº† {best_model[1]['performance_metrics']['speedup_ratio']:.2f}x çš„æ¨ç†åŠ é€Ÿ  
- ä¿æŒäº† {best_model[1]['validation_results']['cosine_similarity']:.4f} çš„è¾“å‡ºç›¸ä¼¼åº¦

## å®éªŒç»“è®º

1. **é‡è¦æ€§åˆ†ææœ‰æ•ˆæ€§**: æˆåŠŸé€šè¿‡å¤šç»´åº¦åˆ†æè¯†åˆ«äº†å…³é”®å±‚
2. **æ¨¡å‹å‹ç¼©å¯è¡Œæ€§**: è¯æ˜äº†å¯ä»¥åœ¨ä¿æŒæ€§èƒ½çš„æƒ…å†µä¸‹æ˜¾è‘—å‹ç¼©æ¨¡å‹
3. **æ–¹æ³•è®ºéªŒè¯**: éªŒè¯äº†åŸºäºçœŸå®æ•°æ®çš„å±‚é€‰æ‹©æ–¹æ³•çš„æœ‰æ•ˆæ€§

æœ¬å®éªŒä¸ºæ„å»ºé«˜æ•ˆçš„ç´§å‡‘Transformeræ¨¡å‹æä¾›äº†å¯é çš„æŠ€æœ¯è·¯å¾„ã€‚
"""
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
    
    def _print_results_summary(self, model_evaluations):
        """æ‰“å°ç»“æœæ‘˜è¦"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ å®éªŒç»“æœæ‘˜è¦")
        logger.info("="*60)
        
        successful_models = [(name, results) for name, results in model_evaluations.items() 
                           if results.get('success', False)]
        
        if not successful_models:
            logger.warning("âŒ æ²¡æœ‰æˆåŠŸçš„æ¨¡å‹è¯„ä¼°")
            return
        
        for name, results in successful_models:
            logger.info(f"\nğŸ“Š {name}:")
            logger.info(f"   å‹ç¼©æ¯”: {results['compression_ratio']:.2f}x")
            logger.info(f"   åŠ é€Ÿæ¯”: {results['performance_metrics']['speedup_ratio']:.2f}x")
            logger.info(f"   ç›¸ä¼¼åº¦: {results['validation_results']['cosine_similarity']:.4f}")
            logger.info(f"   éªŒè¯: {'âœ… é€šè¿‡' if results['validation_results']['validation_passed'] else 'âŒ æœªé€šè¿‡'}")
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        def model_score(model_data):
            _, results = model_data
            return (
                results['performance_metrics']['speedup_ratio'] * 0.4 +
                results['compression_ratio'] * 0.3 +
                results['validation_results']['cosine_similarity'] * 0.3
            )
        
        best_model_name, best_results = max(successful_models, key=model_score)
        
        logger.info(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
        logger.info(f"   é€‰æ‹©å±‚: {best_results['selected_layers']}")
        logger.info("="*60)

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹å®Œæ•´çš„çœŸå®æ•°æ®Transformerå±‚é€‰æ‹©å®éªŒ")
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = IntegratedExperimentRunner()
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    results = runner.run_complete_pipeline(max_analysis_samples=400)
    
    return results

if __name__ == "__main__":
    results = main()
