#!/usr/bin/env python3
"""
çœŸå®å±‚é‡è¦æ€§åˆ†æå™¨
åŸºäºFisherä¿¡æ¯çŸ©é˜µã€SHAPå€¼ã€æ¢¯åº¦èŒƒæ•°ç­‰å¤šç§æ–¹æ³•åˆ†æTransformerå±‚çš„é‡è¦æ€§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns

# å°è¯•å¯¼å…¥SHAPï¼ˆå¯é€‰ï¼‰
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    logging.warning("SHAP not available. Install with: pip install shap")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FisherInformationAnalyzer:
    """Fisherä¿¡æ¯çŸ©é˜µåˆ†æå™¨"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.layer_fisher_scores = {}
        
    def compute_fisher_information_matrix(self, data_loader, max_samples=1000):
        """è®¡ç®—æ¯å±‚çš„Fisherä¿¡æ¯çŸ©é˜µ"""
        logger.info("å¼€å§‹è®¡ç®—Fisherä¿¡æ¯çŸ©é˜µ...")
        
        self.model.eval()
        fisher_dict = defaultdict(float)
        total_samples = 0
        
        # æ”¶é›†æ‰€æœ‰å±‚çš„åç§°
        layer_names = [name for name, _ in self.model.named_parameters() 
                      if 'layers.' in name and ('weight' in name or 'bias' in name)]
        
        for batch_idx, batch_data in enumerate(data_loader):
            if total_samples >= max_samples:
                break
                
            if batch_idx % 50 == 0:
                logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx}, å·²å¤„ç†æ ·æœ¬: {total_samples}")
            
            # å‡†å¤‡è¾“å…¥æ•°æ®
            if isinstance(batch_data, dict):
                inputs = batch_data['input_ids'].to(self.device)
                targets = batch_data.get('labels', inputs).to(self.device)  # ç¡®ä¿æ ‡ç­¾ä¹Ÿåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            else:
                inputs = batch_data.to(self.device)
                targets = inputs
            
            batch_size = inputs.size(0)
            
            # å¯¹æ¯ä¸ªæ ·æœ¬è®¡ç®—Fisherä¿¡æ¯
            for sample_idx in range(batch_size):
                if total_samples >= max_samples:
                    break
                    
                sample_input = inputs[sample_idx:sample_idx+1]
                sample_target = targets[sample_idx:sample_idx+1]
                
                # å‰å‘ä¼ æ’­
                self.model.zero_grad()
                outputs = self.model(sample_input)
                
                # è®¡ç®—æŸå¤±
                if hasattr(outputs, 'logits'):
                    logits = outputs.logits
                else:
                    logits = outputs['logits'] if isinstance(outputs, dict) else outputs
                
                # ä½¿ç”¨äº¤å‰ç†µæŸå¤±
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), 
                                     sample_target.view(-1), 
                                     reduction='mean')
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # ç´¯ç§¯Fisherä¿¡æ¯ (æ¢¯åº¦çš„å¹³æ–¹)
                for name, param in self.model.named_parameters():
                    if param.grad is not None and 'layers.' in name:
                        # æå–å±‚å·
                        layer_num = self._extract_layer_number(name)
                        if layer_num is not None:
                            fisher_value = (param.grad ** 2).sum().item()
                            fisher_dict[layer_num] += fisher_value
                
                total_samples += 1
        
        # å½’ä¸€åŒ–Fisherä¿¡æ¯
        for layer_num in fisher_dict:
            fisher_dict[layer_num] /= total_samples
        
        self.layer_fisher_scores = dict(fisher_dict)
        logger.info(f"Fisherä¿¡æ¯è®¡ç®—å®Œæˆï¼Œå¤„ç†äº† {total_samples} ä¸ªæ ·æœ¬")
        
        return self.layer_fisher_scores
    
    def _extract_layer_number(self, param_name):
        """ä»å‚æ•°åç§°ä¸­æå–å±‚å·"""
        try:
            if 'layers.' in param_name:
                parts = param_name.split('layers.')[1].split('.')[0]
                return int(parts)
        except (ValueError, IndexError):
            pass
        return None

class GradientNormAnalyzer:
    """æ¢¯åº¦èŒƒæ•°åˆ†æå™¨"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.layer_gradient_norms = {}
    
    def compute_gradient_norms(self, data_loader, max_samples=500):
        """è®¡ç®—æ¯å±‚çš„æ¢¯åº¦èŒƒæ•°"""
        logger.info("å¼€å§‹è®¡ç®—æ¢¯åº¦èŒƒæ•°...")
        
        self.model.eval()
        grad_norms = defaultdict(list)
        total_samples = 0
        
        for batch_idx, batch_data in enumerate(data_loader):
            if total_samples >= max_samples:
                break
                
            if batch_idx % 25 == 0:
                logger.info(f"æ¢¯åº¦èŒƒæ•°åˆ†æ - æ‰¹æ¬¡ {batch_idx}")
            
            # å‡†å¤‡æ•°æ®
            if isinstance(batch_data, dict):
                inputs = batch_data['input_ids'].to(self.device)
                targets = batch_data.get('labels', inputs).to(self.device)
            else:
                inputs = batch_data.to(self.device)
                targets = inputs
            
            # å‰å‘ä¼ æ’­
            self.model.zero_grad()
            outputs = self.model(inputs)
            
            # è®¡ç®—æŸå¤±
            if hasattr(outputs, 'logits'):
                logits = outputs.logits
            else:
                logits = outputs['logits'] if isinstance(outputs, dict) else outputs
            
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), 
                                 targets.view(-1), 
                                 reduction='mean')
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # è®¡ç®—æ¯å±‚çš„æ¢¯åº¦èŒƒæ•°
            for name, param in self.model.named_parameters():
                if param.grad is not None and 'layers.' in name:
                    layer_num = self._extract_layer_number(name)
                    if layer_num is not None:
                        grad_norm = param.grad.norm().item()
                        grad_norms[layer_num].append(grad_norm)
            
            total_samples += inputs.size(0)
        
        # è®¡ç®—å¹³å‡æ¢¯åº¦èŒƒæ•°
        avg_grad_norms = {}
        for layer_num, norms in grad_norms.items():
            avg_grad_norms[layer_num] = np.mean(norms)
        
        self.layer_gradient_norms = avg_grad_norms
        logger.info("æ¢¯åº¦èŒƒæ•°è®¡ç®—å®Œæˆ")
        
        return self.layer_gradient_norms
    
    def _extract_layer_number(self, param_name):
        """ä»å‚æ•°åç§°ä¸­æå–å±‚å·"""
        try:
            if 'layers.' in param_name:
                parts = param_name.split('layers.')[1].split('.')[0]
                return int(parts)
        except (ValueError, IndexError):
            pass
        return None

class ActivationAnalyzer:
    """æ¿€æ´»åˆ†æå™¨"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.activation_stats = {}
        self.hooks = []
        
    def compute_activation_statistics(self, data_loader, max_samples=300):
        """è®¡ç®—æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("å¼€å§‹è®¡ç®—æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯...")
        
        # æ³¨å†Œé’©å­å‡½æ•°
        activation_data = defaultdict(list)
        
        def create_hook(layer_idx):
            def hook_fn(module, input, output):
                if isinstance(output, torch.Tensor):
                    # è®¡ç®—æ¿€æ´»çš„ç»Ÿè®¡ä¿¡æ¯
                    activation_data[layer_idx].append({
                        'variance': output.var().item(),
                        'mean': output.mean().item(),
                        'std': output.std().item(),
                        'max': output.max().item(),
                        'min': output.min().item(),
                        'sparsity': (output == 0).float().mean().item()
                    })
            return hook_fn
        
        # ä¸ºæ¯å±‚æ³¨å†Œé’©å­
        for name, module in self.model.named_modules():
            if 'layers.' in name and name.count('.') == 1:  # åªæ³¨å†Œé¡¶å±‚layeræ¨¡å—
                try:
                    layer_idx = int(name.split('layers.')[1])
                    hook = module.register_forward_hook(create_hook(layer_idx))
                    self.hooks.append(hook)
                except (ValueError, IndexError):
                    continue
        
        # è¿è¡Œå‰å‘ä¼ æ’­æ”¶é›†æ¿€æ´»
        self.model.eval()
        total_samples = 0
        
        with torch.no_grad():
            for batch_idx, batch_data in enumerate(data_loader):
                if total_samples >= max_samples:
                    break
                    
                if batch_idx % 20 == 0:
                    logger.info(f"æ¿€æ´»åˆ†æ - æ‰¹æ¬¡ {batch_idx}")
                
                # å‡†å¤‡æ•°æ®
                if isinstance(batch_data, dict):
                    inputs = batch_data['input_ids'].to(self.device)
                else:
                    inputs = batch_data.to(self.device)
                
                # å‰å‘ä¼ æ’­
                _ = self.model(inputs)
                total_samples += inputs.size(0)
        
        # æ¸…ç†é’©å­
        for hook in self.hooks:
            hook.remove()
        self.hooks = []
        
        # èšåˆç»Ÿè®¡ä¿¡æ¯
        layer_stats = {}
        for layer_idx, stats_list in activation_data.items():
            if stats_list:
                layer_stats[layer_idx] = {
                    'avg_variance': np.mean([s['variance'] for s in stats_list]),
                    'avg_mean': np.mean([s['mean'] for s in stats_list]),
                    'avg_std': np.mean([s['std'] for s in stats_list]),
                    'avg_sparsity': np.mean([s['sparsity'] for s in stats_list]),
                    'activation_range': np.mean([s['max'] - s['min'] for s in stats_list])
                }
        
        self.activation_stats = layer_stats
        logger.info("æ¿€æ´»ç»Ÿè®¡è®¡ç®—å®Œæˆ")
        
        return self.activation_stats

class SHAPAnalyzer:
    """SHAPå€¼åˆ†æå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.shap_values = {}
        
    def compute_shap_values(self, data_loader, max_samples=100):
        """è®¡ç®—SHAPå€¼"""
        if not SHAP_AVAILABLE:
            logger.warning("SHAPä¸å¯ç”¨ï¼Œè·³è¿‡SHAPåˆ†æ")
            return {}
        
        logger.info("å¼€å§‹è®¡ç®—SHAPå€¼...")
        
        try:
            # å‡†å¤‡æ•°æ®
            sample_inputs = []
            sample_count = 0
            
            for batch_data in data_loader:
                if sample_count >= max_samples:
                    break
                    
                if isinstance(batch_data, dict):
                    inputs = batch_data['input_ids']
                else:
                    inputs = batch_data
                
                for i in range(min(10, inputs.size(0))):  # æ¯æ‰¹æ¬¡æœ€å¤š10ä¸ªæ ·æœ¬
                    if sample_count >= max_samples:
                        break
                    sample_inputs.append(inputs[i])
                    sample_count += 1
            
            # è½¬æ¢ä¸ºå¼ é‡
            sample_inputs = torch.stack(sample_inputs).to(self.device)
            
            # åˆ›å»ºSHAPè§£é‡Šå™¨
            def model_wrapper(x):
                with torch.no_grad():
                    outputs = self.model(x)
                    if hasattr(outputs, 'logits'):
                        return outputs.logits
                    else:
                        return outputs['logits'] if isinstance(outputs, dict) else outputs
            
            # ä½¿ç”¨å‰10ä¸ªæ ·æœ¬ä½œä¸ºèƒŒæ™¯
            background = sample_inputs[:10]
            explainer = shap.DeepExplainer(model_wrapper, background)
            
            # è®¡ç®—SHAPå€¼
            shap_values = explainer.shap_values(sample_inputs[:20])  # è§£é‡Šå‰20ä¸ªæ ·æœ¬
            
            # èšåˆæ¯å±‚çš„SHAPé‡è¦æ€§
            layer_shap_importance = {}
            if isinstance(shap_values, list):
                shap_values = shap_values[0]  # å–ç¬¬ä¸€ä¸ªç±»åˆ«çš„SHAPå€¼
            
            # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹æ¶æ„è°ƒæ•´SHAPå€¼çš„èšåˆæ–¹å¼
            # ç®€åŒ–ç‰ˆæœ¬ï¼šè®¡ç®—SHAPå€¼çš„ç»å¯¹å€¼å‡å€¼
            for layer_idx in range(len(self.model.layers)):
                layer_shap_importance[layer_idx] = np.abs(shap_values).mean()
            
            self.shap_values = layer_shap_importance
            logger.info("SHAPå€¼è®¡ç®—å®Œæˆ")
            
        except Exception as e:
            logger.error(f"SHAPè®¡ç®—å¤±è´¥: {e}")
            return {}
        
        return self.shap_values

class LayerImportanceIntegrator:
    """å±‚é‡è¦æ€§ç»¼åˆåˆ†æå™¨"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.fisher_analyzer = FisherInformationAnalyzer(model, device)
        self.gradient_analyzer = GradientNormAnalyzer(model, device)
        self.activation_analyzer = ActivationAnalyzer(model, device)
        self.shap_analyzer = SHAPAnalyzer(model, device)
        
    def comprehensive_layer_analysis(self, data_loader, max_samples=1000):
        """ç»¼åˆå±‚é‡è¦æ€§åˆ†æ"""
        logger.info("ğŸ” å¼€å§‹ç»¼åˆå±‚é‡è¦æ€§åˆ†æ...")
        
        analysis_results = {}
        
        # 1. Fisherä¿¡æ¯çŸ©é˜µåˆ†æ
        try:
            fisher_scores = self.fisher_analyzer.compute_fisher_information_matrix(
                data_loader, max_samples
            )
            analysis_results['fisher_information'] = fisher_scores
            logger.info(f"âœ… Fisheråˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ° {len(fisher_scores)} å±‚")
        except Exception as e:
            logger.error(f"âŒ Fisheråˆ†æå¤±è´¥: {e}")
            analysis_results['fisher_information'] = {}
        
        # 2. æ¢¯åº¦èŒƒæ•°åˆ†æ
        try:
            gradient_norms = self.gradient_analyzer.compute_gradient_norms(
                data_loader, max_samples//2
            )
            analysis_results['gradient_norms'] = gradient_norms
            logger.info(f"âœ… æ¢¯åº¦èŒƒæ•°åˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ° {len(gradient_norms)} å±‚")
        except Exception as e:
            logger.error(f"âŒ æ¢¯åº¦èŒƒæ•°åˆ†æå¤±è´¥: {e}")
            analysis_results['gradient_norms'] = {}
        
        # 3. æ¿€æ´»ç»Ÿè®¡åˆ†æ
        try:
            activation_stats = self.activation_analyzer.compute_activation_statistics(
                data_loader, max_samples//3
            )
            analysis_results['activation_statistics'] = activation_stats
            logger.info(f"âœ… æ¿€æ´»åˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ° {len(activation_stats)} å±‚")
        except Exception as e:
            logger.error(f"âŒ æ¿€æ´»åˆ†æå¤±è´¥: {e}")
            analysis_results['activation_statistics'] = {}
        
        # 4. SHAPåˆ†æï¼ˆå¯é€‰ï¼‰
        try:
            shap_values = self.shap_analyzer.compute_shap_values(
                data_loader, max_samples//10
            )
            analysis_results['shap_values'] = shap_values
            if shap_values:
                logger.info(f"âœ… SHAPåˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ° {len(shap_values)} å±‚")
            else:
                logger.info("âš ï¸ SHAPåˆ†æè·³è¿‡")
        except Exception as e:
            logger.error(f"âŒ SHAPåˆ†æå¤±è´¥: {e}")
            analysis_results['shap_values'] = {}
        
        # 5. ç»¼åˆè¯„åˆ†
        combined_scores = self._compute_combined_importance_scores(analysis_results)
        analysis_results['combined_importance'] = combined_scores
        
        logger.info("ğŸ‰ ç»¼åˆå±‚é‡è¦æ€§åˆ†æå®Œæˆ!")
        return analysis_results
    
    def _compute_combined_importance_scores(self, analysis_results):
        """è®¡ç®—ç»¼åˆé‡è¦æ€§è¯„åˆ†"""
        logger.info("è®¡ç®—ç»¼åˆé‡è¦æ€§è¯„åˆ†...")
        
        # è·å–æ‰€æœ‰æ£€æµ‹åˆ°çš„å±‚
        all_layers = set()
        for analysis_name, scores in analysis_results.items():
            if isinstance(scores, dict):
                all_layers.update(scores.keys())
        
        if not all_layers:
            logger.warning("æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•å±‚ï¼Œè¿”å›ç©ºç»“æœ")
            return {}
        
        all_layers = sorted(list(all_layers))
        combined_scores = {}
        
        # å½’ä¸€åŒ–å„é¡¹æŒ‡æ ‡
        normalized_scores = {}
        
        # Fisherä¿¡æ¯å½’ä¸€åŒ–
        fisher_scores = analysis_results.get('fisher_information', {})
        if fisher_scores:
            max_fisher = max(fisher_scores.values()) if fisher_scores.values() else 1
            normalized_scores['fisher'] = {
                layer: score / max_fisher for layer, score in fisher_scores.items()
            }
        
        # æ¢¯åº¦èŒƒæ•°å½’ä¸€åŒ–
        gradient_scores = analysis_results.get('gradient_norms', {})
        if gradient_scores:
            max_grad = max(gradient_scores.values()) if gradient_scores.values() else 1
            normalized_scores['gradient'] = {
                layer: score / max_grad for layer, score in gradient_scores.items()
            }
        
        # æ¿€æ´»ç»Ÿè®¡å½’ä¸€åŒ–
        activation_scores = analysis_results.get('activation_statistics', {})
        if activation_scores:
            # ä½¿ç”¨æ–¹å·®ä½œä¸ºæ¿€æ´»é‡è¦æ€§æŒ‡æ ‡
            variance_scores = {
                layer: stats['avg_variance'] 
                for layer, stats in activation_scores.items()
            }
            max_var = max(variance_scores.values()) if variance_scores.values() else 1
            normalized_scores['activation'] = {
                layer: score / max_var for layer, score in variance_scores.items()
            }
        
        # SHAPå€¼å½’ä¸€åŒ–
        shap_scores = analysis_results.get('shap_values', {})
        if shap_scores:
            max_shap = max(shap_scores.values()) if shap_scores.values() else 1
            normalized_scores['shap'] = {
                layer: score / max_shap for layer, score in shap_scores.items()
            }
        
        # ç»¼åˆè¯„åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        weights = {
            'fisher': 0.4,      # Fisherä¿¡æ¯æœ€é‡è¦
            'gradient': 0.3,    # æ¢¯åº¦èŒƒæ•°æ¬¡é‡è¦
            'activation': 0.2,  # æ¿€æ´»ç»Ÿè®¡ä¸­ç­‰é‡è¦
            'shap': 0.1        # SHAPå€¼è¾…åŠ©å‚è€ƒ
        }
        
        for layer in all_layers:
            total_score = 0.0
            total_weight = 0.0
            
            for metric, weight in weights.items():
                if metric in normalized_scores and layer in normalized_scores[metric]:
                    total_score += weight * normalized_scores[metric][layer]
                    total_weight += weight
            
            # é¿å…é™¤é›¶
            if total_weight > 0:
                combined_scores[layer] = total_score / total_weight
            else:
                combined_scores[layer] = 0.0
        
        return combined_scores
    
    def select_important_layers(self, combined_scores, target_count=8, method='top_k'):
        """åŸºäºç»¼åˆè¯„åˆ†é€‰æ‹©é‡è¦å±‚"""
        logger.info(f"é€‰æ‹©å‰ {target_count} ä¸ªé‡è¦å±‚...")
        
        if not combined_scores:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„é‡è¦æ€§è¯„åˆ†")
            return []
        
        if method == 'top_k':
            # ç®€å•çš„Top-Ké€‰æ‹©
            sorted_layers = sorted(
                combined_scores.items(),
                key=lambda x: x[1],
                reverse=True
            )
            selected_layers = [layer for layer, score in sorted_layers[:target_count]]
            
        elif method == 'distributed_selection':
            # åˆ†å¸ƒå¼é€‰æ‹©ï¼šåœ¨ä¸åŒåŒºé—´é€‰æ‹©é‡è¦å±‚
            sorted_layers = sorted(combined_scores.items(), key=lambda x: x[0])  # æŒ‰å±‚å·æ’åº
            total_layers = len(sorted_layers)
            
            # å°†å±‚åˆ†ä¸ºå‡ ä¸ªåŒºé—´
            num_sections = min(4, target_count)
            layers_per_section = target_count // num_sections
            extra_layers = target_count % num_sections
            
            selected_layers = []
            section_size = total_layers // num_sections
            
            for section_idx in range(num_sections):
                start_idx = section_idx * section_size
                end_idx = start_idx + section_size if section_idx < num_sections - 1 else total_layers
                
                # åœ¨å½“å‰åŒºé—´ä¸­æŒ‰é‡è¦æ€§é€‰æ‹©
                section_layers = sorted_layers[start_idx:end_idx]
                section_scores = {layer: combined_scores[layer] for layer, _ in section_layers}
                section_sorted = sorted(section_scores.items(), key=lambda x: x[1], reverse=True)
                
                # é€‰æ‹©å½“å‰åŒºé—´çš„å±‚
                layers_to_select = layers_per_section + (1 if section_idx < extra_layers else 0)
                for layer, _ in section_sorted[:layers_to_select]:
                    selected_layers.append(layer)
            
        else:
            raise ValueError(f"æœªçŸ¥çš„é€‰æ‹©æ–¹æ³•: {method}")
        
        selected_layers = sorted(selected_layers)
        logger.info(f"é€‰æ‹©çš„é‡è¦å±‚: {selected_layers}")
        
        return selected_layers
    
    def create_analysis_visualization(self, analysis_results, output_dir):
        """åˆ›å»ºåˆ†æå¯è§†åŒ–"""
        logger.info("ç”Ÿæˆåˆ†æå¯è§†åŒ–...")
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®ç»˜å›¾é£æ ¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Transformer Layer Importance Analysis', fontsize=16)
        
        # 1. Fisherä¿¡æ¯å¯è§†åŒ–
        fisher_scores = analysis_results.get('fisher_information', {})
        if fisher_scores:
            layers = sorted(fisher_scores.keys())
            scores = [fisher_scores[layer] for layer in layers]
            axes[0,0].bar(layers, scores, alpha=0.7, color='blue')
            axes[0,0].set_title('Fisher Information Matrix')
            axes[0,0].set_xlabel('Layer Index')
            axes[0,0].set_ylabel('Fisher Score')
        
        # 2. æ¢¯åº¦èŒƒæ•°å¯è§†åŒ–
        gradient_scores = analysis_results.get('gradient_norms', {})
        if gradient_scores:
            layers = sorted(gradient_scores.keys())
            scores = [gradient_scores[layer] for layer in layers]
            axes[0,1].bar(layers, scores, alpha=0.7, color='green')
            axes[0,1].set_title('Gradient Norms')
            axes[0,1].set_xlabel('Layer Index')
            axes[0,1].set_ylabel('Gradient Norm')
        
        # 3. æ¿€æ´»æ–¹å·®å¯è§†åŒ–
        activation_stats = analysis_results.get('activation_statistics', {})
        if activation_stats:
            layers = sorted(activation_stats.keys())
            variances = [activation_stats[layer]['avg_variance'] for layer in layers]
            axes[1,0].bar(layers, variances, alpha=0.7, color='red')
            axes[1,0].set_title('Activation Variance')
            axes[1,0].set_xlabel('Layer Index')
            axes[1,0].set_ylabel('Average Variance')
        
        # 4. ç»¼åˆé‡è¦æ€§è¯„åˆ†
        combined_scores = analysis_results.get('combined_importance', {})
        if combined_scores:
            layers = sorted(combined_scores.keys())
            scores = [combined_scores[layer] for layer in layers]
            bars = axes[1,1].bar(layers, scores, alpha=0.7, color='purple')
            axes[1,1].set_title('Combined Importance Scores')
            axes[1,1].set_xlabel('Layer Index')
            axes[1,1].set_ylabel('Combined Score')
            
            # é«˜äº®é‡è¦å±‚
            top_8_layers = self.select_important_layers(combined_scores, 8)
            for i, layer in enumerate(layers):
                if layer in top_8_layers:
                    bars[i].set_color('orange')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        viz_file = output_dir / 'layer_importance_analysis.png'
        plt.savefig(viz_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"å¯è§†åŒ–å·²ä¿å­˜: {viz_file}")
        return viz_file

def create_synthetic_data_loader(batch_size=8, seq_length=128, vocab_size=50000, num_batches=50):
    """åˆ›å»ºåˆæˆæ•°æ®åŠ è½½å™¨ç”¨äºæµ‹è¯•"""
    logger.info(f"åˆ›å»ºåˆæˆæ•°æ®åŠ è½½å™¨: {num_batches} æ‰¹æ¬¡")
    
    class SyntheticDataset:
        def __init__(self, num_batches, batch_size, seq_length, vocab_size):
            self.num_batches = num_batches
            self.batch_size = batch_size
            self.seq_length = seq_length
            self.vocab_size = vocab_size
            
        def __iter__(self):
            for _ in range(self.num_batches):
                # ç”Ÿæˆéšæœºè¾“å…¥åºåˆ—
                input_ids = torch.randint(0, self.vocab_size, (self.batch_size, self.seq_length))
                # ç”Ÿæˆæ ‡ç­¾ï¼ˆç®€å•åœ°ä½¿ç”¨è¾“å…¥çš„ä¸‹ä¸€ä¸ªtokenï¼‰
                labels = torch.cat([input_ids[:, 1:], input_ids[:, :1]], dim=1)
                
                yield {
                    'input_ids': input_ids,
                    'labels': labels
                }
    
    return SyntheticDataset(num_batches, batch_size, seq_length, vocab_size)

def main():
    """ä¸»å‡½æ•° - è¿è¡Œå±‚é‡è¦æ€§åˆ†æ"""
    logger.info("ğŸš€ å¼€å§‹çœŸå®å±‚é‡è¦æ€§åˆ†æå®éªŒ")
    
    # 1. åˆ›å»ºæ¨¡å‹ï¼ˆè¿™é‡Œä½¿ç”¨ä¹‹å‰çš„æ¨¡æ‹Ÿæ¨¡å‹ï¼‰
    from real_compact_model_builder import CompactTransformerBuilder
    
    builder = CompactTransformerBuilder()
    model = builder.load_original_model()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {device}")
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    data_loader = create_synthetic_data_loader(
        batch_size=4,
        seq_length=64,
        vocab_size=50000,
        num_batches=100
    )
    
    # 3. åˆå§‹åŒ–åˆ†æå™¨
    analyzer = LayerImportanceIntegrator(model, device)
    
    # 4. è¿è¡Œç»¼åˆåˆ†æ
    analysis_results = analyzer.comprehensive_layer_analysis(
        data_loader, 
        max_samples=500
    )
    
    # 5. é€‰æ‹©é‡è¦å±‚
    combined_scores = analysis_results['combined_importance']
    
    # ä½¿ç”¨ä¸¤ç§æ–¹æ³•é€‰æ‹©
    top_k_layers = analyzer.select_important_layers(
        combined_scores, target_count=8, method='top_k'
    )
    
    distributed_layers = analyzer.select_important_layers(
        combined_scores, target_count=8, method='distributed_selection'
    )
    
    # 6. åˆ›å»ºå¯è§†åŒ–
    output_dir = Path("results/layer_importance_analysis")
    viz_file = analyzer.create_analysis_visualization(analysis_results, output_dir)
    
    # 7. ä¿å­˜è¯¦ç»†ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    final_results = {
        'experiment_info': {
            'timestamp': timestamp,
            'total_layers': len(model.layers),
            'analysis_methods': ['fisher_information', 'gradient_norms', 'activation_statistics', 'shap_values'],
            'device': str(device)
        },
        'analysis_results': analysis_results,
        'layer_selection': {
            'top_k_method': top_k_layers,
            'distributed_method': distributed_layers
        },
        'layer_importance_ranking': sorted(
            combined_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
    }
    
    # ä¿å­˜JSONç»“æœ
    results_file = output_dir / f"layer_importance_analysis_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
    create_analysis_report(final_results, output_dir / f"analysis_report_{timestamp}.md")
    
    logger.info("ğŸ‰ å±‚é‡è¦æ€§åˆ†æå®Œæˆ!")
    logger.info(f"Top-Ké€‰æ‹©å±‚: {top_k_layers}")
    logger.info(f"åˆ†å¸ƒå¼é€‰æ‹©å±‚: {distributed_layers}")
    logger.info(f"ç»“æœä¿å­˜è‡³: {results_file}")
    
    return final_results, top_k_layers, distributed_layers

def create_analysis_report(results, output_file):
    """åˆ›å»ºåˆ†ææŠ¥å‘Š"""
    timestamp = results['experiment_info']['timestamp']
    total_layers = results['experiment_info']['total_layers']
    
    # è·å–å‰10ä¸ªæœ€é‡è¦çš„å±‚
    top_layers = results['layer_importance_ranking'][:10]
    
    analysis_results = results['analysis_results']
    
    report_content = f"""# Transformerå±‚é‡è¦æ€§åˆ†ææŠ¥å‘Š

## å®éªŒæ¦‚è§ˆ

- **åˆ†ææ—¶é—´**: {timestamp}
- **æ€»å±‚æ•°**: {total_layers}
- **åˆ†ææ–¹æ³•**: Fisherä¿¡æ¯çŸ©é˜µ, æ¢¯åº¦èŒƒæ•°, æ¿€æ´»ç»Ÿè®¡, SHAPå€¼
- **è®¾å¤‡**: {results['experiment_info']['device']}

## é‡è¦æ€§æ’å (Top 10)

| æ’å | å±‚ç´¢å¼• | é‡è¦æ€§è¯„åˆ† |
|------|--------|------------|
"""
    
    for rank, (layer, score) in enumerate(top_layers, 1):
        report_content += f"| {rank} | {layer} | {score:.4f} |\n"
    
    report_content += f"""

## å±‚é€‰æ‹©ç»“æœ

### Top-Kæ–¹æ³•é€‰æ‹©çš„å±‚
{results['layer_selection']['top_k_method']}

### åˆ†å¸ƒå¼æ–¹æ³•é€‰æ‹©çš„å±‚  
{results['layer_selection']['distributed_method']}

## åˆ†ææ–¹æ³•è¯¦æƒ…

### Fisherä¿¡æ¯çŸ©é˜µ
- **æ£€æµ‹å±‚æ•°**: {len(analysis_results.get('fisher_information', {}))}
- **æœ€é«˜åˆ†å±‚**: {max(analysis_results.get('fisher_information', {}).items(), key=lambda x: x[1], default=('N/A', 0))[0]}

### æ¢¯åº¦èŒƒæ•°åˆ†æ
- **æ£€æµ‹å±‚æ•°**: {len(analysis_results.get('gradient_norms', {}))}
- **æœ€é«˜åˆ†å±‚**: {max(analysis_results.get('gradient_norms', {}).items(), key=lambda x: x[1], default=('N/A', 0))[0]}

### æ¿€æ´»ç»Ÿè®¡åˆ†æ
- **æ£€æµ‹å±‚æ•°**: {len(analysis_results.get('activation_statistics', {}))}

### SHAPå€¼åˆ†æ
- **æ£€æµ‹å±‚æ•°**: {len(analysis_results.get('shap_values', {}))}
- **çŠ¶æ€**: {'å·²å®Œæˆ' if analysis_results.get('shap_values') else 'è·³è¿‡/å¤±è´¥'}

## ç»“è®º

åŸºäºå¤šç»´åº¦çš„å±‚é‡è¦æ€§åˆ†æï¼Œæˆ‘ä»¬æˆåŠŸè¯†åˆ«äº†Transformeræ¨¡å‹ä¸­çš„å…³é”®å±‚ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼š

1. **é«˜é‡è¦æ€§å±‚é›†ä¸­åŒºåŸŸ**: é€šè¿‡ç»¼åˆè¯„åˆ†è¯†åˆ«å‡ºäº†æœ€å…³é”®çš„å±‚
2. **åˆ†ææ–¹æ³•äº’è¡¥æ€§**: ä¸åŒåˆ†ææ–¹æ³•æä¾›äº†å±‚é‡è¦æ€§çš„ä¸åŒè§†è§’
3. **å±‚é€‰æ‹©ç­–ç•¥**: æä¾›äº†Top-Kå’Œåˆ†å¸ƒå¼ä¸¤ç§é€‰æ‹©ç­–ç•¥

è¿™äº›ç»“æœå¯ç”¨äºæ„å»ºé«˜æ•ˆçš„ç´§å‡‘æ¨¡å‹ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—å‡å°‘è®¡ç®—å¼€é”€ã€‚
"""
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report_content)

if __name__ == "__main__":
    results, top_k_layers, distributed_layers = main()
