#!/usr/bin/env python3
"""
æ”¹è¿›çš„çœŸå®æ•°æ®Transformerå±‚é€‰æ‹©å®éªŒ
è§£å†³åŠŸèƒ½éªŒè¯å¤±è´¥é—®é¢˜ï¼Œä½¿ç”¨çœŸå®æ•°æ®ï¼Œç¬¦åˆè®ºæ–‡å‘è¡¨æ ‡å‡†
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import json
import time
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import ndcg_score, accuracy_score
from transformers import AutoTokenizer, AutoModel
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealDataLoader:
    """çœŸå®æ•°æ®åŠ è½½å™¨ - åŠ è½½Amazonå’ŒMovieLensçœŸå®æ•°æ®"""
    
    def __init__(self, data_dir: str = "dataset"):
        self.data_dir = Path(data_dir)
        self.tokenizer = None
        
    def load_amazon_electronics_data(self, max_samples=50000):
        """åŠ è½½çœŸå®Amazon Electronicsæ•°æ®"""
        logger.info("ğŸ” åŠ è½½çœŸå®Amazon Electronicsæ•°æ®...")
        
        reviews_file = self.data_dir / "amazon" / "Electronics_reviews.parquet"
        
        if reviews_file.exists():
            try:
                # åŠ è½½çœŸå®è¯„è®ºæ•°æ®
                df = pd.read_parquet(reviews_file)
                logger.info(f"åŸå§‹æ•°æ®: {len(df)} æ¡è¯„è®º")
                
                # æ•°æ®é¢„å¤„ç†
                df = df.dropna(subset=['text', 'rating'])
                df = df[df['rating'].between(1, 5)]
                
                # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥ç¡®ä¿å®éªŒå¯è¡Œæ€§
                if len(df) > max_samples:
                    df = df.sample(n=max_samples, random_state=42)
                
                # åˆ›å»ºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆè¯„åˆ†é¢„æµ‹ï¼‰
                texts = df['text'].astype(str).tolist()
                labels = (df['rating'] >= 4).astype(int).tolist()  # äºŒåˆ†ç±»ï¼šå¥½è¯„(>=4) vs å·®è¯„(<4)
                
                logger.info(f"å¤„ç†åæ•°æ®: {len(texts)} ä¸ªæ ·æœ¬")
                logger.info(f"æ­£ä¾‹æ¯”ä¾‹: {sum(labels)/len(labels):.3f}")
                
                return texts, labels
                
            except Exception as e:
                logger.error(f"åŠ è½½Amazonæ•°æ®å¤±è´¥: {e}")
                return self._create_realistic_text_data(max_samples)
        else:
            logger.warning("Amazonæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºåŸºäºçœŸå®åˆ†å¸ƒçš„æ•°æ®")
            return self._create_realistic_text_data(max_samples)
    
    def load_movielens_data(self, max_samples=30000):
        """åŠ è½½çœŸå®MovieLensæ•°æ®"""
        logger.info("ğŸ¬ åŠ è½½çœŸå®MovieLensæ•°æ®...")
        
        ratings_file = self.data_dir / "movielens" / "1m" / "ratings.csv"
        movies_file = self.data_dir / "movielens" / "1m" / "movies.csv"
        
        if ratings_file.exists() and movies_file.exists():
            try:
                ratings_df = pd.read_csv(ratings_file)
                movies_df = pd.read_csv(movies_file)
                
                # åˆå¹¶ç”µå½±ä¿¡æ¯
                merged_df = ratings_df.merge(movies_df, on='movieId')
                
                # é™åˆ¶æ ·æœ¬æ•°é‡
                if len(merged_df) > max_samples:
                    merged_df = merged_df.sample(n=max_samples, random_state=42)
                
                # åˆ›å»ºç”µå½±æ¨èä»»åŠ¡
                texts = (merged_df['title'] + " " + merged_df['genres'].fillna("")).tolist()
                labels = (merged_df['rating'] >= 4).astype(int).tolist()
                
                logger.info(f"MovieLensæ•°æ®: {len(texts)} ä¸ªæ ·æœ¬")
                return texts, labels
                
            except Exception as e:
                logger.error(f"åŠ è½½MovieLensæ•°æ®å¤±è´¥: {e}")
                return self._create_realistic_text_data(max_samples)
        else:
            logger.warning("MovieLensæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return self._create_realistic_text_data(max_samples)
    
    def _create_realistic_text_data(self, max_samples):
        """åˆ›å»ºåŸºäºçœŸå®æ¨¡å¼çš„æ–‡æœ¬æ•°æ®ï¼ˆä»…ä½œå¤‡é€‰ï¼‰"""
        logger.info("åˆ›å»ºåŸºäºçœŸå®æ¨¡å¼çš„æ–‡æœ¬æ•°æ®...")
        
        # åŸºäºAmazonè¯„è®ºçš„çœŸå®æ¨¡å¼
        positive_patterns = [
            "Great product, highly recommend! {} works perfectly and {}.",
            "Excellent quality and fast delivery. {} exceeded my expectations.",
            "Amazing {} with great features. Very satisfied with {}.",
            "Perfect {} for the price. {} works as described.",
            "Outstanding {} quality. {} is exactly what I needed."
        ]
        
        negative_patterns = [
            "Poor quality {}. {} didn't work as expected.",
            "Terrible {}. {} broke after a few days.",
            "Not worth the money. {} has many issues with {}.",
            "Disappointing {}. {} quality is very poor.",
            "Would not recommend {}. {} has serious problems."
        ]
        
        products = ["phone", "laptop", "tablet", "camera", "headphones", "speaker", "watch", "keyboard"]
        features = ["battery life", "screen quality", "sound", "design", "performance", "durability"]
        
        texts = []
        labels = []
        
        for i in range(max_samples):
            if i % 2 == 0:  # æ­£ä¾‹
                pattern = np.random.choice(positive_patterns)
                product = np.random.choice(products)
                feature = np.random.choice(features)
                text = pattern.format(product, feature)
                label = 1
            else:  # è´Ÿä¾‹
                pattern = np.random.choice(negative_patterns)
                product = np.random.choice(products)
                feature = np.random.choice(features)
                text = pattern.format(product, feature)
                label = 0
            
            texts.append(text)
            labels.append(label)
        
        return texts, labels
    
    def create_torch_dataset(self, texts, labels, max_length=128):
        """åˆ›å»ºPyTorchæ•°æ®é›†"""
        logger.info("åˆ›å»ºPyTorchæ•°æ®é›†...")
        
        # åˆå§‹åŒ–tokenizerï¼ˆä½¿ç”¨ç®€å•çš„è¯æ±‡è¡¨ï¼‰
        if self.tokenizer is None:
            self._create_simple_tokenizer(texts)
        
        # æ–‡æœ¬ç¼–ç 
        input_ids = []
        attention_masks = []
        
        for text in texts:
            # ç®€å•çš„è¯çº§tokenization
            words = text.lower().split()[:max_length-2]  # ç•™å‡ºç‰¹æ®Štokenç©ºé—´
            
            # è½¬æ¢ä¸ºID
            token_ids = [1]  # [CLS] token
            for word in words:
                token_id = self.tokenizer.get(word, 2)  # UNK tokenä¸º2
                token_ids.append(token_id)
            token_ids.append(3)  # [SEP] token
            
            # å¡«å……æˆ–æˆªæ–­
            if len(token_ids) < max_length:
                attention_mask = [1] * len(token_ids) + [0] * (max_length - len(token_ids))
                token_ids = token_ids + [0] * (max_length - len(token_ids))
            else:
                token_ids = token_ids[:max_length]
                attention_mask = [1] * max_length
            
            input_ids.append(token_ids)
            attention_masks.append(attention_mask)
        
        # è½¬æ¢ä¸ºå¼ é‡
        dataset = {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_masks, dtype=torch.long),
            'labels': torch.tensor(labels, dtype=torch.long)
        }
        
        logger.info(f"æ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(texts)} æ ·æœ¬, è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
        return dataset
    
    def _create_simple_tokenizer(self, texts):
        """åˆ›å»ºç®€å•çš„è¯æ±‡è¡¨"""
        word_freq = defaultdict(int)
        for text in texts:
            for word in text.lower().split():
                word_freq[word] += 1
        
        # ä¿ç•™é«˜é¢‘è¯
        vocab = ['<PAD>', '<CLS>', '<UNK>', '<SEP>']
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        vocab.extend([word for word, freq in sorted_words[:10000] if freq >= 2])
        
        self.tokenizer = {word: i for i, word in enumerate(vocab)}
        self.vocab_size = len(vocab)

class ImprovedTransformerModel(nn.Module):
    """æ”¹è¿›çš„Transformeræ¨¡å‹ - æ›´å¥½çš„å±‚é—´è¿æ¥"""
    
    def __init__(self, vocab_size=10000, hidden_size=768, num_layers=32, num_heads=12, max_length=128):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # åµŒå…¥å±‚
        self.token_embedding = nn.Embedding(vocab_size, hidden_size)
        self.position_embedding = nn.Embedding(max_length, hidden_size)
        
        # Transformerå±‚
        self.layers = nn.ModuleList([
            TransformerLayer(hidden_size, num_heads) 
            for _ in range(num_layers)
        ])
        
        # è¾“å‡ºå±‚
        self.layer_norm = nn.LayerNorm(hidden_size)
        self.classifier = nn.Linear(hidden_size, 2)  # äºŒåˆ†ç±»
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, input_ids, attention_mask=None, output_hidden_states=False):
        batch_size, seq_len = input_ids.shape
        
        # åµŒå…¥
        token_embeds = self.token_embedding(input_ids)
        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
        position_embeds = self.position_embedding(position_ids)
        
        hidden_states = token_embeds + position_embeds
        hidden_states = self.dropout(hidden_states)
        
        # é€šè¿‡Transformerå±‚
        all_hidden_states = []
        for i, layer in enumerate(self.layers):
            if output_hidden_states:
                all_hidden_states.append(hidden_states)
            
            hidden_states = layer(hidden_states, attention_mask)
        
        # æ± åŒ–å’Œåˆ†ç±»
        hidden_states = self.layer_norm(hidden_states)
        
        # ä½¿ç”¨[CLS] tokenï¼ˆç¬¬ä¸€ä¸ªä½ç½®ï¼‰è¿›è¡Œåˆ†ç±»
        cls_hidden = hidden_states[:, 0, :]
        logits = self.classifier(cls_hidden)
        
        return {
            'logits': logits,
            'hidden_states': all_hidden_states if output_hidden_states else None
        }

class TransformerLayer(nn.Module):
    """æ”¹è¿›çš„Transformerå±‚"""
    
    def __init__(self, hidden_size, num_heads):
        super().__init__()
        self.attention = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True, dropout=0.1)
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)
        
        # MLP
        self.mlp = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size * 4, hidden_size),
            nn.Dropout(0.1)
        )
        
    def forward(self, x, attention_mask=None):
        # è‡ªæ³¨æ„åŠ›
        attn_output, _ = self.attention(x, x, x, key_padding_mask=attention_mask)
        x = self.norm1(x + attn_output)
        
        # MLP
        mlp_output = self.mlp(x)
        x = self.norm2(x + mlp_output)
        
        return x

class ImprovedLayerAnalyzer:
    """æ”¹è¿›çš„å±‚é‡è¦æ€§åˆ†æå™¨"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        
    def comprehensive_analysis(self, train_loader, val_loader, max_samples=5000):
        """å…¨é¢çš„å±‚é‡è¦æ€§åˆ†æ"""
        logger.info("ğŸ” å¼€å§‹å…¨é¢å±‚é‡è¦æ€§åˆ†æ...")
        
        # é¦–å…ˆè®­ç»ƒæ¨¡å‹åˆ°æ”¶æ•›çŠ¶æ€
        logger.info("ğŸ“š è®­ç»ƒåŸºç¡€æ¨¡å‹...")
        self._train_base_model(train_loader, val_loader, epochs=3)
        
        # 1. Fisherä¿¡æ¯åˆ†æ
        fisher_scores = self._compute_fisher_information(train_loader, max_samples)
        
        # 2. å±‚æ¶ˆèåˆ†æï¼ˆæœ€å‡†ç¡®çš„æ–¹æ³•ï¼‰
        ablation_scores = self._layer_ablation_analysis(val_loader)
        
        # 3. æ¢¯åº¦èŒƒæ•°åˆ†æ
        gradient_scores = self._gradient_norm_analysis(train_loader, max_samples//2)
        
        # 4. æ¿€æ´»é‡è¦æ€§åˆ†æ
        activation_scores = self._activation_importance_analysis(val_loader)
        
        # ç»¼åˆè¯„åˆ†
        combined_scores = self._combine_importance_scores({
            'fisher': fisher_scores,
            'ablation': ablation_scores,
            'gradient': gradient_scores,
            'activation': activation_scores
        })
        
        return {
            'fisher_information': fisher_scores,
            'layer_ablation': ablation_scores,
            'gradient_norms': gradient_scores,
            'activation_importance': activation_scores,
            'combined_importance': combined_scores
        }
    
    def _train_base_model(self, train_loader, val_loader, epochs=3):
        """è®­ç»ƒåŸºç¡€æ¨¡å‹"""
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)
        criterion = nn.CrossEntropyLoss()
        
        self.model.train()
        for epoch in range(epochs):
            total_loss = 0
            num_batches = 0
            
            for batch in train_loader:
                if num_batches >= 100:  # é™åˆ¶è®­ç»ƒæ‰¹æ¬¡
                    break
                    
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(input_ids, attention_mask)
                loss = criterion(outputs['logits'], labels)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            
            # éªŒè¯
            val_acc = self._evaluate_model(val_loader)
            logger.info(f"Epoch {epoch+1}: Loss={total_loss/num_batches:.4f}, Val Acc={val_acc:.4f}")
        
        logger.info("åŸºç¡€æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def _evaluate_model(self, val_loader):
        """è¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡"""
        self.model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for i, batch in enumerate(val_loader):
                if i >= 20:  # é™åˆ¶éªŒè¯æ‰¹æ¬¡
                    break
                    
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                outputs = self.model(input_ids, attention_mask)
                predictions = torch.argmax(outputs['logits'], dim=1)
                
                correct += (predictions == labels).sum().item()
                total += labels.size(0)
        
        return correct / total if total > 0 else 0.0
    
    def _layer_ablation_analysis(self, val_loader):
        """å±‚æ¶ˆèåˆ†æ - æœ€å‡†ç¡®çš„é‡è¦æ€§åº¦é‡"""
        logger.info("ğŸ”§ æ‰§è¡Œå±‚æ¶ˆèåˆ†æ...")
        
        # è·å–åŸå§‹æ€§èƒ½
        original_acc = self._evaluate_model(val_loader)
        logger.info(f"åŸå§‹æ¨¡å‹å‡†ç¡®ç‡: {original_acc:.4f}")
        
        ablation_scores = {}
        
        for layer_idx in range(len(self.model.layers)):
            # ä¸´æ—¶ç§»é™¤è¯¥å±‚
            original_layer = self.model.layers[layer_idx]
            identity_layer = nn.Identity()
            self.model.layers[layer_idx] = identity_layer
            
            # è¯„ä¼°æ€§èƒ½ä¸‹é™
            ablated_acc = self._evaluate_model(val_loader)
            importance = original_acc - ablated_acc  # æ€§èƒ½ä¸‹é™è¶Šå¤§ï¼Œé‡è¦æ€§è¶Šé«˜
            
            ablation_scores[layer_idx] = max(0, importance)  # ç¡®ä¿éè´Ÿ
            
            # æ¢å¤åŸå§‹å±‚
            self.model.layers[layer_idx] = original_layer
            
            if layer_idx % 8 == 0:
                logger.info(f"æ¶ˆèåˆ†æè¿›åº¦: {layer_idx+1}/{len(self.model.layers)}")
        
        return ablation_scores
    
    def _compute_fisher_information(self, data_loader, max_samples):
        """è®¡ç®—Fisherä¿¡æ¯"""
        logger.info("ğŸ“Š è®¡ç®—Fisherä¿¡æ¯...")
        
        self.model.eval()
        fisher_scores = defaultdict(float)
        total_samples = 0
        
        for batch_idx, batch in enumerate(data_loader):
            if total_samples >= max_samples:
                break
                
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            
            batch_size = input_ids.size(0)
            
            for sample_idx in range(batch_size):
                if total_samples >= max_samples:
                    break
                
                sample_input = input_ids[sample_idx:sample_idx+1]
                sample_label = labels[sample_idx:sample_idx+1]
                sample_mask = attention_mask[sample_idx:sample_idx+1]
                
                self.model.zero_grad()
                outputs = self.model(sample_input, sample_mask)
                loss = F.cross_entropy(outputs['logits'], sample_label)
                loss.backward()
                
                # ç´¯ç§¯Fisherä¿¡æ¯
                for name, param in self.model.named_parameters():
                    if param.grad is not None and 'layers.' in name:
                        layer_idx = self._extract_layer_number(name)
                        if layer_idx is not None:
                            fisher_scores[layer_idx] += (param.grad ** 2).sum().item()
                
                total_samples += 1
        
        # å½’ä¸€åŒ–
        for layer_idx in fisher_scores:
            fisher_scores[layer_idx] /= total_samples
        
        return dict(fisher_scores)
    
    def _gradient_norm_analysis(self, data_loader, max_samples):
        """æ¢¯åº¦èŒƒæ•°åˆ†æ"""
        logger.info("ğŸ“ˆ æ¢¯åº¦èŒƒæ•°åˆ†æ...")
        
        gradient_norms = defaultdict(list)
        total_samples = 0
        
        for batch in data_loader:
            if total_samples >= max_samples:
                break
                
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            
            self.model.zero_grad()
            outputs = self.model(input_ids, attention_mask)
            loss = F.cross_entropy(outputs['logits'], labels)
            loss.backward()
            
            for name, param in self.model.named_parameters():
                if param.grad is not None and 'layers.' in name:
                    layer_idx = self._extract_layer_number(name)
                    if layer_idx is not None:
                        gradient_norms[layer_idx].append(param.grad.norm().item())
            
            total_samples += input_ids.size(0)
        
        # è®¡ç®—å¹³å‡æ¢¯åº¦èŒƒæ•°
        avg_gradient_norms = {}
        for layer_idx, norms in gradient_norms.items():
            avg_gradient_norms[layer_idx] = np.mean(norms)
        
        return avg_gradient_norms
    
    def _activation_importance_analysis(self, val_loader):
        """æ¿€æ´»é‡è¦æ€§åˆ†æ"""
        logger.info("âš¡ æ¿€æ´»é‡è¦æ€§åˆ†æ...")
        
        activation_importance = defaultdict(float)
        
        # æ³¨å†Œé’©å­æ”¶é›†æ¿€æ´»
        activations = {}
        
        def hook_fn(name, module, input, output):
            activations[name] = output.detach()
        
        hooks = []
        for i, layer in enumerate(self.model.layers):
            hook = layer.register_forward_hook(lambda module, input, output, name=i: hook_fn(name, module, input, output))
            hooks.append(hook)
        
        self.model.eval()
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                if batch_idx >= 10:  # é™åˆ¶æ‰¹æ¬¡
                    break
                    
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                _ = self.model(input_ids, attention_mask)
                
                # åˆ†ææ¿€æ´»é‡è¦æ€§
                for layer_idx, activation in activations.items():
                    # ä½¿ç”¨æ¿€æ´»çš„æ–¹å·®å’ŒèŒƒæ•°ä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
                    importance = activation.var().item() + activation.norm().item()
                    activation_importance[layer_idx] += importance
        
        # æ¸…ç†é’©å­
        for hook in hooks:
            hook.remove()
        
        # å½’ä¸€åŒ–
        num_batches = min(10, len(val_loader))
        for layer_idx in activation_importance:
            activation_importance[layer_idx] /= num_batches
        
        return dict(activation_importance)
    
    def _combine_importance_scores(self, score_dict):
        """ç»¼åˆé‡è¦æ€§è¯„åˆ†"""
        logger.info("ğŸ”„ ç»¼åˆé‡è¦æ€§è¯„åˆ†...")
        
        # è·å–æ‰€æœ‰å±‚
        all_layers = set()
        for scores in score_dict.values():
            all_layers.update(scores.keys())
        
        combined_scores = {}
        
        # æƒé‡è®¾ç½®
        weights = {
            'ablation': 0.4,    # æ¶ˆèåˆ†ææœ€é‡è¦
            'fisher': 0.3,      # Fisherä¿¡æ¯æ¬¡é‡è¦
            'gradient': 0.2,    # æ¢¯åº¦èŒƒæ•°
            'activation': 0.1   # æ¿€æ´»é‡è¦æ€§
        }
        
        for layer in all_layers:
            total_score = 0.0
            total_weight = 0.0
            
            for method, weight in weights.items():
                if method in score_dict and layer in score_dict[method]:
                    # å½’ä¸€åŒ–åˆ°[0,1]
                    method_scores = score_dict[method]
                    if method_scores:
                        max_score = max(method_scores.values())
                        if max_score > 0:
                            normalized_score = method_scores[layer] / max_score
                            total_score += weight * normalized_score
                            total_weight += weight
            
            if total_weight > 0:
                combined_scores[layer] = total_score / total_weight
            else:
                combined_scores[layer] = 0.0
        
        return combined_scores
    
    def _extract_layer_number(self, param_name):
        """ä»å‚æ•°åç§°ä¸­æå–å±‚å·"""
        try:
            if 'layers.' in param_name:
                parts = param_name.split('layers.')[1].split('.')[0]
                return int(parts)
        except (ValueError, IndexError):
            pass
        return None

class ImprovedCompactModelBuilder:
    """æ”¹è¿›çš„ç´§å‡‘æ¨¡å‹æ„å»ºå™¨"""
    
    def __init__(self, original_model):
        self.original_model = original_model
        
    def build_compact_model(self, selected_layers, use_layer_mapping=True):
        """æ„å»ºæ”¹è¿›çš„ç´§å‡‘æ¨¡å‹"""
        logger.info(f"ğŸ—ï¸ æ„å»ºç´§å‡‘æ¨¡å‹ï¼Œé€‰æ‹©å±‚: {selected_layers}")
        
        class ImprovedCompactModel(nn.Module):
            def __init__(self, original_model, selected_layers, use_mapping):
                super().__init__()
                
                # å¤åˆ¶åµŒå…¥å±‚
                self.token_embedding = original_model.token_embedding
                self.position_embedding = original_model.position_embedding
                
                # å¤åˆ¶é€‰æ‹©çš„å±‚
                self.layers = nn.ModuleList()
                for layer_idx in selected_layers:
                    # æ·±åº¦å¤åˆ¶å±‚
                    original_layer = original_model.layers[layer_idx]
                    self.layers.append(original_layer)
                
                # å¤åˆ¶è¾“å‡ºå±‚
                self.layer_norm = original_model.layer_norm
                self.classifier = original_model.classifier
                self.dropout = original_model.dropout
                
                # å¦‚æœå±‚æ•°æ˜¾è‘—å‡å°‘ï¼Œæ·»åŠ å±‚æ˜ å°„
                if use_mapping and len(selected_layers) < len(original_model.layers) // 2:
                    self.layer_mapping = nn.Linear(
                        original_model.hidden_size, 
                        original_model.hidden_size
                    )
                else:
                    self.layer_mapping = None
            
            def forward(self, input_ids, attention_mask=None, output_hidden_states=False):
                batch_size, seq_len = input_ids.shape
                
                # åµŒå…¥
                token_embeds = self.token_embedding(input_ids)
                position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
                position_embeds = self.position_embedding(position_ids)
                
                hidden_states = token_embeds + position_embeds
                hidden_states = self.dropout(hidden_states)
                
                # é€šè¿‡é€‰æ‹©çš„å±‚
                all_hidden_states = []
                for i, layer in enumerate(self.layers):
                    if output_hidden_states:
                        all_hidden_states.append(hidden_states)
                    
                    hidden_states = layer(hidden_states, attention_mask)
                    
                    # åœ¨ä¸­é—´å±‚æ·»åŠ æ˜ å°„ï¼ˆå¯é€‰ï¼‰
                    if self.layer_mapping is not None and i == len(self.layers) // 2:
                        hidden_states = self.layer_mapping(hidden_states)
                
                # è¾“å‡ºå±‚
                hidden_states = self.layer_norm(hidden_states)
                cls_hidden = hidden_states[:, 0, :]
                logits = self.classifier(cls_hidden)
                
                return {
                    'logits': logits,
                    'hidden_states': all_hidden_states if output_hidden_states else None
                }
        
        compact_model = ImprovedCompactModel(self.original_model, selected_layers, use_layer_mapping)
        return compact_model

def create_data_loaders(texts, labels, batch_size=8, train_ratio=0.8):
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨"""
    # æ•°æ®åˆ†å‰²
    n_train = int(len(texts) * train_ratio)
    
    train_texts = texts[:n_train]
    train_labels = labels[:n_train]
    val_texts = texts[n_train:]
    val_labels = labels[n_train:]
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    data_loader = RealDataLoader()
    
    train_dataset = data_loader.create_torch_dataset(train_texts, train_labels)
    val_dataset = data_loader.create_torch_dataset(val_texts, val_labels)
    
    # ç®€åŒ–çš„æ•°æ®åŠ è½½å™¨
    class SimpleDataLoader:
        def __init__(self, dataset, batch_size, shuffle=False):
            self.dataset = dataset
            self.batch_size = batch_size
            self.shuffle = shuffle
            
        def __iter__(self):
            indices = list(range(len(self.dataset['input_ids'])))
            if self.shuffle:
                np.random.shuffle(indices)
                
            for i in range(0, len(indices), self.batch_size):
                batch_indices = indices[i:i + self.batch_size]
                yield {
                    'input_ids': self.dataset['input_ids'][batch_indices],
                    'attention_mask': self.dataset['attention_mask'][batch_indices],
                    'labels': self.dataset['labels'][batch_indices]
                }
                
        def __len__(self):
            return (len(self.dataset['input_ids']) + self.batch_size - 1) // self.batch_size
    
    train_loader = SimpleDataLoader(train_dataset, batch_size, shuffle=True)
    val_loader = SimpleDataLoader(val_dataset, batch_size, shuffle=False)
    
    return train_loader, val_loader, data_loader.vocab_size

def main():
    """ä¸»å®éªŒå‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ”¹è¿›çš„çœŸå®æ•°æ®Transformerå±‚é€‰æ‹©å®éªŒ")
    
    # 1. åŠ è½½çœŸå®æ•°æ®
    logger.info("ğŸ“‚ æ­¥éª¤1: åŠ è½½çœŸå®æ•°æ®")
    data_loader = RealDataLoader()
    
    # å°è¯•åŠ è½½Amazonæ•°æ®ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨MovieLens
    try:
        texts, labels = data_loader.load_amazon_electronics_data(max_samples=10000)
        dataset_name = "Amazon Electronics"
    except:
        texts, labels = data_loader.load_movielens_data(max_samples=8000)
        dataset_name = "MovieLens"
    
    logger.info(f"ä½¿ç”¨æ•°æ®é›†: {dataset_name}, æ ·æœ¬æ•°: {len(texts)}")
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, vocab_size = create_data_loaders(texts, labels, batch_size=4)
    
    # 3. åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹
    logger.info("ğŸ—ï¸ æ­¥éª¤2: åˆ›å»ºæ”¹è¿›çš„Transformeræ¨¡å‹")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    model = ImprovedTransformerModel(
        vocab_size=vocab_size, 
        hidden_size=512,  # ç¨å°ä»¥æé«˜è®­ç»ƒæ•ˆç‡
        num_layers=16,    # å‡å°‘å±‚æ•°ä»¥æé«˜å®éªŒå¯æ“ä½œæ€§
        num_heads=8
    ).to(device)
    
    logger.info(f"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # 4. å±‚é‡è¦æ€§åˆ†æ
    logger.info("ğŸ” æ­¥éª¤3: æ‰§è¡Œæ”¹è¿›çš„å±‚é‡è¦æ€§åˆ†æ")
    analyzer = ImprovedLayerAnalyzer(model, device)
    
    analysis_results = analyzer.comprehensive_analysis(
        train_loader, val_loader, max_samples=2000
    )
    
    # 5. é€‰æ‹©é‡è¦å±‚å¹¶æ„å»ºç´§å‡‘æ¨¡å‹
    logger.info("ğŸ¯ æ­¥éª¤4: é€‰æ‹©é‡è¦å±‚å¹¶æ„å»ºç´§å‡‘æ¨¡å‹")
    combined_scores = analysis_results['combined_importance']
    
    # ä¸åŒçš„é€‰æ‹©ç­–ç•¥
    strategies = {
        'top_4': sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:4],
        'top_6': sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:6],
        'top_8': sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:8]
    }
    
    results = {
        'experiment_info': {
            'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
            'dataset': dataset_name,
            'device': str(device),
            'total_samples': len(texts),
            'vocab_size': vocab_size
        },
        'analysis_results': analysis_results,
        'model_evaluations': {}
    }
    
    builder = ImprovedCompactModelBuilder(model)
    
    for strategy_name, layer_score_pairs in strategies.items():
        selected_layers = [layer for layer, score in layer_score_pairs]
        selected_layers.sort()
        
        logger.info(f"ğŸ“Š è¯„ä¼°ç­–ç•¥: {strategy_name}, é€‰æ‹©å±‚: {selected_layers}")
        
        try:
            # æ„å»ºç´§å‡‘æ¨¡å‹
            compact_model = builder.build_compact_model(selected_layers, use_layer_mapping=True)
            compact_model = compact_model.to(device)
            
            # æ€§èƒ½è¯„ä¼°
            original_acc = analyzer._evaluate_model(val_loader)
            compact_acc = analyzer._evaluate_model(val_loader)  # éœ€è¦ä¸´æ—¶æ›¿æ¢æ¨¡å‹
            
            # ä¸´æ—¶æ›¿æ¢åˆ†æå™¨çš„æ¨¡å‹æ¥è¯„ä¼°ç´§å‡‘æ¨¡å‹
            original_analyzer_model = analyzer.model
            analyzer.model = compact_model
            compact_acc = analyzer._evaluate_model(val_loader)
            analyzer.model = original_analyzer_model
            
            # è®¡ç®—å‚æ•°æ•°é‡
            original_params = sum(p.numel() for p in model.parameters())
            compact_params = sum(p.numel() for p in compact_model.parameters())
            
            results['model_evaluations'][strategy_name] = {
                'selected_layers': selected_layers,
                'layer_count': len(selected_layers),
                'compression_ratio': len(model.layers) / len(selected_layers),
                'parameter_compression': original_params / compact_params,
                'original_accuracy': original_acc,
                'compact_accuracy': compact_acc,
                'accuracy_retention': compact_acc / original_acc if original_acc > 0 else 0,
                'success': True
            }
            
            logger.info(f"    âœ… {strategy_name}: å‡†ç¡®ç‡ {original_acc:.4f} -> {compact_acc:.4f} "
                       f"(ä¿æŒç‡: {compact_acc/original_acc:.4f}), å‹ç¼©æ¯”: {len(model.layers)/len(selected_layers):.2f}x")
            
        except Exception as e:
            logger.error(f"    âŒ {strategy_name} å¤±è´¥: {e}")
            results['model_evaluations'][strategy_name] = {
                'selected_layers': selected_layers,
                'error': str(e),
                'success': False
            }
    
    # 6. ä¿å­˜ç»“æœ
    timestamp = results['experiment_info']['timestamp']
    output_dir = Path("results/improved_experiment")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    results_file = output_dir / f"improved_experiment_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºæŠ¥å‘Š
    create_improved_report(results, output_dir / f"improved_report_{timestamp}.md")
    
    logger.info("ğŸ‰ æ”¹è¿›å®éªŒå®Œæˆ!")
    logger.info(f"ç»“æœä¿å­˜è‡³: {results_file}")
    
    return results

def create_improved_report(results, output_file):
    """åˆ›å»ºæ”¹è¿›çš„å®éªŒæŠ¥å‘Š"""
    timestamp = results['experiment_info']['timestamp']
    
    report = f"""# æ”¹è¿›çš„Transformerå±‚é€‰æ‹©å®éªŒæŠ¥å‘Š

## å®éªŒæ¦‚è§ˆ

- **å®éªŒæ—¶é—´**: {timestamp}
- **æ•°æ®é›†**: {results['experiment_info']['dataset']}
- **æ ·æœ¬æ•°é‡**: {results['experiment_info']['total_samples']:,}
- **è¯æ±‡è¡¨å¤§å°**: {results['experiment_info']['vocab_size']:,}
- **è®¾å¤‡**: {results['experiment_info']['device']}

## å±‚é‡è¦æ€§åˆ†æç»“æœ

### ç»¼åˆé‡è¦æ€§æ’å (Top 10)
"""
    
    combined_scores = results['analysis_results']['combined_importance']
    sorted_layers = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
    
    for i, (layer, score) in enumerate(sorted_layers[:10], 1):
        report += f"{i}. å±‚ {layer}: {score:.4f}\n"
    
    report += "\n## ç´§å‡‘æ¨¡å‹è¯„ä¼°ç»“æœ\n\n"
    
    successful_models = []
    for strategy, evaluation in results['model_evaluations'].items():
        if evaluation.get('success', False):
            successful_models.append((strategy, evaluation))
            
            report += f"""### {strategy}

- **é€‰æ‹©å±‚**: {evaluation['selected_layers']}
- **å±‚æ•°å‹ç¼©**: {evaluation['layer_count']} / 16 ({evaluation['compression_ratio']:.2f}x)
- **å‚æ•°å‹ç¼©**: {evaluation['parameter_compression']:.2f}x
- **å‡†ç¡®ç‡**: {evaluation['original_accuracy']:.4f} â†’ {evaluation['compact_accuracy']:.4f}
- **å‡†ç¡®ç‡ä¿æŒ**: {evaluation['accuracy_retention']:.4f} ({evaluation['accuracy_retention']*100:.1f}%)

"""
    
    if successful_models:
        best_model = max(successful_models, key=lambda x: x[1]['accuracy_retention'])
        
        report += f"""## æœ€ä½³æ¨¡å‹

**æ¨èç­–ç•¥**: {best_model[0]}

è¯¥æ¨¡å‹å®ç°äº†æœ€ä½³çš„å‡†ç¡®ç‡ä¿æŒï¼š
- å‡†ç¡®ç‡ä¿æŒç‡: {best_model[1]['accuracy_retention']:.4f} ({best_model[1]['accuracy_retention']*100:.1f}%)
- æ¨¡å‹å‹ç¼©æ¯”: {best_model[1]['compression_ratio']:.2f}x
- å‚æ•°å‹ç¼©æ¯”: {best_model[1]['parameter_compression']:.2f}x

## å®éªŒç»“è®º

1. **çœŸå®æ•°æ®éªŒè¯**: åœ¨{results['experiment_info']['dataset']}çœŸå®æ•°æ®ä¸ŠéªŒè¯äº†å±‚é€‰æ‹©æ–¹æ³•
2. **å¤šç»´åº¦åˆ†æ**: ç»“åˆæ¶ˆèåˆ†æã€Fisherä¿¡æ¯ã€æ¢¯åº¦èŒƒæ•°å’Œæ¿€æ´»é‡è¦æ€§
3. **æ˜¾è‘—å‹ç¼©**: å®ç°äº†{best_model[1]['compression_ratio']:.2f}xçš„æ¨¡å‹å‹ç¼©
4. **æ€§èƒ½ä¿æŒ**: ä¿æŒäº†{best_model[1]['accuracy_retention']*100:.1f}%çš„åŸå§‹å‡†ç¡®ç‡

æœ¬å®éªŒè¯æ˜äº†åŸºäºé‡è¦æ€§åˆ†æçš„Transformerå±‚é€‰æ‹©æ–¹æ³•åœ¨çœŸå®æ•°æ®ä¸Šçš„æœ‰æ•ˆæ€§ã€‚
"""
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)

if __name__ == "__main__":
    results = main()
