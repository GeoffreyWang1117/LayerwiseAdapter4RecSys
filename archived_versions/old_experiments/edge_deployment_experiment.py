#!/usr/bin/env python3
"""
Edge Device Deployment Experiment
è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²å®éªŒ

æœ¬å®éªŒéªŒè¯Fisher-LDåœ¨Jetson Orin Nanoä¸Šçš„å®é™…éƒ¨ç½²æ€§èƒ½
å¯¹åº”è®ºæ–‡Table 6: Edge deployment performance comparison
"""

import subprocess
import json
import time
import torch
import psutil
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EdgeDeploymentExperiment:
    """è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²å®éªŒç±»"""
    
    def __init__(self):
        self.jetson_ip = "100.111.167.60"
        self.jetson_user = "geoffrey"
        self.jetson_password = "926494"  # æ³¨æ„ï¼šå®é™…ä½¿ç”¨æ—¶åº”ä½¿ç”¨å¯†é’¥è®¤è¯
        
        self.results = {
            'experiment_name': 'Edge Device Deployment Validation',
            'timestamp': datetime.now().isoformat(),
            'local_hardware': self._get_local_hardware_info(),
            'edge_hardware': {},
            'performance_comparison': {},
            'deployment_status': 'initializing'
        }
        
    def _get_local_hardware_info(self) -> Dict:
        """è·å–æœ¬åœ°ç¡¬ä»¶ä¿¡æ¯"""
        try:
            # GPUä¿¡æ¯
            if torch.cuda.is_available():
                gpu_info = {
                    'gpu_count': torch.cuda.device_count(),
                    'gpu_name': torch.cuda.get_device_name(0),
                    'gpu_memory': torch.cuda.get_device_properties(0).total_memory // (1024**3)
                }
            else:
                gpu_info = {'gpu_available': False}
                
            # CPUå’Œå†…å­˜ä¿¡æ¯
            cpu_info = {
                'cpu_count': psutil.cpu_count(logical=True),
                'cpu_freq': psutil.cpu_freq().max if psutil.cpu_freq() else 'Unknown',
                'memory_total': psutil.virtual_memory().total // (1024**3)
            }
            
            return {'gpu': gpu_info, 'cpu': cpu_info}
        except Exception as e:
            logger.error(f"è·å–ç¡¬ä»¶ä¿¡æ¯å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def test_jetson_connection(self) -> bool:
        """æµ‹è¯•Jetsonè¿æ¥"""
        logger.info(f"æµ‹è¯•è¿æ¥åˆ°Jetson Orin Nano: {self.jetson_ip}")
        
        try:
            # ä½¿ç”¨sshpassè¿›è¡Œè‡ªåŠ¨å¯†ç è®¤è¯ï¼ˆéœ€è¦å®‰è£…sshpassï¼‰
            cmd = f'sshpass -p "{self.jetson_password}" ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 {self.jetson_user}@{self.jetson_ip} "echo Connected"'
            
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=15)
            
            if result.returncode == 0:
                logger.info("âœ… Jetsonè¿æ¥æˆåŠŸ")
                return True
            else:
                logger.error(f"âŒ Jetsonè¿æ¥å¤±è´¥: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error("âŒ è¿æ¥è¶…æ—¶")
            return False
        except Exception as e:
            logger.error(f"âŒ è¿æ¥å¼‚å¸¸: {e}")
            return False
    
    def get_jetson_hardware_info(self) -> Dict:
        """è·å–Jetsonç¡¬ä»¶ä¿¡æ¯"""
        logger.info("è·å–Jetsonç¡¬ä»¶ä¿¡æ¯...")
        
        commands = {
            'gpu_info': 'nvidia-smi --query-gpu=name,memory.total,power.max_limit --format=csv,noheader,nounits 2>/dev/null || echo "GPU info not available"',
            'cpu_info': 'nproc && cat /proc/cpuinfo | grep "model name" | head -1',
            'memory_info': 'free -h',
            'system_info': 'uname -a',
            'jetpack_version': 'cat /etc/nv_tegra_release 2>/dev/null || echo "JetPack version not found"'
        }
        
        hardware_info = {}
        
        for info_type, cmd in commands.items():
            try:
                full_cmd = f'sshpass -p "{self.jetson_password}" ssh -o StrictHostKeyChecking=no {self.jetson_user}@{self.jetson_ip} "{cmd}"'
                result = subprocess.run(full_cmd, shell=True, capture_output=True, text=True, timeout=10)
                
                if result.returncode == 0:
                    hardware_info[info_type] = result.stdout.strip()
                else:
                    hardware_info[info_type] = f"Error: {result.stderr.strip()}"
                    
            except Exception as e:
                hardware_info[info_type] = f"Exception: {str(e)}"
        
        return hardware_info
    
    def create_deployment_package(self) -> Path:
        """åˆ›å»ºéƒ¨ç½²åŒ…"""
        logger.info("åˆ›å»ºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²åŒ…...")
        
        # åˆ›å»ºè½»é‡çº§æ¨¡å‹å’Œæ¨ç†è„šæœ¬
        deployment_dir = Path("edge_deployment")
        deployment_dir.mkdir(exist_ok=True)
        
        # ç®€åŒ–çš„æ¨ç†è„šæœ¬
        inference_script = '''#!/usr/bin/env python3
"""
Lightweight Fisher-LD Inference Script for Edge Device
"""

import torch
import torch.nn as nn
import time
import json
import sys
from datetime import datetime

class SimplifiedStudentModel(nn.Module):
    """ç®€åŒ–çš„å­¦ç”Ÿæ¨¡å‹ç”¨äºè¾¹ç¼˜æ¨ç†"""
    
    def __init__(self, input_dim=768, hidden_dim=256, num_classes=10):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, num_classes)
        )
        
    def forward(self, x):
        return self.encoder(x)

def benchmark_inference(model, num_samples=100, input_dim=768):
    """æ¨ç†åŸºå‡†æµ‹è¯•"""
    model.eval()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    # é¢„çƒ­
    dummy_input = torch.randn(1, input_dim).to(device)
    for _ in range(10):
        with torch.no_grad():
            _ = model(dummy_input)
    
    # æ­£å¼æµ‹è¯•
    latencies = []
    
    for i in range(num_samples):
        input_data = torch.randn(1, input_dim).to(device)
        
        start_time = time.time()
        with torch.no_grad():
            output = model(input_data)
        end_time = time.time()
        
        latencies.append((end_time - start_time) * 1000)  # ms
    
    return {
        'avg_latency_ms': sum(latencies) / len(latencies),
        'min_latency_ms': min(latencies),
        'max_latency_ms': max(latencies),
        'total_samples': num_samples,
        'device': str(device)
    }

def main():
    print("ğŸš€ Fisher-LD Edge Inference Benchmark")
    
    try:
        # åˆ›å»ºæ¨¡å‹
        model = SimplifiedStudentModel()
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        results = benchmark_inference(model, num_samples=50)
        
        results['timestamp'] = datetime.now().isoformat()
        results['model_params'] = sum(p.numel() for p in model.parameters())
        
        # ä¿å­˜ç»“æœ
        with open('edge_inference_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print("âœ… åŸºå‡†æµ‹è¯•å®Œæˆ")
        print(f"å¹³å‡æ¨ç†æ—¶é—´: {results['avg_latency_ms']:.2f}ms")
        print(f"è®¾å¤‡: {results['device']}")
        
        return results
        
    except Exception as e:
        error_result = {
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }
        
        with open('edge_inference_error.json', 'w') as f:
            json.dump(error_result, f, indent=2)
        
        print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        return error_result

if __name__ == "__main__":
    main()
'''
        
        # ä¿å­˜æ¨ç†è„šæœ¬
        script_path = deployment_dir / "edge_inference.py"
        with open(script_path, 'w') as f:
            f.write(inference_script)
        
        # åˆ›å»ºrequirements.txt
        requirements = """torch>=1.9.0
numpy>=1.20.0
json
"""
        
        with open(deployment_dir / "requirements.txt", 'w') as f:
            f.write(requirements)
        
        logger.info(f"âœ… éƒ¨ç½²åŒ…åˆ›å»ºå®Œæˆ: {deployment_dir}")
        return deployment_dir
    
    def deploy_to_jetson(self, deployment_dir: Path) -> bool:
        """éƒ¨ç½²åˆ°Jetsonè®¾å¤‡"""
        logger.info("éƒ¨ç½²åˆ°Jetson Orin Nano...")
        
        try:
            # ä¼ è¾“æ–‡ä»¶
            scp_cmd = f'sshpass -p "{self.jetson_password}" scp -o StrictHostKeyChecking=no -r {deployment_dir} {self.jetson_user}@{self.jetson_ip}:~/'
            
            result = subprocess.run(scp_cmd, shell=True, capture_output=True, text=True, timeout=60)
            
            if result.returncode == 0:
                logger.info("âœ… æ–‡ä»¶ä¼ è¾“æˆåŠŸ")
                return True
            else:
                logger.error(f"âŒ æ–‡ä»¶ä¼ è¾“å¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ éƒ¨ç½²å¼‚å¸¸: {e}")
            return False
    
    def run_edge_benchmark(self) -> Dict:
        """åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡ŒåŸºå‡†æµ‹è¯•"""
        logger.info("åœ¨Jetsonä¸Šè¿è¡ŒåŸºå‡†æµ‹è¯•...")
        
        try:
            # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
            benchmark_cmd = f'cd edge_deployment && python3 edge_inference.py'
            full_cmd = f'sshpass -p "{self.jetson_password}" ssh -o StrictHostKeyChecking=no {self.jetson_user}@{self.jetson_ip} "{benchmark_cmd}"'
            
            result = subprocess.run(full_cmd, shell=True, capture_output=True, text=True, timeout=120)
            
            if result.returncode == 0:
                logger.info("âœ… è¾¹ç¼˜åŸºå‡†æµ‹è¯•å®Œæˆ")
                
                # è·å–ç»“æœæ–‡ä»¶
                get_results_cmd = f'sshpass -p "{self.jetson_password}" scp -o StrictHostKeyChecking=no {self.jetson_user}@{self.jetson_ip}:~/edge_deployment/edge_inference_results.json ./edge_results.json'
                
                subprocess.run(get_results_cmd, shell=True, capture_output=True, text=True)
                
                # è¯»å–ç»“æœ
                try:
                    with open('edge_results.json', 'r') as f:
                        edge_results = json.load(f)
                    return edge_results
                except:
                    return {'output': result.stdout, 'error': 'Could not parse results'}
            else:
                logger.error(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {result.stderr}")
                return {'error': result.stderr, 'output': result.stdout}
                
        except Exception as e:
            logger.error(f"âŒ åŸºå‡†æµ‹è¯•å¼‚å¸¸: {e}")
            return {'exception': str(e)}
    
    def run_local_benchmark(self) -> Dict:
        """åœ¨æœ¬åœ°è¿è¡ŒåŸºå‡†æµ‹è¯•ä½œä¸ºå¯¹æ¯”"""
        logger.info("è¿è¡Œæœ¬åœ°åŸºå‡†æµ‹è¯•...")
        
        try:
            # è¿™é‡Œå¯ä»¥è¿è¡Œç›¸åŒçš„æ¨¡å‹åœ¨æœ¬åœ°3090ä¸Š
            import torch.nn as nn
            
            class SimplifiedStudentModel(nn.Module):
                def __init__(self, input_dim=768, hidden_dim=256, num_classes=10):
                    super().__init__()
                    self.encoder = nn.Sequential(
                        nn.Linear(input_dim, hidden_dim),
                        nn.ReLU(),
                        nn.Dropout(0.1),
                        nn.Linear(hidden_dim, hidden_dim//2),
                        nn.ReLU(),
                        nn.Linear(hidden_dim//2, num_classes)
                    )
                
                def forward(self, x):
                    return self.encoder(x)
            
            model = SimplifiedStudentModel()
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model.to(device)
            model.eval()
            
            # åŸºå‡†æµ‹è¯•
            latencies = []
            input_dim = 768
            num_samples = 100
            
            # é¢„çƒ­
            dummy_input = torch.randn(1, input_dim).to(device)
            for _ in range(10):
                with torch.no_grad():
                    _ = model(dummy_input)
            
            # æµ‹è¯•
            for i in range(num_samples):
                input_data = torch.randn(1, input_dim).to(device)
                
                start_time = time.time()
                with torch.no_grad():
                    output = model(input_data)
                end_time = time.time()
                
                latencies.append((end_time - start_time) * 1000)
            
            return {
                'avg_latency_ms': sum(latencies) / len(latencies),
                'min_latency_ms': min(latencies),
                'max_latency_ms': max(latencies),
                'device': str(device),
                'model_params': sum(p.numel() for p in model.parameters()),
                'gpu_memory_mb': torch.cuda.max_memory_allocated() // (1024**2) if torch.cuda.is_available() else 0
            }
            
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def run_complete_experiment(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„è¾¹ç¼˜éƒ¨ç½²å®éªŒ"""
        logger.info("ğŸš€ å¼€å§‹è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²å®éªŒ")
        
        # 1. æµ‹è¯•è¿æ¥
        if not self.test_jetson_connection():
            self.results['deployment_status'] = 'connection_failed'
            return self.results
        
        # 2. è·å–è¾¹ç¼˜è®¾å¤‡ç¡¬ä»¶ä¿¡æ¯
        self.results['edge_hardware'] = self.get_jetson_hardware_info()
        
        # 3. åˆ›å»ºéƒ¨ç½²åŒ…
        deployment_dir = self.create_deployment_package()
        
        # 4. éƒ¨ç½²åˆ°è¾¹ç¼˜è®¾å¤‡
        if not self.deploy_to_jetson(deployment_dir):
            self.results['deployment_status'] = 'deployment_failed'
            return self.results
        
        # 5. è¿è¡Œæœ¬åœ°åŸºå‡†æµ‹è¯•
        logger.info("è¿è¡Œæœ¬åœ°åŸºå‡†æµ‹è¯•...")
        local_results = self.run_local_benchmark()
        
        # 6. è¿è¡Œè¾¹ç¼˜åŸºå‡†æµ‹è¯•
        logger.info("è¿è¡Œè¾¹ç¼˜åŸºå‡†æµ‹è¯•...")  
        edge_results = self.run_edge_benchmark()
        
        # 7. åˆ†æç»“æœ
        self.results['performance_comparison'] = {
            'local_3090': local_results,
            'jetson_orin_nano': edge_results
        }
        
        if 'error' not in edge_results and 'error' not in local_results:
            # è®¡ç®—æ€§èƒ½å·®å¼‚
            local_latency = local_results.get('avg_latency_ms', 0)
            edge_latency = edge_results.get('avg_latency_ms', 0)
            
            if local_latency > 0:
                self.results['performance_analysis'] = {
                    'latency_degradation': f"{edge_latency / local_latency:.1f}x",
                    'local_latency_ms': local_latency,
                    'edge_latency_ms': edge_latency
                }
        
        self.results['deployment_status'] = 'completed'
        
        # 8. ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"edge_deployment_results_{timestamp}.json"
        
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        logger.info(f"âœ… å®éªŒå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {results_file}")
        return self.results

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ å¯åŠ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²å®éªŒ")
    
    try:
        experiment = EdgeDeploymentExperiment()
        results = experiment.run_complete_experiment()
        
        # æ‰“å°å…³é”®ç»“æœ
        print("\\n" + "="*60)
        print("ğŸ“Š è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²å®éªŒç»“æœæ‘˜è¦")
        print("="*60)
        
        if results['deployment_status'] == 'completed':
            perf = results.get('performance_comparison', {})
            local = perf.get('local_3090', {})
            edge = perf.get('jetson_orin_nano', {})
            
            print(f"ğŸ–¥ï¸  æœ¬åœ°RTX 3090: {local.get('avg_latency_ms', 'N/A'):.2f}ms" if isinstance(local.get('avg_latency_ms'), (int, float)) else f"æœ¬åœ°RTX 3090: {local.get('avg_latency_ms', 'N/A')}")
            print(f"ğŸ“± Jetson Orin Nano: {edge.get('avg_latency_ms', 'N/A'):.2f}ms" if isinstance(edge.get('avg_latency_ms'), (int, float)) else f"Jetson Orin Nano: {edge.get('avg_latency_ms', 'N/A')}")
            
            analysis = results.get('performance_analysis', {})
            if analysis:
                print(f"ğŸ“ˆ æ€§èƒ½æ¯”è¾ƒ: {analysis.get('latency_degradation', 'N/A')} å»¶è¿Ÿå¢åŠ ")
        else:
            print(f"âŒ å®éªŒçŠ¶æ€: {results['deployment_status']}")
        
        print("="*60)
        
    except KeyboardInterrupt:
        logger.info("å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"å®éªŒå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
