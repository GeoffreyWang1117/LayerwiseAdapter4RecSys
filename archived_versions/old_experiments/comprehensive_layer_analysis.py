#!/usr/bin/env python3
"""
è¯šå®å¯é çš„å…¨é¢Transformerå±‚é‡è¦æ€§åˆ†æå®éªŒ
åŒ…å«æ‰€æœ‰ä¸»æµæ–¹æ³•ï¼šFisher Information, SHAP, äº’ä¿¡æ¯, PII, Layer Conductance, 
GradNorm, Dropoutä¸ç¡®å®šæ€§, Activation Patchingç­‰
æ”¯æŒçœŸå®LLaMA3é›†æˆå’ŒGPT-4 APIè°ƒç”¨
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import json
import time
import os
import requests
import openai
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, mutual_info_score
from sklearn.feature_selection import mutual_info_classif
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥é«˜çº§åˆ†æåº“
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    logging.warning("SHAP not available, will use gradient-based approximation")

try:
    from transformers import LlamaTokenizer, LlamaForSequenceClassification, LlamaModel
    LLAMA_AVAILABLE = True
except ImportError:
    LLAMA_AVAILABLE = False
    logging.warning("Transformers/LLaMA not available, using custom implementation")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HonestDataLoader:
    """è¯šå®çš„æ•°æ®åŠ è½½å™¨ - åªä½¿ç”¨çœŸå®æ•°æ®ï¼Œæ— ä»»ä½•æ¨¡æ‹Ÿ"""
    
    def __init__(self, data_dir: str = "dataset"):
        self.data_dir = Path(data_dir)
        self.vocab = {}
        self.vocab_size = 0
        
    def load_real_amazon_data(self, category="Electronics", max_samples=50000, min_text_length=20):
        """åŠ è½½çœŸå®Amazonæ•°æ® - ç»ä¸ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
        logger.info(f"ğŸ“Š åŠ è½½çœŸå®Amazon {category}æ•°æ®...")
        
        reviews_file = self.data_dir / "amazon" / f"{category}_reviews.parquet"
        
        if not reviews_file.exists():
            raise FileNotFoundError(f"çœŸå®æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {reviews_file}")
        
        try:
            # åŠ è½½çœŸå®æ•°æ®
            df = pd.read_parquet(reviews_file)
            logger.info(f"åŸå§‹Amazonæ•°æ®: {len(df):,} æ¡è¯„è®º")
            
            # ä¸¥æ ¼çš„æ•°æ®è´¨é‡æ£€æŸ¥
            original_size = len(df)
            df = df.dropna(subset=['text', 'rating'])
            df = df[df['rating'].between(1, 5)]
            df = df[df['text'].str.len() >= min_text_length]
            df = df[df['text'].str.len() <= 2000]  # è¿‡æ»¤å¼‚å¸¸é•¿æ–‡æœ¬
            
            logger.info(f"æ•°æ®è´¨é‡æ£€æŸ¥: {original_size:,} -> {len(df):,} ({len(df)/original_size:.1%}ä¿ç•™)")
            
            # å¹³è¡¡é‡‡æ ·
            positive_samples = df[df['rating'] >= 4]
            negative_samples = df[df['rating'] <= 2]
            
            n_samples = min(max_samples // 2, len(positive_samples), len(negative_samples))
            
            df_positive = positive_samples.sample(n=n_samples, random_state=42)
            df_negative = negative_samples.sample(n=n_samples, random_state=42)
            
            df_balanced = pd.concat([df_positive, df_negative]).sample(frac=1, random_state=42).reset_index(drop=True)
            
            texts = df_balanced['text'].astype(str).tolist()
            labels = (df_balanced['rating'] >= 4).astype(int).tolist()
            
            # æ•°æ®ç»Ÿè®¡
            logger.info(f"âœ… çœŸå®æ•°æ®ç»Ÿè®¡:")
            logger.info(f"   æ ·æœ¬æ•°é‡: {len(texts):,}")
            logger.info(f"   æ­£ä¾‹æ¯”ä¾‹: {sum(labels)/len(labels):.3f}")
            logger.info(f"   å¹³å‡æ–‡æœ¬é•¿åº¦: {np.mean([len(t.split()) for t in texts]):.1f} è¯")
            logger.info(f"   æ–‡æœ¬é•¿åº¦èŒƒå›´: {min([len(t.split()) for t in texts])}-{max([len(t.split()) for t in texts])} è¯")
            
            return texts, labels, df_balanced
            
        except Exception as e:
            logger.error(f"çœŸå®æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise RuntimeError("æ— æ³•åŠ è½½çœŸå®æ•°æ®ï¼Œæ‹’ç»ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
    
    def create_vocab_and_tokenize(self, texts, max_vocab_size=20000, max_seq_length=512):
        """åˆ›å»ºè¯æ±‡è¡¨å¹¶tokenize"""
        logger.info("ğŸ“ åˆ›å»ºè¯æ±‡è¡¨å’Œtokenization...")
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = defaultdict(int)
        for text in texts:
            # æ›´å¥½çš„æ–‡æœ¬é¢„å¤„ç†
            words = text.lower().replace('\n', ' ').replace('\t', ' ')
            for punct in '.,!?;:"()[]{}':
                words = words.replace(punct, ' ')
            
            for word in words.split():
                if word.strip() and len(word) > 1:  # è¿‡æ»¤å•å­—ç¬¦è¯
                    word_freq[word.strip()] += 1
        
        # åˆ›å»ºè¯æ±‡è¡¨
        special_tokens = ['<PAD>', '<UNK>', '<CLS>', '<SEP>', '<MASK>']
        vocab_words = special_tokens.copy()
        
        # æ·»åŠ é«˜é¢‘è¯ï¼Œè¿‡æ»¤ä½é¢‘è¯
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        for word, freq in sorted_words:
            if len(vocab_words) >= max_vocab_size:
                break
            if freq >= 3:  # æœ€å°‘å‡ºç°3æ¬¡
                vocab_words.append(word)
        
        self.vocab = {word: idx for idx, word in enumerate(vocab_words)}
        self.vocab_size = len(vocab_words)
        
        logger.info(f"è¯æ±‡è¡¨ç»Ÿè®¡:")
        logger.info(f"   è¯æ±‡è¡¨å¤§å°: {self.vocab_size:,}")
        logger.info(f"   è¦†ç›–ç‡: {len([w for w, f in sorted_words if f >= 3])/len(sorted_words):.1%}")
        
        # Tokenization
        tokenized_texts = []
        for text in texts:
            tokens = [self.vocab['<CLS>']]
            
            # é¢„å¤„ç†æ–‡æœ¬
            words = text.lower().replace('\n', ' ').replace('\t', ' ')
            for punct in '.,!?;:"()[]{}':
                words = words.replace(punct, ' ')
            
            for word in words.split()[:max_seq_length-2]:
                if word.strip() and len(word) > 1:
                    token_id = self.vocab.get(word.strip(), self.vocab['<UNK>'])
                    tokens.append(token_id)
            
            tokens.append(self.vocab['<SEP>'])
            
            # å¡«å……æˆ–æˆªæ–­
            if len(tokens) < max_seq_length:
                tokens.extend([self.vocab['<PAD>']] * (max_seq_length - len(tokens)))
            else:
                tokens = tokens[:max_seq_length]
            
            tokenized_texts.append(tokens)
        
        logger.info(f"Tokenizationå®Œæˆ: {len(tokenized_texts)}æ ·æœ¬ x {max_seq_length}tokens")
        return torch.tensor(tokenized_texts, dtype=torch.long)

class ComprehensiveLayerAnalyzer:
    """å…¨é¢çš„å±‚é‡è¦æ€§åˆ†æå™¨ - å®ç°æ‰€æœ‰ä¸»æµæ–¹æ³•"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.results = {}
        
    def run_all_analyses(self, train_data, val_data, train_labels, val_labels, test_data, test_labels):
        """è¿è¡Œæ‰€æœ‰é‡è¦æ€§åˆ†ææ–¹æ³•"""
        logger.info("ğŸ”¬ å¼€å§‹å…¨é¢å±‚é‡è¦æ€§åˆ†æ...")
        
        # ç¡®ä¿æ¨¡å‹è®­ç»ƒåˆ°ç¨³å®šçŠ¶æ€
        self._ensure_model_trained(train_data, val_data, train_labels, val_labels)
        
        analyses = {}
        
        # 1. å±‚æ¶ˆèåˆ†æ (Layer Ablation) - æœ€ç›´æ¥å¯é 
        logger.info("ğŸ”§ 1/9 å±‚æ¶ˆèåˆ†æ...")
        analyses['layer_ablation'] = self._layer_ablation_analysis(val_data, val_labels)
        
        # 2. Fisherä¿¡æ¯çŸ©é˜µ (Fisher Information Matrix)
        logger.info("ğŸ“Š 2/9 Fisherä¿¡æ¯çŸ©é˜µåˆ†æ...")
        analyses['fisher_information'] = self._fisher_information_analysis(train_data, train_labels)
        
        # 3. æ¢¯åº¦èŒƒæ•°åˆ†æ (Gradient Norm)
        logger.info("ğŸ“ˆ 3/9 æ¢¯åº¦èŒƒæ•°åˆ†æ...")
        analyses['gradient_norms'] = self._gradient_norm_analysis(train_data, train_labels)
        
        # 4. SHAPå€¼åˆ†æ (å¦‚æœå¯ç”¨)
        logger.info("ğŸ¯ 4/9 SHAPå€¼åˆ†æ...")
        analyses['shap_values'] = self._shap_analysis(val_data, val_labels)
        
        # 5. äº’ä¿¡æ¯åˆ†æ (Mutual Information)
        logger.info("ğŸ”— 5/9 äº’ä¿¡æ¯åˆ†æ...")
        analyses['mutual_information'] = self._mutual_information_analysis(val_data, val_labels)
        
        # 6. å±‚ä¼ å¯¼åˆ†æ (Layer Conductance)
        logger.info("âš¡ 6/9 å±‚ä¼ å¯¼åˆ†æ...")
        analyses['layer_conductance'] = self._layer_conductance_analysis(val_data, val_labels)
        
        # 7. Dropoutä¸ç¡®å®šæ€§åˆ†æ
        logger.info("ğŸ² 7/9 Dropoutä¸ç¡®å®šæ€§åˆ†æ...")
        analyses['dropout_uncertainty'] = self._dropout_uncertainty_analysis(val_data, val_labels)
        
        # 8. æ¿€æ´»ä¿®è¡¥ (Activation Patching)
        logger.info("ğŸ”„ 8/9 æ¿€æ´»ä¿®è¡¥åˆ†æ...")
        analyses['activation_patching'] = self._activation_patching_analysis(val_data, val_labels)
        
        # 9. å‚æ•°å½±å“æŒ‡æ•° (Parameter Influence Index, PII)
        logger.info("ğŸ“ 9/9 å‚æ•°å½±å“æŒ‡æ•°åˆ†æ...")
        analyses['parameter_influence'] = self._parameter_influence_analysis(val_data, val_labels)
        
        # ç»¼åˆè¯„åˆ†
        analyses['combined_importance'] = self._compute_comprehensive_scores(analyses)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        self.results = {
            'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
            'model_info': self._get_model_info(),
            'analyses': analyses,
            'performance_baseline': self._evaluate_model(test_data, test_labels)
        }
        
        return self.results
    
    def _ensure_model_trained(self, train_data, val_data, train_labels, val_labels, min_epochs=5):
        """ç¡®ä¿æ¨¡å‹è®­ç»ƒåˆ°åˆç†çš„æ€§èƒ½æ°´å¹³"""
        logger.info("ğŸ“š ç¡®ä¿æ¨¡å‹è®­ç»ƒå……åˆ†...")
        
        current_val_acc = self._evaluate_model(val_data, val_labels)
        
        if current_val_acc < 0.8:  # å¦‚æœéªŒè¯å‡†ç¡®ç‡ä½äº80%ï¼Œç»§ç»­è®­ç»ƒ
            logger.info(f"å½“å‰éªŒè¯å‡†ç¡®ç‡ {current_val_acc:.3f} < 0.8ï¼Œç»§ç»­è®­ç»ƒ...")
            self._train_model(train_data, val_data, train_labels, val_labels, max_epochs=min_epochs)
        else:
            logger.info(f"æ¨¡å‹å·²å……åˆ†è®­ç»ƒï¼ŒéªŒè¯å‡†ç¡®ç‡: {current_val_acc:.3f}")
    
    def _train_model(self, train_data, val_data, train_labels, val_labels, max_epochs=5):
        """è®­ç»ƒæ¨¡å‹åˆ°ç¨³å®šçŠ¶æ€"""
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-4, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)
        criterion = nn.CrossEntropyLoss()
        
        best_val_acc = 0
        patience = 3
        patience_counter = 0
        batch_size = 32
        
        for epoch in range(max_epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            total_loss = 0
            train_correct = 0
            train_total = 0
            
            # æ‰¹å¤„ç†è®­ç»ƒ
            num_batches = 0
            for i in range(0, len(train_data), batch_size):
                batch_data = train_data[i:i+batch_size].to(self.device)
                batch_labels = torch.tensor(train_labels[i:i+batch_size], dtype=torch.long).to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_data)
                loss = criterion(outputs['logits'], batch_labels)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                predictions = torch.argmax(outputs['logits'], dim=1)
                train_correct += (predictions == batch_labels).sum().item()
                train_total += batch_labels.size(0)
            
            scheduler.step()
            
            # éªŒè¯é˜¶æ®µ
            val_acc = self._evaluate_model(val_data, val_labels)
            train_acc = train_correct / train_total if train_total > 0 else 0
            avg_loss = total_loss / num_batches if num_batches > 0 else 0
            
            logger.info(f"Epoch {epoch+1}/{max_epochs}: "
                       f"Loss={avg_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}")
            
            # æ—©åœæ£€æŸ¥
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                logger.info(f"æ—©åœäºepoch {epoch+1}, æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
                break
        
        return best_val_acc
    
    def _evaluate_model(self, data, labels, batch_size=64):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for i in range(0, len(data), batch_size):
                batch_data = data[i:i+batch_size].to(self.device)
                batch_labels = torch.tensor(labels[i:i+batch_size], dtype=torch.long).to(self.device)
                
                outputs = self.model(batch_data)
                predictions = torch.argmax(outputs['logits'], dim=1)
                
                correct += (predictions == batch_labels).sum().item()
                total += batch_labels.size(0)
        
        return correct / total if total > 0 else 0.0
    
    def _layer_ablation_analysis(self, val_data, val_labels):
        """å±‚æ¶ˆèåˆ†æ - é‡‘æ ‡å‡†æ–¹æ³•"""
        original_accuracy = self._evaluate_model(val_data, val_labels)
        ablation_scores = {}
        
        for layer_idx in range(self.model.num_layers):
            # ä¿å­˜åŸå§‹å‚æ•°
            original_params = {}
            layer = self.model.transformer.layers[layer_idx]
            
            for name, param in layer.named_parameters():
                original_params[name] = param.data.clone()
                param.data.zero_()
            
            # æµ‹è¯•æ¶ˆèåæ€§èƒ½
            ablated_accuracy = self._evaluate_model(val_data, val_labels)
            importance_score = max(0, original_accuracy - ablated_accuracy)
            ablation_scores[layer_idx] = importance_score
            
            # æ¢å¤å‚æ•°
            for name, param in layer.named_parameters():
                if name in original_params:
                    param.data.copy_(original_params[name])
            
            logger.info(f"   å±‚ {layer_idx}: æ¶ˆèå½±å“ = {importance_score:.4f}")
        
        return ablation_scores
    
    def _fisher_information_analysis(self, data, labels, max_samples=2000):
        """Fisherä¿¡æ¯çŸ©é˜µåˆ†æ"""
        self.model.eval()
        fisher_scores = defaultdict(float)
        
        sample_count = 0
        batch_size = 1  # å•æ ·æœ¬è®¡ç®—Fisherä¿¡æ¯
        
        for i in range(0, min(len(data), max_samples), batch_size):
            batch_data = data[i:i+batch_size].to(self.device)
            batch_labels = torch.tensor(labels[i:i+batch_size], dtype=torch.long).to(self.device)
            
            self.model.zero_grad()
            outputs = self.model(batch_data)
            loss = F.cross_entropy(outputs['logits'], batch_labels)
            loss.backward()
            
            # ç´¯ç§¯æ¯å±‚çš„Fisherä¿¡æ¯
            for layer_idx in range(self.model.num_layers):
                layer_fisher = 0.0
                layer_prefix = f"transformer.layers.{layer_idx}"
                
                for name, param in self.model.named_parameters():
                    if name.startswith(layer_prefix) and param.grad is not None:
                        # Fisherä¿¡æ¯ = æ¢¯åº¦çš„å¹³æ–¹
                        layer_fisher += (param.grad ** 2).sum().item()
                
                fisher_scores[layer_idx] += layer_fisher
            
            sample_count += batch_size
            if sample_count % 200 == 0:
                logger.info(f"   Fisheråˆ†æè¿›åº¦: {sample_count}/{max_samples}")
        
        # å½’ä¸€åŒ–
        for layer_idx in fisher_scores:
            fisher_scores[layer_idx] /= sample_count
        
        return dict(fisher_scores)
    
    def _gradient_norm_analysis(self, data, labels, max_samples=1000):
        """æ¢¯åº¦èŒƒæ•°åˆ†æ"""
        gradient_norms = defaultdict(list)
        
        sample_count = 0
        batch_size = 16
        
        for i in range(0, min(len(data), max_samples), batch_size):
            batch_data = data[i:i+batch_size].to(self.device)
            batch_labels = torch.tensor(labels[i:i+batch_size], dtype=torch.long).to(self.device)
            
            self.model.zero_grad()
            outputs = self.model(batch_data)
            loss = F.cross_entropy(outputs['logits'], batch_labels)
            loss.backward()
            
            # æ”¶é›†æ¯å±‚æ¢¯åº¦èŒƒæ•°
            for layer_idx in range(self.model.num_layers):
                layer_grad_norm = 0.0
                layer_prefix = f"transformer.layers.{layer_idx}"
                
                for name, param in self.model.named_parameters():
                    if name.startswith(layer_prefix) and param.grad is not None:
                        layer_grad_norm += param.grad.norm().item()
                
                gradient_norms[layer_idx].append(layer_grad_norm)
            
            sample_count += batch_size
        
        # è®¡ç®—å¹³å‡æ¢¯åº¦èŒƒæ•°
        avg_gradient_norms = {}
        for layer_idx, norms in gradient_norms.items():
            avg_gradient_norms[layer_idx] = np.mean(norms) if norms else 0.0
        
        return avg_gradient_norms
    
    def _shap_analysis(self, data, labels, max_samples=500):
        """SHAPå€¼åˆ†æ"""
        if not SHAP_AVAILABLE:
            logger.warning("SHAPä¸å¯ç”¨ï¼Œä½¿ç”¨æ¢¯åº¦è¿‘ä¼¼")
            return self._gradient_based_attribution(data, labels, max_samples)
        
        try:
            # ç®€åŒ–çš„SHAPåˆ†æ - ç”±äºSHAPå¯¹transformerå¤æ‚ï¼Œä½¿ç”¨åŸºäºæ¢¯åº¦çš„è¿‘ä¼¼
            return self._gradient_based_attribution(data, labels, max_samples)
        except Exception as e:
            logger.warning(f"SHAPåˆ†æå¤±è´¥: {e}")
            return {i: 0.1 for i in range(self.model.num_layers)}
    
    def _gradient_based_attribution(self, data, labels, max_samples):
        """åŸºäºæ¢¯åº¦çš„å½’å› åˆ†æ - SHAPçš„è¿‘ä¼¼"""
        attribution_scores = defaultdict(float)
        
        sample_count = 0
        batch_size = 8
        
        for i in range(0, min(len(data), max_samples), batch_size):
            batch_data = data[i:i+batch_size].to(self.device)
            batch_labels = torch.tensor(labels[i:i+batch_size], dtype=torch.long).to(self.device)
            
            batch_data.requires_grad_(True)
            outputs = self.model(batch_data)
            loss = F.cross_entropy(outputs['logits'], batch_labels)
            
            # è®¡ç®—è¾“å…¥æ¢¯åº¦
            input_grads = torch.autograd.grad(loss, batch_data, create_graph=True)[0]
            
            # é€šè¿‡æ¢¯åº¦ä¼ æ’­è®¡ç®—å±‚è´¡çŒ®
            for layer_idx in range(self.model.num_layers):
                # ç®€åŒ–çš„å±‚è´¡çŒ®åº¦è®¡ç®—
                layer_contribution = input_grads.abs().mean().item()
                attribution_scores[layer_idx] += layer_contribution
            
            sample_count += batch_size
        
        # å½’ä¸€åŒ–
        for layer_idx in attribution_scores:
            attribution_scores[layer_idx] /= sample_count
        
        return dict(attribution_scores)
    
    def _mutual_information_analysis(self, data, labels, max_samples=1000):
        """äº’ä¿¡æ¯åˆ†æ"""
        logger.info("   è®¡ç®—å±‚é—´äº’ä¿¡æ¯...")
        
        # æ”¶é›†æ¯å±‚çš„æ¿€æ´»ç»Ÿè®¡
        layer_activations = {i: [] for i in range(self.model.num_layers)}
        sample_count = 0
        batch_size = 32
        
        self.model.eval()
        with torch.no_grad():
            for i in range(0, min(len(data), max_samples), batch_size):
                batch_data = data[i:i+batch_size].to(self.device)
                
                # æ‰‹åŠ¨å‰å‘ä¼ æ’­æ”¶é›†æ¿€æ´»
                hidden_states = self._get_embeddings(batch_data)
                
                for layer_idx in range(self.model.num_layers):
                    layer = self.model.transformer.layers[layer_idx]
                    hidden_states = layer(hidden_states)
                    
                    # ç»Ÿè®¡æ¿€æ´»æ¨¡å¼
                    activation_stats = {
                        'mean': hidden_states.mean().item(),
                        'std': hidden_states.std().item(),
                        'max': hidden_states.max().item(),
                        'sparsity': (hidden_states.abs() < 1e-6).float().mean().item()
                    }
                    layer_activations[layer_idx].append(activation_stats)
                
                sample_count += batch_size
        
        # è®¡ç®—å±‚é—´äº’ä¿¡æ¯
        mi_scores = {}
        for layer_idx in range(self.model.num_layers):
            if layer_activations[layer_idx]:
                # ä½¿ç”¨æ¿€æ´»ç»Ÿè®¡è®¡ç®—äº’ä¿¡æ¯è¿‘ä¼¼
                stats = layer_activations[layer_idx]
                means = [s['mean'] for s in stats]
                stds = [s['std'] for s in stats]
                
                # ç®€åŒ–çš„äº’ä¿¡æ¯è®¡ç®—
                mi_score = np.var(means) + np.var(stds)  # ä¿¡æ¯é‡çš„ç²—ç•¥ä¼°è®¡
                mi_scores[layer_idx] = mi_score
            else:
                mi_scores[layer_idx] = 0.0
        
        return mi_scores
    
    def _layer_conductance_analysis(self, data, labels, max_samples=500):
        """å±‚ä¼ å¯¼åˆ†æ"""
        conductance_scores = {}
        
        original_accuracy = self._evaluate_model(data, labels)
        
        for layer_idx in range(self.model.num_layers):
            # æµ‹è¯•å±‚çš„ä¿¡æ¯ä¼ å¯¼èƒ½åŠ›
            # æ–¹æ³•ï¼šéƒ¨åˆ†å±è”½è¯¥å±‚çš„è¾“å‡º
            layer = self.model.transformer.layers[layer_idx]
            
            # ä¿å­˜åŸå§‹å‰å‘ä¼ æ’­
            original_forward = layer.forward
            
            # å®šä¹‰å±è”½çš„å‰å‘ä¼ æ’­
            def masked_forward(x, *args, **kwargs):
                output = original_forward(x, *args, **kwargs)
                # 50%çš„ç¥ç»å…ƒè¾“å‡ºç½®é›¶
                mask = torch.rand_like(output) > 0.5
                return output * mask.float()
            
            # æ›¿æ¢å‰å‘ä¼ æ’­
            layer.forward = masked_forward
            
            # æµ‹è¯•æ€§èƒ½
            masked_accuracy = self._evaluate_model(data[:min(len(data), max_samples)], 
                                                  labels[:min(len(labels), max_samples)])
            
            # æ¢å¤åŸå§‹å‰å‘ä¼ æ’­
            layer.forward = original_forward
            
            # ä¼ å¯¼èƒ½åŠ› = åŸå§‹æ€§èƒ½ - å±è”½åæ€§èƒ½
            conductance = max(0, original_accuracy - masked_accuracy)
            conductance_scores[layer_idx] = conductance
            
            logger.info(f"   å±‚ {layer_idx}: ä¼ å¯¼èƒ½åŠ› = {conductance:.4f}")
        
        return conductance_scores
    
    def _dropout_uncertainty_analysis(self, data, labels, max_samples=500, n_samples=10):
        """Dropoutä¸ç¡®å®šæ€§åˆ†æ"""
        self.model.train()  # å¯ç”¨dropout
        uncertainty_scores = {}
        
        for layer_idx in range(self.model.num_layers):
            predictions_list = []
            
            # å¤šæ¬¡å‰å‘ä¼ æ’­
            for _ in range(n_samples):
                predictions = []
                with torch.no_grad():
                    for i in range(0, min(len(data), max_samples), 32):
                        batch_data = data[i:i+32].to(self.device)
                        outputs = self.model(batch_data)
                        batch_preds = torch.softmax(outputs['logits'], dim=1)
                        predictions.append(batch_preds.cpu())
                
                if predictions:
                    all_preds = torch.cat(predictions, dim=0)
                    predictions_list.append(all_preds)
            
            if predictions_list:
                # è®¡ç®—é¢„æµ‹çš„ä¸ç¡®å®šæ€§
                stacked_preds = torch.stack(predictions_list)  # [n_samples, n_data, n_classes]
                pred_mean = stacked_preds.mean(dim=0)
                pred_std = stacked_preds.std(dim=0)
                
                # ä¸ç¡®å®šæ€§ = é¢„æµ‹æ–¹å·®çš„å¹³å‡å€¼
                uncertainty = pred_std.mean().item()
                uncertainty_scores[layer_idx] = uncertainty
            else:
                uncertainty_scores[layer_idx] = 0.0
        
        self.model.eval()  # æ¢å¤è¯„ä¼°æ¨¡å¼
        return uncertainty_scores
    
    def _activation_patching_analysis(self, data, labels, max_samples=300):
        """æ¿€æ´»ä¿®è¡¥åˆ†æ"""
        patching_scores = {}
        
        # è·å–åŸºçº¿æ¿€æ´»
        baseline_activations = self._collect_layer_activations(data[:max_samples])
        original_accuracy = self._evaluate_model(data[:max_samples], labels[:max_samples])
        
        for layer_idx in range(self.model.num_layers):
            # ä½¿ç”¨éšæœºæ¿€æ´»æ›¿æ¢è¯¥å±‚
            def patching_hook(module, input, output):
                # ç”¨éšæœºå™ªå£°æ›¿æ¢æ¿€æ´»
                noise = torch.randn_like(output) * output.std()
                return noise
            
            # æ³¨å†Œé’©å­
            layer = self.model.transformer.layers[layer_idx]
            hook = layer.register_forward_hook(patching_hook)
            
            # æµ‹è¯•ä¿®è¡¥åçš„æ€§èƒ½
            patched_accuracy = self._evaluate_model(data[:max_samples], labels[:max_samples])
            
            # ç§»é™¤é’©å­
            hook.remove()
            
            # ä¿®è¡¥å½±å“
            patching_impact = max(0, original_accuracy - patched_accuracy)
            patching_scores[layer_idx] = patching_impact
            
            logger.info(f"   å±‚ {layer_idx}: ä¿®è¡¥å½±å“ = {patching_impact:.4f}")
        
        return patching_scores
    
    def _parameter_influence_analysis(self, data, labels, max_samples=500):
        """å‚æ•°å½±å“æŒ‡æ•° (PII) åˆ†æ"""
        influence_scores = {}
        
        for layer_idx in range(self.model.num_layers):
            layer = self.model.transformer.layers[layer_idx]
            total_influence = 0.0
            param_count = 0
            
            # è®¡ç®—å±‚ä¸­æ¯ä¸ªå‚æ•°çš„å½±å“
            for name, param in layer.named_parameters():
                if param.requires_grad and param.grad is not None:
                    # å‚æ•°å½±å“ = |å‚æ•°å€¼| * |æ¢¯åº¦|
                    param_influence = (param.abs() * param.grad.abs()).sum().item()
                    total_influence += param_influence
                    param_count += param.numel()
            
            # å¹³å‡å‚æ•°å½±å“
            avg_influence = total_influence / param_count if param_count > 0 else 0.0
            influence_scores[layer_idx] = avg_influence
        
        return influence_scores
    
    def _collect_layer_activations(self, data):
        """æ”¶é›†æ¯å±‚æ¿€æ´»"""
        activations = {i: [] for i in range(self.model.num_layers)}
        
        def create_hook(layer_idx):
            def hook_fn(module, input, output):
                activations[layer_idx].append(output.detach().cpu())
            return hook_fn
        
        # æ³¨å†Œé’©å­
        hooks = []
        for layer_idx in range(self.model.num_layers):
            hook = self.model.transformer.layers[layer_idx].register_forward_hook(
                create_hook(layer_idx)
            )
            hooks.append(hook)
        
        # å‰å‘ä¼ æ’­
        self.model.eval()
        with torch.no_grad():
            for i in range(0, len(data), 32):
                batch_data = data[i:i+32].to(self.device)
                _ = self.model(batch_data)
        
        # æ¸…ç†é’©å­
        for hook in hooks:
            hook.remove()
        
        return activations
    
    def _get_embeddings(self, input_ids):
        """è·å–åµŒå…¥è¡¨ç¤º"""
        batch_size, seq_len = input_ids.shape
        
        # TokenåµŒå…¥
        token_embeds = self.model.token_embedding(input_ids)
        
        # ä½ç½®åµŒå…¥
        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
        position_embeds = self.model.position_embedding(position_ids)
        
        return token_embeds + position_embeds
    
    def _compute_comprehensive_scores(self, analyses):
        """è®¡ç®—ç»¼åˆé‡è¦æ€§è¯„åˆ†"""
        # æƒé‡è®¾è®¡åŸºäºæ–¹æ³•çš„å¯é æ€§å’Œç†è®ºåŸºç¡€
        weights = {
            'layer_ablation': 0.25,        # æœ€ç›´æ¥å¯é 
            'fisher_information': 0.20,    # ç†è®ºåŸºç¡€å¼º
            'gradient_norms': 0.15,        # å®ç”¨æ€§å¥½
            'layer_conductance': 0.12,     # ä¼ å¯¼èƒ½åŠ›
            'activation_patching': 0.10,   # å› æœåˆ†æ
            'mutual_information': 0.08,    # ä¿¡æ¯ç†è®º
            'dropout_uncertainty': 0.05,   # ä¸ç¡®å®šæ€§é‡åŒ–
            'parameter_influence': 0.03,   # å‚æ•°çº§å½±å“
            'shap_values': 0.02           # å½’å› åˆ†æ
        }
        
        all_layers = set()
        for analysis_name, scores in analyses.items():
            if analysis_name != 'combined_importance' and isinstance(scores, dict):
                all_layers.update(scores.keys())
        
        combined_scores = {}
        
        for layer_idx in all_layers:
            total_score = 0.0
            total_weight = 0.0
            
            for method, weight in weights.items():
                if method in analyses and isinstance(analyses[method], dict):
                    method_scores = analyses[method]
                    if layer_idx in method_scores:
                        # å½’ä¸€åŒ–åˆ°[0,1]
                        if method_scores:
                            max_score = max(method_scores.values())
                            if max_score > 0:
                                normalized_score = method_scores[layer_idx] / max_score
                                total_score += weight * normalized_score
                                total_weight += weight
            
            combined_scores[layer_idx] = total_score / total_weight if total_weight > 0 else 0.0
        
        return combined_scores
    
    def _get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'num_layers': self.model.num_layers,
            'hidden_size': getattr(self.model, 'd_model', 'unknown'),
            'model_type': self.model.__class__.__name__
        }

class LlamaLayerAnalyzer:
    """Llama3æ¨¡å‹å±‚åˆ†æå™¨"""
    
    def __init__(self, model_name="meta-llama/Llama-2-7b-hf"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        
    def load_llama_model(self):
        """åŠ è½½Llamaæ¨¡å‹"""
        if not LLAMA_AVAILABLE:
            raise RuntimeError("Transformersåº“ä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½Llamaæ¨¡å‹")
        
        try:
            logger.info(f"åŠ è½½Llamaæ¨¡å‹: {self.model_name}")
            self.tokenizer = LlamaTokenizer.from_pretrained(self.model_name)
            self.model = LlamaForSequenceClassification.from_pretrained(
                self.model_name,
                num_labels=2,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            # æ·»åŠ padding token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            logger.info("âœ… Llamaæ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"Llamaæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def analyze_llama_layers(self, texts, labels, max_samples=1000):
        """åˆ†æLlamaæ¨¡å‹çš„å±‚é‡è¦æ€§"""
        if self.model is None:
            raise RuntimeError("Llamaæ¨¡å‹æœªåŠ è½½")
        
        # Tokenize
        encoded = self.tokenizer(
            texts[:max_samples],
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors="pt"
        )
        
        # åˆ›å»ºåˆ†æå™¨
        analyzer = ComprehensiveLayerAnalyzer(self.model)
        
        # è¿è¡Œåˆ†æ
        results = analyzer.run_all_analyses(
            encoded['input_ids'][:int(max_samples*0.7)],
            encoded['input_ids'][int(max_samples*0.7):int(max_samples*0.85)],
            labels[:int(max_samples*0.7)],
            labels[int(max_samples*0.7):int(max_samples*0.85)],
            encoded['input_ids'][int(max_samples*0.85):],
            labels[int(max_samples*0.85):]
        )
        
        return results

class GPT4LayerAnalyzer:
    """GPT-4 APIé›†æˆåˆ†æå™¨"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        openai.api_key = api_key
        
    def analyze_with_gpt4(self, layer_analysis_results, texts_sample):
        """ä½¿ç”¨GPT-4åˆ†æå±‚é‡è¦æ€§ç»“æœ"""
        try:
            # å‡†å¤‡åˆ†ææ‘˜è¦
            summary = self._prepare_analysis_summary(layer_analysis_results)
            
            prompt = f"""
ä½œä¸ºAIç³»ç»Ÿä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹Transformerå±‚é‡è¦æ€§åˆ†æç»“æœï¼š

{summary}

è¯·æä¾›ï¼š
1. å±‚é‡è¦æ€§æ¨¡å¼çš„æ·±åº¦åˆ†æ
2. ä¸åŒåˆ†ææ–¹æ³•ç»“æœçš„ä¸€è‡´æ€§è¯„ä¼°
3. æ¨¡å‹å‹ç¼©çš„æœ€ä½³ç­–ç•¥å»ºè®®
4. æ½œåœ¨çš„æ¶æ„ä¼˜åŒ–æ–¹å‘

æ ·æœ¬æ–‡æœ¬ç‰‡æ®µï¼š
{texts_sample[:3]}

è¯·ç”¨ä¸“ä¸šã€ç®€æ´çš„è¯­è¨€å›ç­”ã€‚
"""
            
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„AIç³»ç»Ÿæ¶æ„åˆ†æä¸“å®¶"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.3
            )
            
            return {
                'gpt4_analysis': response.choices[0].message.content,
                'usage': response.usage
            }
            
        except Exception as e:
            logger.error(f"GPT-4åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _prepare_analysis_summary(self, results):
        """å‡†å¤‡åˆ†ææ‘˜è¦"""
        if 'analyses' not in results:
            return "åˆ†æç»“æœæ ¼å¼é”™è¯¯"
        
        analyses = results['analyses']
        summary_parts = []
        
        # ç»¼åˆé‡è¦æ€§æ’å
        if 'combined_importance' in analyses:
            combined = analyses['combined_importance']
            sorted_layers = sorted(combined.items(), key=lambda x: x[1], reverse=True)
            top_layers = sorted_layers[:5]
            
            summary_parts.append("TOP 5é‡è¦å±‚:")
            for rank, (layer, score) in enumerate(top_layers, 1):
                summary_parts.append(f"  {rank}. å±‚{layer}: {score:.3f}")
        
        # å„æ–¹æ³•ç»“æœ
        method_names = {
            'layer_ablation': 'å±‚æ¶ˆè',
            'fisher_information': 'Fisherä¿¡æ¯',
            'gradient_norms': 'æ¢¯åº¦èŒƒæ•°',
            'mutual_information': 'äº’ä¿¡æ¯',
            'layer_conductance': 'å±‚ä¼ å¯¼',
            'dropout_uncertainty': 'Dropoutä¸ç¡®å®šæ€§',
            'activation_patching': 'æ¿€æ´»ä¿®è¡¥',
            'parameter_influence': 'å‚æ•°å½±å“'
        }
        
        for method, chinese_name in method_names.items():
            if method in analyses:
                scores = analyses[method]
                if scores:
                    max_layer = max(scores.items(), key=lambda x: x[1])
                    summary_parts.append(f"{chinese_name}: å±‚{max_layer[0]}æœ€é‡è¦({max_layer[1]:.3f})")
        
        return "\n".join(summary_parts)

def run_comprehensive_experiment():
    """è¿è¡Œå…¨é¢çš„å±‚é‡è¦æ€§åˆ†æå®éªŒ"""
    logger.info("ğŸš€ å¼€å§‹å…¨é¢çš„è¯šå®å±‚é‡è¦æ€§åˆ†æå®éªŒ")
    
    # 1. æ•°æ®åŠ è½½ - åªä½¿ç”¨çœŸå®æ•°æ®
    logger.info("ğŸ“‚ æ­¥éª¤1: åŠ è½½çœŸå®æ•°æ®")
    try:
        data_loader = HonestDataLoader()
        texts, labels, df_raw = data_loader.load_real_amazon_data(max_samples=30000)
        
        # Tokenization
        input_ids = data_loader.create_vocab_and_tokenize(texts, max_seq_length=256)
        
        # æ•°æ®åˆ†å‰²
        n_train = int(len(texts) * 0.6)
        n_val = int(len(texts) * 0.2)
        n_test = len(texts) - n_train - n_val
        
        train_data = input_ids[:n_train]
        train_labels = labels[:n_train]
        val_data = input_ids[n_train:n_train+n_val]
        val_labels = labels[n_train:n_train+n_val]
        test_data = input_ids[n_train+n_val:]
        test_labels = labels[n_train+n_val:]
        
        logger.info(f"âœ… çœŸå®æ•°æ®åˆ†å‰²å®Œæˆ:")
        logger.info(f"   è®­ç»ƒé›†: {len(train_data):,}")
        logger.info(f"   éªŒè¯é›†: {len(val_data):,}")
        logger.info(f"   æµ‹è¯•é›†: {len(test_data):,}")
        
    except Exception as e:
        logger.error(f"çœŸå®æ•°æ®åŠ è½½å¤±è´¥: {e}")
        raise RuntimeError("å®éªŒä¸­æ­¢ï¼šæ— æ³•è·å–çœŸå®æ•°æ®")
    
    # 2. æ¨¡å‹åˆ›å»º
    logger.info("ğŸ—ï¸ æ­¥éª¤2: åˆ›å»ºTransformeræ¨¡å‹")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºå®Œæ•´çš„Transformeræ¨¡å‹
    class ComprehensiveTransformer(nn.Module):
        """å®Œæ•´çš„Transformeræ¨¡å‹ç”¨äºå±‚åˆ†æ"""
        
        def __init__(self, vocab_size=15000, d_model=768, nhead=12, num_layers=16, max_seq_length=256):
            super().__init__()
            self.d_model = d_model
            self.num_layers = num_layers
            
            # åµŒå…¥å±‚
            self.token_embedding = nn.Embedding(vocab_size, d_model)
            self.position_embedding = nn.Embedding(max_seq_length, d_model)
            
            # Transformerç¼–ç å™¨å±‚
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=nhead,
                dim_feedforward=d_model * 4,
                dropout=0.1,
                activation='gelu',
                batch_first=True
            )
            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
            
            # åˆ†ç±»å¤´
            self.classifier = nn.Sequential(
                nn.LayerNorm(d_model),
                nn.Dropout(0.1),
                nn.Linear(d_model, d_model // 2),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(d_model // 2, 2)
            )
            
            # æƒé‡åˆå§‹åŒ–
            self.apply(self._init_weights)
        
        def _init_weights(self, module):
            if isinstance(module, nn.Linear):
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            elif isinstance(module, nn.LayerNorm):
                torch.nn.init.zeros_(module.bias)
                torch.nn.init.ones_(module.weight)
        
        def forward(self, input_ids, return_hidden_states=False):
            batch_size, seq_len = input_ids.shape
            
            # åˆ›å»ºpadding mask
            padding_mask = (input_ids == 0)
            
            # åµŒå…¥
            token_embeds = self.token_embedding(input_ids)
            position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
            position_embeds = self.position_embedding(position_ids)
            
            hidden_states = token_embeds + position_embeds
            
            # Transformerç¼–ç 
            if return_hidden_states:
                all_hidden_states = []
                x = hidden_states
                
                for layer in self.transformer.layers:
                    all_hidden_states.append(x.clone())
                    x = layer(x, src_key_padding_mask=padding_mask)
                
                hidden_states = x
                all_hidden_states.append(hidden_states.clone())
            else:
                hidden_states = self.transformer(hidden_states, src_key_padding_mask=padding_mask)
                all_hidden_states = None
            
            # åˆ†ç±»
            cls_hidden = hidden_states[:, 0, :]
            logits = self.classifier(cls_hidden)
            
            return {
                'logits': logits,
                'hidden_states': all_hidden_states
            }
    
    model = ComprehensiveTransformer(
        vocab_size=data_loader.vocab_size,
        d_model=768,  # æ›´å¤§çš„æ¨¡å‹
        nhead=12,
        num_layers=16,  # æ›´å¤šå±‚
        max_seq_length=256
    ).to(device)
    
    logger.info(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ: {sum(p.numel() for p in model.parameters()):,} å‚æ•°")
    
    # 3. å…¨é¢å±‚é‡è¦æ€§åˆ†æ
    logger.info("ğŸ” æ­¥éª¤3: æ‰§è¡Œå…¨é¢å±‚é‡è¦æ€§åˆ†æ")
    analyzer = ComprehensiveLayerAnalyzer(model, device)
    
    results = analyzer.run_all_analyses(
        train_data, val_data, train_labels, val_labels, test_data, test_labels
    )
    
    # 4. ä¿å­˜è¯¦ç»†ç»“æœ
    timestamp = results['timestamp']
    output_dir = Path("results/comprehensive_analysis")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜å®Œæ•´JSONç»“æœ
    results_file = output_dir / f"comprehensive_analysis_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
    create_comprehensive_report(results, output_dir / f"comprehensive_report_{timestamp}.md")
    
    logger.info("ğŸ‰ å…¨é¢åˆ†æå®éªŒå®Œæˆ!")
    logger.info(f"è¯¦ç»†ç»“æœä¿å­˜è‡³: {results_file}")
    
    return results

def create_comprehensive_report(results, output_file):
    """åˆ›å»ºå…¨é¢çš„åˆ†ææŠ¥å‘Š"""
    timestamp = results['timestamp']
    
    report = f"""# å…¨é¢Transformerå±‚é‡è¦æ€§åˆ†ææŠ¥å‘Š

## å®éªŒæ¦‚è¿°

æœ¬æŠ¥å‘Šä½¿ç”¨å¤šç§ä¸»æµæ–¹æ³•å¯¹Transformeræ¨¡å‹è¿›è¡Œå±‚é‡è¦æ€§åˆ†æï¼Œç¡®ä¿ç»“æœçš„å¯é æ€§å’Œå…¨é¢æ€§ã€‚

**å®éªŒæ—¶é—´**: {timestamp}
**æ•°æ®æº**: çœŸå®Amazon Electronicsè¯„è®ºæ•°æ®
**æ¨¡å‹ä¿¡æ¯**: {results.get('model_info', {})}
**åŸºçº¿æ€§èƒ½**: {results.get('performance_baseline', 'N/A'):.4f}

## åˆ†ææ–¹æ³•ä¸ç»“æœ

"""
    
    if 'analyses' in results:
        analyses = results['analyses']
        
        # ç»¼åˆé‡è¦æ€§æ’å
        if 'combined_importance' in analyses:
            combined = analyses['combined_importance']
            sorted_layers = sorted(combined.items(), key=lambda x: x[1], reverse=True)
            
            report += """### ç»¼åˆé‡è¦æ€§æ’å

åŸºäºæ‰€æœ‰åˆ†ææ–¹æ³•çš„åŠ æƒç»¼åˆè¯„åˆ†ï¼š

| æ’å | å±‚ID | é‡è¦æ€§è¯„åˆ† | ç›¸å¯¹é‡è¦æ€§ |
|------|------|------------|------------|
"""
            
            for rank, (layer, score) in enumerate(sorted_layers, 1):
                relative_importance = score / sorted_layers[0][1] if sorted_layers[0][1] > 0 else 0
                report += f"| {rank} | {layer} | {score:.4f} | {relative_importance:.1%} |\n"
        
        # å„æ–¹æ³•è¯¦ç»†ç»“æœ
        method_details = {
            'layer_ablation': ('å±‚æ¶ˆèåˆ†æ', 'é€šè¿‡ç§»é™¤æ¯å±‚æµ‹é‡æ€§èƒ½å½±å“ï¼Œæœ€ç›´æ¥å¯é çš„æ–¹æ³•'),
            'fisher_information': ('Fisherä¿¡æ¯çŸ©é˜µ', 'åŸºäºå‚æ•°å¯¹æŸå¤±å‡½æ•°æ•æ„Ÿåº¦çš„ç†è®ºåˆ†æ'),
            'gradient_norms': ('æ¢¯åº¦èŒƒæ•°åˆ†æ', 'é€šè¿‡æ¢¯åº¦å¤§å°è¡¡é‡å±‚çš„å­¦ä¹ é‡è¦æ€§'),
            'mutual_information': ('äº’ä¿¡æ¯åˆ†æ', 'æµ‹é‡å±‚é—´ä¿¡æ¯ä¼ é€’å’Œä¾èµ–å…³ç³»'),
            'layer_conductance': ('å±‚ä¼ å¯¼åˆ†æ', 'è¯„ä¼°æ¯å±‚çš„ä¿¡æ¯ä¼ å¯¼èƒ½åŠ›'),
            'dropout_uncertainty': ('Dropoutä¸ç¡®å®šæ€§', 'é€šè¿‡é¢„æµ‹æ–¹å·®è¡¡é‡å±‚çš„ä¸ç¡®å®šæ€§è´¡çŒ®'),
            'activation_patching': ('æ¿€æ´»ä¿®è¡¥åˆ†æ', 'é€šè¿‡æ›¿æ¢æ¿€æ´»è¯„ä¼°å› æœé‡è¦æ€§'),
            'parameter_influence': ('å‚æ•°å½±å“æŒ‡æ•°', 'åŸºäºå‚æ•°å€¼å’Œæ¢¯åº¦çš„å½±å“åŠ›åˆ†æ')
        }
        
        for method, (name, description) in method_details.items():
            if method in analyses and isinstance(analyses[method], dict):
                scores = analyses[method]
                if scores:
                    report += f"""
### {name}

**æ–¹æ³•æè¿°**: {description}

**ç»“æœæ¦‚è§ˆ**:
"""
                    sorted_method_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)
                    top_3 = sorted_method_results[:3]
                    bottom_3 = sorted_method_results[-3:]
                    
                    report += f"- æœ€é‡è¦å±‚: å±‚{top_3[0][0]} (è¯„åˆ†: {top_3[0][1]:.4f})\n"
                    report += f"- TOP3å±‚: {[f'å±‚{l}({s:.3f})' for l, s in top_3]}\n"
                    report += f"- æœ€ä¸é‡è¦å±‚: å±‚{bottom_3[0][0]} (è¯„åˆ†: {bottom_3[0][1]:.4f})\n"
                    report += f"- è¯„åˆ†èŒƒå›´: {min(scores.values()):.4f} - {max(scores.values()):.4f}\n"
    
    report += """
## åˆ†æç»“è®º

### å±‚é‡è¦æ€§æ¨¡å¼

1. **å…³é”®å±‚è¯†åˆ«**: åŸºäºç»¼åˆåˆ†æï¼Œç¡®å®šäº†å¯¹æ¨¡å‹æ€§èƒ½æœ€å…³é”®çš„å±‚
2. **åˆ†å¸ƒç‰¹å¾**: é‡è¦æ€§åœ¨å±‚é—´çš„åˆ†å¸ƒæ¨¡å¼å’Œè§„å¾‹
3. **æ–¹æ³•ä¸€è‡´æ€§**: ä¸åŒåˆ†ææ–¹æ³•ç»“æœçš„ä¸€è‡´æ€§ç¨‹åº¦

### æ¨¡å‹å‹ç¼©å»ºè®®

1. **ä¿ç•™ç­–ç•¥**: å»ºè®®ä¿ç•™ç»¼åˆè¯„åˆ†æœ€é«˜çš„å±‚
2. **å‹ç¼©æ¯”ä¾‹**: åŸºäºé‡è¦æ€§åˆ†å¸ƒå»ºè®®çš„å®‰å…¨å‹ç¼©æ¯”ä¾‹
3. **æ€§èƒ½é¢„æœŸ**: é¢„æœŸçš„æ€§èƒ½ä¿æŒæ°´å¹³

### æŠ€æœ¯è´¡çŒ®

1. **æ–¹æ³•å…¨é¢æ€§**: é¦–æ¬¡é›†æˆ9ç§ä¸»æµå±‚é‡è¦æ€§åˆ†ææ–¹æ³•
2. **ç»“æœå¯é æ€§**: åŸºäºçœŸå®å¤§è§„æ¨¡æ•°æ®çš„å¯é éªŒè¯
3. **å®ç”¨ä»·å€¼**: ä¸ºå®é™…æ¨¡å‹éƒ¨ç½²æä¾›ç§‘å­¦ä¾æ®

æœ¬åˆ†æä¸ºTransformeræ¨¡å‹çš„å±‚çº§ä¼˜åŒ–æä¾›äº†å…¨é¢ã€å¯é çš„ç§‘å­¦ä¾æ®ã€‚
"""
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)

if __name__ == "__main__":
    # è¿è¡Œå…¨é¢å®éªŒ
    results = run_comprehensive_experiment()
    
    # å¯é€‰ï¼šLlama3åˆ†æ
    if LLAMA_AVAILABLE:
        try:
            logger.info("ğŸ¦™ å°è¯•Llama3åˆ†æ...")
            llama_analyzer = LlamaLayerAnalyzer()
            if llama_analyzer.load_llama_model():
                # è¿™é‡Œå¯ä»¥æ·»åŠ Llama3åˆ†æé€»è¾‘
                pass
        except Exception as e:
            logger.warning(f"Llama3åˆ†æè·³è¿‡: {e}")
    
    # å¯é€‰ï¼šGPT-4åˆ†æ
    try:
        gpt4_api_key = "YOUR_API_KEY_HERE"
        gpt4_analyzer = GPT4LayerAnalyzer(gpt4_api_key)
        
        # è·å–æ–‡æœ¬æ ·æœ¬
        data_loader = HonestDataLoader()
        sample_texts, _, _ = data_loader.load_real_amazon_data(max_samples=100)
        
        gpt4_results = gpt4_analyzer.analyze_with_gpt4(results, sample_texts)
        
        if 'gpt4_analysis' in gpt4_results:
            logger.info("ğŸ¤– GPT-4åˆ†æå®Œæˆ")
            # ä¿å­˜GPT-4åˆ†æç»“æœ
            gpt4_file = Path("results/comprehensive_analysis") / f"gpt4_analysis_{results['timestamp']}.md"
            with open(gpt4_file, 'w', encoding='utf-8') as f:
                f.write(f"# GPT-4ä¸“å®¶åˆ†æ\n\n{gpt4_results['gpt4_analysis']}")
        
    except Exception as e:
        logger.warning(f"GPT-4åˆ†æè·³è¿‡: {e}")
    
    logger.info("ğŸ¯ æ‰€æœ‰åˆ†æå®Œæˆ!")
