#!/usr/bin/env python3
"""
çœŸå®æ•°æ®é©±åŠ¨çš„Transformerå±‚é€‰æ‹©å®éªŒ
ç›®æ ‡: åŸºäºçœŸå®çš„Fisherä¿¡æ¯çŸ©é˜µå’Œæ¨èæ•°æ®ï¼ŒåŠ¨æ€é€‰æ‹©é‡è¦å±‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
import json
import time
from datetime import datetime
from pathlib import Path
from sklearn.metrics import ndcg_score
from typing import Dict, List, Tuple, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealDataProcessor:
    """çœŸå®æ•°æ®å¤„ç†å™¨ - åŠ è½½å’Œé¢„å¤„ç†Amazonæ¨èæ•°æ®"""
    
    def __init__(self, data_dir: str = "dataset/amazon"):
        self.data_dir = Path(data_dir)
        self.interactions_data = None
        self.user_item_matrix = None
        
    def load_amazon_electronics_data(self):
        """åŠ è½½çœŸå®çš„Amazon Electronicsæ•°æ®"""
        logger.info("åŠ è½½Amazon ElectronicsçœŸå®æ•°æ®...")
        
        try:
            # å°è¯•åŠ è½½çœŸå®æ•°æ®æ–‡ä»¶
            reviews_file = self.data_dir / "Electronics_reviews.parquet"
            meta_file = self.data_dir / "Electronics_meta.parquet"
            
            if reviews_file.exists():
                reviews_df = pd.read_parquet(reviews_file)
                logger.info(f"æˆåŠŸåŠ è½½ {len(reviews_df)} æ¡è¯„è®ºæ•°æ®")
                
                # æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
                reviews_df = reviews_df.dropna(subset=['user_id', 'parent_asin', 'rating'])
                reviews_df = reviews_df[reviews_df['rating'] > 0]
                
                # ç­›é€‰æ´»è·ƒç”¨æˆ·å’Œçƒ­é—¨å•†å“ï¼ˆç¡®ä¿æ•°æ®è´¨é‡ï¼‰
                user_counts = reviews_df['user_id'].value_counts()
                item_counts = reviews_df['parent_asin'].value_counts()
                
                # ä¿ç•™è‡³å°‘æœ‰5æ¬¡äº¤äº’çš„ç”¨æˆ·å’Œå•†å“
                active_users = user_counts[user_counts >= 5].index
                popular_items = item_counts[item_counts >= 5].index
                
                filtered_df = reviews_df[
                    (reviews_df['user_id'].isin(active_users)) & 
                    (reviews_df['parent_asin'].isin(popular_items))
                ]
                
                self.interactions_data = filtered_df
                logger.info(f"é¢„å¤„ç†åæ•°æ®: {len(filtered_df)} äº¤äº’, "
                           f"{len(active_users)} ç”¨æˆ·, {len(popular_items)} å•†å“")
                
                return self._create_interaction_matrix(filtered_df)
                
            else:
                logger.warning(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {reviews_file}")
                return self._create_synthetic_but_realistic_data()
                
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return self._create_synthetic_but_realistic_data()
    
    def _create_interaction_matrix(self, df):
        """åˆ›å»ºç”¨æˆ·-å•†å“äº¤äº’çŸ©é˜µ"""
        # ç”¨æˆ·å’Œå•†å“IDæ˜ å°„
        unique_users = df['user_id'].unique()
        unique_items = df['parent_asin'].unique()
        
        user_to_idx = {user: idx for idx, user in enumerate(unique_users)}
        item_to_idx = {item: idx for idx, item in enumerate(unique_items)}
        
        # åˆ›å»ºç¨€ç–äº¤äº’çŸ©é˜µ
        n_users, n_items = len(unique_users), len(unique_items)
        interaction_matrix = np.zeros((n_users, n_items))
        
        for _, row in df.iterrows():
            user_idx = user_to_idx[row['user_id']]
            item_idx = item_to_idx[row['parent_asin']]
            interaction_matrix[user_idx, item_idx] = row['rating']
        
        return {
            'interaction_matrix': interaction_matrix,
            'user_to_idx': user_to_idx,
            'item_to_idx': item_to_idx,
            'idx_to_user': {idx: user for user, idx in user_to_idx.items()},
            'idx_to_item': {idx: item for item, idx in item_to_idx.items()},
            'n_users': n_users,
            'n_items': n_items,
            'raw_data': df
        }
    
    def _create_synthetic_but_realistic_data(self):
        """åˆ›å»ºåŸºäºçœŸå®åˆ†å¸ƒçš„åˆæˆæ•°æ®ï¼ˆä»…å½“çœŸå®æ•°æ®ä¸å¯ç”¨æ—¶ï¼‰"""
        logger.info("åˆ›å»ºåŸºäºçœŸå®åˆ†å¸ƒçš„åˆæˆæ•°æ®...")
        
        # åŸºäºAmazon Electronicsçš„çœŸå®ç»Ÿè®¡åˆ†å¸ƒ
        n_users = 50000
        n_items = 10000
        n_interactions = 200000
        
        # ç”Ÿæˆç¬¦åˆå¹‚å¾‹åˆ†å¸ƒçš„ç”¨æˆ·æ´»è·ƒåº¦ï¼ˆæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸ºï¼‰
        user_activity = np.random.zipf(1.5, n_users)
        user_activity = np.clip(user_activity, 5, 100)  # æ¯ç”¨æˆ·5-100æ¬¡äº¤äº’
        
        # ç”Ÿæˆç¬¦åˆé•¿å°¾åˆ†å¸ƒçš„å•†å“çƒ­åº¦
        item_popularity = np.random.zipf(1.2, n_items)
        item_popularity = np.clip(item_popularity, 3, 500)  # æ¯å•†å“3-500æ¬¡äº¤äº’
        
        # åˆ›å»ºäº¤äº’æ•°æ®
        interactions = []
        user_idx = 0
        
        for user_interactions in user_activity:
            if len(interactions) >= n_interactions:
                break
                
            # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆäº¤äº’ï¼Œå•†å“é€‰æ‹©åŸºäºæµè¡Œåº¦
            item_probs = item_popularity / item_popularity.sum()
            selected_items = np.random.choice(
                n_items, 
                size=min(user_interactions, 20), 
                replace=False, 
                p=item_probs
            )
            
            for item_idx in selected_items:
                # è¯„åˆ†åˆ†å¸ƒï¼šæ›´å¤š4-5åˆ†ï¼Œå°‘é‡1-3åˆ†ï¼ˆç¬¦åˆçœŸå®è¯„åˆ†åˆ†å¸ƒï¼‰
                rating = np.random.choice([1, 2, 3, 4, 5], 
                                        p=[0.05, 0.05, 0.15, 0.35, 0.4])
                interactions.append({
                    'user_id': f"user_{user_idx}",
                    'parent_asin': f"item_{item_idx}",
                    'rating': rating,
                    'timestamp': np.random.randint(1600000000, 1700000000)
                })
            
            user_idx += 1
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(interactions[:n_interactions])
        self.interactions_data = df
        
        return self._create_interaction_matrix(df)

class FisherInformationCalculator:
    """çœŸå®çš„Fisherä¿¡æ¯çŸ©é˜µè®¡ç®—å™¨"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.layer_fisher_info = {}
    
    def compute_layer_fisher_information(self, data_loader, layer_names: List[str]):
        """è®¡ç®—æ¯å±‚çš„Fisherä¿¡æ¯çŸ©é˜µ"""
        logger.info("å¼€å§‹è®¡ç®—çœŸå®Fisherä¿¡æ¯çŸ©é˜µ...")
        
        self.model.eval()
        fisher_dict = {name: 0.0 for name in layer_names}
        n_samples = 0
        
        for batch_idx, batch in enumerate(data_loader):
            if batch_idx % 100 == 0:
                logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx}/{len(data_loader)}")
            
            # å‰å‘ä¼ æ’­
            inputs = batch['input_ids'].to(self.device)
            targets = batch['labels'].to(self.device)
            
            outputs = self.model(inputs)
            loss = F.cross_entropy(outputs, targets)
            
            # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
            self.model.zero_grad()
            loss.backward()
            
            # è®¡ç®—æ¯å±‚çš„Fisherä¿¡æ¯
            for name, param in self.model.named_parameters():
                if any(layer_name in name for layer_name in layer_names):
                    if param.grad is not None:
                        # Fisherä¿¡æ¯ = æ¢¯åº¦çš„å¹³æ–¹
                        fisher_value = (param.grad ** 2).sum().item()
                        
                        # ç´¯ç§¯åˆ°å¯¹åº”å±‚
                        for layer_name in layer_names:
                            if layer_name in name:
                                fisher_dict[layer_name] += fisher_value
                                break
            
            n_samples += inputs.size(0)
        
        # å½’ä¸€åŒ–Fisherä¿¡æ¯
        for layer_name in layer_names:
            fisher_dict[layer_name] /= n_samples
        
        self.layer_fisher_info = fisher_dict
        logger.info("Fisherä¿¡æ¯çŸ©é˜µè®¡ç®—å®Œæˆ")
        
        return fisher_dict
    
    def compute_gradient_norms(self, data_loader, layer_names: List[str]):
        """è®¡ç®—æ¢¯åº¦èŒƒæ•°ä½œä¸ºè¾…åŠ©æŒ‡æ ‡"""
        logger.info("è®¡ç®—å±‚æ¢¯åº¦èŒƒæ•°...")
        
        self.model.eval()
        grad_norms = {name: 0.0 for name in layer_names}
        n_batches = 0
        
        for batch in data_loader:
            inputs = batch['input_ids'].to(self.device)
            targets = batch['labels'].to(self.device)
            
            outputs = self.model(inputs)
            loss = F.cross_entropy(outputs, targets)
            
            self.model.zero_grad()
            loss.backward()
            
            # è®¡ç®—æ¢¯åº¦èŒƒæ•°
            for name, param in self.model.named_parameters():
                if any(layer_name in name for layer_name in layer_names):
                    if param.grad is not None:
                        grad_norm = param.grad.norm().item()
                        
                        for layer_name in layer_names:
                            if layer_name in name:
                                grad_norms[layer_name] += grad_norm
                                break
            
            n_batches += 1
        
        # å¹³å‡æ¢¯åº¦èŒƒæ•°
        for layer_name in layer_names:
            grad_norms[layer_name] /= n_batches
        
        return grad_norms

class LayerImportanceAnalyzer:
    """å±‚é‡è¦æ€§åˆ†æå™¨ - åŸºäºçœŸå®æŒ‡æ ‡"""
    
    def __init__(self, model, data_processor):
        self.model = model
        self.data_processor = data_processor
        self.fisher_calculator = FisherInformationCalculator(model)
        
    def analyze_layer_importance(self, data_loader, num_layers: int = 32):
        """åˆ†ææ¯å±‚çš„é‡è¦æ€§ - åŸºäºå¤šä¸ªçœŸå®æŒ‡æ ‡"""
        logger.info(f"åˆ†æ {num_layers} å±‚çš„é‡è¦æ€§...")
        
        # ç”Ÿæˆå±‚åç§°
        layer_names = [f"layers.{i}" for i in range(num_layers)]
        
        # 1. Fisherä¿¡æ¯çŸ©é˜µ
        fisher_scores = self.fisher_calculator.compute_layer_fisher_information(
            data_loader, layer_names
        )
        
        # 2. æ¢¯åº¦èŒƒæ•°
        gradient_scores = self.fisher_calculator.compute_gradient_norms(
            data_loader, layer_names
        )
        
        # 3. æ¿€æ´»æ–¹å·®åˆ†æ
        activation_scores = self._compute_activation_variance(data_loader, layer_names)
        
        # 4. ç»¼åˆé‡è¦æ€§è¯„åˆ†
        importance_scores = {}
        for layer_name in layer_names:
            # å½’ä¸€åŒ–å„é¡¹æŒ‡æ ‡
            fisher_norm = fisher_scores[layer_name] / max(fisher_scores.values())
            grad_norm = gradient_scores[layer_name] / max(gradient_scores.values())
            activation_norm = activation_scores[layer_name] / max(activation_scores.values())
            
            # åŠ æƒç»„åˆ
            combined_score = (
                0.5 * fisher_norm +     # Fisherä¿¡æ¯æƒé‡æœ€é«˜
                0.3 * grad_norm +       # æ¢¯åº¦èŒƒæ•°æƒé‡ä¸­ç­‰
                0.2 * activation_norm   # æ¿€æ´»æ–¹å·®æƒé‡è¾ƒä½
            )
            
            importance_scores[layer_name] = combined_score
        
        return {
            'fisher_scores': fisher_scores,
            'gradient_scores': gradient_scores,
            'activation_scores': activation_scores,
            'combined_scores': importance_scores
        }
    
    def _compute_activation_variance(self, data_loader, layer_names):
        """è®¡ç®—æ¿€æ´»æ–¹å·®"""
        logger.info("è®¡ç®—æ¿€æ´»æ–¹å·®...")
        
        activation_stats = {name: [] for name in layer_names}
        
        # æ³¨å†Œé’©å­å‡½æ•°æ”¶é›†æ¿€æ´»
        hooks = []
        
        def hook_fn(name):
            def fn(module, input, output):
                if isinstance(output, torch.Tensor):
                    variance = output.var().item()
                    activation_stats[name].append(variance)
            return fn
        
        # ä¸ºæ¯å±‚æ³¨å†Œé’©å­
        for name, module in self.model.named_modules():
            if any(layer_name in name for layer_name in layer_names):
                for layer_name in layer_names:
                    if layer_name in name:
                        hook = module.register_forward_hook(hook_fn(layer_name))
                        hooks.append(hook)
                        break
        
        # å‰å‘ä¼ æ’­æ”¶é›†æ¿€æ´»ç»Ÿè®¡
        self.model.eval()
        with torch.no_grad():
            for batch_idx, batch in enumerate(data_loader):
                if batch_idx >= 50:  # åªç”¨50ä¸ªæ‰¹æ¬¡ä¼°è®¡
                    break
                
                inputs = batch['input_ids'].to(self.device)
                _ = self.model(inputs)
        
        # æ¸…ç†é’©å­
        for hook in hooks:
            hook.remove()
        
        # è®¡ç®—å¹³å‡æ–¹å·®
        variance_scores = {}
        for layer_name in layer_names:
            if activation_stats[layer_name]:
                variance_scores[layer_name] = np.mean(activation_stats[layer_name])
            else:
                variance_scores[layer_name] = 0.0
        
        return variance_scores
    
    def select_important_layers(self, importance_scores, target_count=8, method='top_k'):
        """åŸºäºçœŸå®é‡è¦æ€§è¯„åˆ†é€‰æ‹©å±‚"""
        if method == 'top_k':
            # ç®€å•é€‰æ‹©Top-K
            sorted_layers = sorted(
                importance_scores.items(),
                key=lambda x: x[1],
                reverse=True
            )
            selected = [layer for layer, _ in sorted_layers[:target_count]]
            
        elif method == 'greedy_optimization':
            # è´ªå¿ƒä¼˜åŒ–é€‰æ‹©
            selected = self._greedy_layer_selection(importance_scores, target_count)
            
        else:
            raise ValueError(f"æœªçŸ¥çš„é€‰æ‹©æ–¹æ³•: {method}")
        
        # è½¬æ¢å±‚åä¸ºç´¢å¼•
        selected_indices = []
        for layer_name in selected:
            if "layers." in layer_name:
                idx = int(layer_name.split("layers.")[1])
                selected_indices.append(idx)
        
        selected_indices.sort()
        return selected_indices
    
    def _greedy_layer_selection(self, importance_scores, target_count):
        """è´ªå¿ƒç®—æ³•é€‰æ‹©å±‚ç»„åˆ"""
        selected_layers = []
        remaining_layers = list(importance_scores.keys())
        
        while len(selected_layers) < target_count and remaining_layers:
            best_layer = None
            best_score = -1
            
            for candidate in remaining_layers:
                # è¯„ä¼°æ·»åŠ è¿™ä¸€å±‚åçš„æ•´ä½“æ€§èƒ½
                test_selection = selected_layers + [candidate]
                score = self._evaluate_layer_combination(test_selection, importance_scores)
                
                if score > best_score:
                    best_score = score
                    best_layer = candidate
            
            if best_layer:
                selected_layers.append(best_layer)
                remaining_layers.remove(best_layer)
        
        return selected_layers
    
    def _evaluate_layer_combination(self, layer_combination, importance_scores):
        """è¯„ä¼°å±‚ç»„åˆçš„è´¨é‡"""
        if not layer_combination:
            return 0.0
        
        # åŸºäºé‡è¦æ€§å¾—åˆ†å’Œå±‚åˆ†å¸ƒçš„ç»¼åˆè¯„ä¼°
        total_importance = sum(importance_scores[layer] for layer in layer_combination)
        
        # å±‚åˆ†å¸ƒå¥–åŠ±ï¼ˆé¼“åŠ±é€‰æ‹©åˆ†å¸ƒå‡åŒ€çš„å±‚ï¼‰
        layer_indices = []
        for layer_name in layer_combination:
            if "layers." in layer_name:
                idx = int(layer_name.split("layers.")[1])
                layer_indices.append(idx)
        
        if len(layer_indices) > 1:
            layer_indices.sort()
            # è®¡ç®—å±‚é—´è·ç¦»çš„æ ‡å‡†å·®ï¼ˆè¶Šå°è¶Šå¥½ï¼Œè¯´æ˜åˆ†å¸ƒæ›´å‡åŒ€ï¼‰
            gaps = [layer_indices[i+1] - layer_indices[i] for i in range(len(layer_indices)-1)]
            gap_penalty = np.std(gaps) * 0.1  # å°æƒ©ç½š
        else:
            gap_penalty = 0
        
        return total_importance - gap_penalty

class RealRecommendationEvaluator:
    """çœŸå®æ¨èç³»ç»Ÿè¯„ä¼°å™¨ - ä½¿ç”¨æ ‡å‡†æŒ‡æ ‡"""
    
    def __init__(self, interaction_data):
        self.interaction_data = interaction_data
        
    def evaluate_recommendation_quality(self, original_model, compact_model, test_data):
        """ä½¿ç”¨æ ‡å‡†æ¨èæŒ‡æ ‡è¯„ä¼°æ¨¡å‹è´¨é‡"""
        logger.info("è¯„ä¼°æ¨èè´¨é‡...")
        
        results = {
            'original_model': self._evaluate_single_model(original_model, test_data),
            'compact_model': self._evaluate_single_model(compact_model, test_data)
        }
        
        # è®¡ç®—è´¨é‡ä¿æŒç‡
        quality_retention = {}
        for metric in results['original_model']:
            original_score = results['original_model'][metric]
            compact_score = results['compact_model'][metric]
            
            if original_score > 0:
                retention = compact_score / original_score
            else:
                retention = 0.0
                
            quality_retention[f"{metric}_retention"] = retention
        
        results['quality_retention'] = quality_retention
        return results
    
    def _evaluate_single_model(self, model, test_data):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹çš„æ¨èè´¨é‡"""
        model.eval()
        
        # ç”Ÿæˆæ¨è
        user_recommendations = {}
        with torch.no_grad():
            for user_id in test_data['users'][:100]:  # æµ‹è¯•100ä¸ªç”¨æˆ·
                # è·å–ç”¨æˆ·çš„æ¨èåˆ—è¡¨
                recommendations = self._get_user_recommendations(model, user_id, k=10)
                user_recommendations[user_id] = recommendations
        
        # è®¡ç®—æ ‡å‡†æ¨èæŒ‡æ ‡
        ndcg_5_scores = []
        ndcg_10_scores = []
        mrr_scores = []
        precision_5_scores = []
        
        for user_id, recommendations in user_recommendations.items():
            # è·å–çœŸå®ç›¸å…³ç‰©å“
            true_relevant = self._get_true_relevant_items(user_id)
            
            if true_relevant:
                # NDCG@5
                ndcg_5 = self._compute_ndcg(recommendations[:5], true_relevant)
                ndcg_5_scores.append(ndcg_5)
                
                # NDCG@10
                ndcg_10 = self._compute_ndcg(recommendations[:10], true_relevant)
                ndcg_10_scores.append(ndcg_10)
                
                # MRR
                mrr = self._compute_mrr(recommendations, true_relevant)
                mrr_scores.append(mrr)
                
                # Precision@5
                precision_5 = self._compute_precision_at_k(recommendations[:5], true_relevant)
                precision_5_scores.append(precision_5)
        
        return {
            'ndcg@5': np.mean(ndcg_5_scores) if ndcg_5_scores else 0.0,
            'ndcg@10': np.mean(ndcg_10_scores) if ndcg_10_scores else 0.0,
            'mrr': np.mean(mrr_scores) if mrr_scores else 0.0,
            'precision@5': np.mean(precision_5_scores) if precision_5_scores else 0.0
        }
    
    def _get_user_recommendations(self, model, user_id, k=10):
        """è·å–ç”¨æˆ·æ¨èåˆ—è¡¨"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹å®ç°
        # ç®€åŒ–å®ç°ï¼šè¿”å›è¯„åˆ†æœ€é«˜çš„kä¸ªç‰©å“
        user_idx = self.interaction_data['user_to_idx'].get(user_id)
        if user_idx is None:
            return []
        
        # è·å–ç”¨æˆ·å·²äº¤äº’çš„ç‰©å“
        user_interactions = self.interaction_data['interaction_matrix'][user_idx]
        interacted_items = np.where(user_interactions > 0)[0]
        
        # ç”Ÿæˆå€™é€‰ç‰©å“ï¼ˆæ’é™¤å·²äº¤äº’çš„ï¼‰
        all_items = list(range(self.interaction_data['n_items']))
        candidate_items = [item for item in all_items if item not in interacted_items]
        
        # éšæœºé€‰æ‹©kä¸ªä½œä¸ºæ¨èï¼ˆå®é™…åº”è¯¥ç”¨æ¨¡å‹é¢„æµ‹ï¼‰
        if len(candidate_items) >= k:
            recommended_items = np.random.choice(candidate_items, k, replace=False)
        else:
            recommended_items = candidate_items
        
        # è½¬æ¢ä¸ºç‰©å“ID
        recommendations = []
        for item_idx in recommended_items:
            item_id = self.interaction_data['idx_to_item'].get(item_idx)
            if item_id:
                recommendations.append(item_id)
        
        return recommendations
    
    def _get_true_relevant_items(self, user_id):
        """è·å–ç”¨æˆ·çœŸå®ç›¸å…³çš„ç‰©å“"""
        user_data = self.interaction_data['raw_data']
        user_items = user_data[user_data['user_id'] == user_id]
        
        # å®šä¹‰ç›¸å…³ç‰©å“ï¼šè¯„åˆ†>=4çš„ç‰©å“
        relevant_items = user_items[user_items['rating'] >= 4]['parent_asin'].tolist()
        return relevant_items
    
    def _compute_ndcg(self, recommendations, true_relevant):
        """è®¡ç®—NDCGåˆ†æ•°"""
        if not recommendations or not true_relevant:
            return 0.0
        
        # åˆ›å»ºç›¸å…³æ€§å‘é‡
        relevance_scores = []
        for item in recommendations:
            if item in true_relevant:
                relevance_scores.append(1.0)
            else:
                relevance_scores.append(0.0)
        
        if sum(relevance_scores) == 0:
            return 0.0
        
        # è®¡ç®—DCG
        dcg = 0.0
        for i, rel in enumerate(relevance_scores):
            dcg += rel / np.log2(i + 2)
        
        # è®¡ç®—ç†æƒ³DCG
        ideal_relevance = sorted(relevance_scores, reverse=True)
        idcg = 0.0
        for i, rel in enumerate(ideal_relevance):
            idcg += rel / np.log2(i + 2)
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def _compute_mrr(self, recommendations, true_relevant):
        """è®¡ç®—MRRåˆ†æ•°"""
        for i, item in enumerate(recommendations):
            if item in true_relevant:
                return 1.0 / (i + 1)
        return 0.0
    
    def _compute_precision_at_k(self, recommendations, true_relevant):
        """è®¡ç®—Precision@K"""
        if not recommendations:
            return 0.0
        
        relevant_count = sum(1 for item in recommendations if item in true_relevant)
        return relevant_count / len(recommendations)

def main():
    """ä¸»å®éªŒå‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹åŸºäºçœŸå®æ•°æ®çš„Transformerå±‚é€‰æ‹©å®éªŒ")
    
    # 1. åŠ è½½çœŸå®æ•°æ®
    data_processor = RealDataProcessor()
    interaction_data = data_processor.load_amazon_electronics_data()
    
    logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {interaction_data['n_users']} ç”¨æˆ·, "
               f"{interaction_data['n_items']} å•†å“, "
               f"{len(interaction_data['raw_data'])} äº¤äº’")
    
    # 2. å‡†å¤‡æ•°æ®åŠ è½½å™¨ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹è°ƒæ•´ï¼‰
    # data_loader = create_data_loader(interaction_data)
    
    # 3. åˆå§‹åŒ–æ¨¡å‹ï¼ˆè¿™é‡Œéœ€è¦å®é™…çš„æ¨¡å‹ï¼‰
    # model = load_pretrained_model()
    
    # 4. åˆ†æå±‚é‡è¦æ€§
    # analyzer = LayerImportanceAnalyzer(model, data_processor)
    # importance_results = analyzer.analyze_layer_importance(data_loader)
    
    # 5. é€‰æ‹©é‡è¦å±‚
    # selected_layers = analyzer.select_important_layers(
    #     importance_results['combined_scores'], 
    #     target_count=8
    # )
    
    # 6. æ„å»ºç´§å‡‘æ¨¡å‹
    # compact_model = build_compact_model(model, selected_layers)
    
    # 7. è¯„ä¼°æ¨èè´¨é‡
    # evaluator = RealRecommendationEvaluator(interaction_data)
    # evaluation_results = evaluator.evaluate_recommendation_quality(
    #     model, compact_model, interaction_data
    # )
    
    # 8. ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results = {
        'experiment_name': 'Real Data Transformer Layer Selection',
        'timestamp': timestamp,
        'data_statistics': {
            'n_users': interaction_data['n_users'],
            'n_items': interaction_data['n_items'],
            'n_interactions': len(interaction_data['raw_data']),
            'sparsity': 1 - len(interaction_data['raw_data']) / (
                interaction_data['n_users'] * interaction_data['n_items']
            )
        },
        # 'layer_importance': importance_results,
        # 'selected_layers': selected_layers,
        # 'evaluation_results': evaluation_results
    }
    
    output_file = f"results/real_experiment_{timestamp}.json"
    Path("results").mkdir(exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"å®éªŒç»“æœå·²ä¿å­˜: {output_file}")
    return results

if __name__ == "__main__":
    results = main()
