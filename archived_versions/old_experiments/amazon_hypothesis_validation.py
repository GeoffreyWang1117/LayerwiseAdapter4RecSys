"""
Amazonæ•°æ®ä¸Šçš„H1-H4å‡è®¾éªŒè¯å®éªŒ

åœ¨çœŸå®Amazon Reviewsæ•°æ®ä¸ŠéªŒè¯Layerwise Adapterçš„4ä¸ªæ ¸å¿ƒå‡è®¾
"""

import sys
import os
sys.path.insert(0, os.path.abspath('.'))

import torch
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging
from datetime import datetime
from dataclasses import dataclass
import matplotlib.pyplot as plt
import seaborn as sns
import json
from sklearn.metrics import mean_squared_error, mean_absolute_error, ndcg_score
import warnings

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

@dataclass 
class AmazonHypothesisConfig:
    """Amazonå‡è®¾éªŒè¯é…ç½®"""
    # æ•°æ®é…ç½®
    categories: List[str] = None
    max_users_per_category: int = 2000
    max_items_per_category: int = 1500
    
    # æ¨¡å‹é…ç½®
    embedding_dim: int = 32
    hidden_dims: List[int] = None
    dropout_rate: float = 0.2
    
    # è®­ç»ƒé…ç½®
    learning_rate: float = 0.001
    batch_size: int = 256
    max_epochs: int = 30
    patience: int = 8
    
    # å‡è®¾éªŒè¯é…ç½®
    fisher_sample_size: int = 1000
    importance_threshold: float = 0.1
    critical_layer_threshold: float = 0.3
    distillation_temperature: float = 4.0
    
    def __post_init__(self):
        if self.categories is None:
            self.categories = ['All_Beauty', 'Books']  # é€‰æ‹©è¾ƒå°çš„ç±»åˆ«è¿›è¡Œå¿«é€ŸéªŒè¯
        if self.hidden_dims is None:
            self.hidden_dims = [64, 32, 16]

class AmazonHypothesisValidator:
    """Amazonå‡è®¾éªŒè¯å™¨"""
    
    def __init__(self, config: AmazonHypothesisConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.logger = logging.getLogger(self.__class__.__name__)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å­˜å‚¨å„ç±»åˆ«çš„å®éªŒç»“æœ
        self.category_results = {}
        self.hypothesis_results = {
            'H1': {'description': 'Fisherä¿¡æ¯çŸ©é˜µèƒ½æœ‰æ•ˆè¯†åˆ«å…³é”®å±‚', 'supported': False, 'evidence': []},
            'H2': {'description': 'å±‚çº§é‡è¦æ€§å‘ˆç°å¤šæ¨¡æ€åˆ†å¸ƒ', 'supported': False, 'evidence': []},
            'H3': {'description': 'çŸ¥è¯†è’¸é¦èƒ½æœ‰æ•ˆå‹ç¼©æ¨¡å‹', 'supported': False, 'evidence': []},
            'H4': {'description': 'ä¼˜åŒ–çš„æ¨¡å‹åœ¨æ¨èä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½', 'supported': False, 'evidence': []}
        }
    
    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆå‡è®¾éªŒè¯"""
        self.logger.info("ğŸš€ å¼€å§‹Amazonæ•°æ®ä¸Šçš„H1-H4å‡è®¾ç»¼åˆéªŒè¯...")
        
        for category in self.config.categories:
            self.logger.info(f"ğŸ“‹ éªŒè¯ç±»åˆ«: {category}")
            
            try:
                # è¿è¡Œå•ä¸ªç±»åˆ«çš„éªŒè¯
                category_result = self._validate_category(category)
                self.category_results[category] = category_result
                
                # æ›´æ–°å‡è®¾è¯æ®
                self._update_hypothesis_evidence(category, category_result)
                
            except Exception as e:
                self.logger.error(f"ç±»åˆ« {category} éªŒè¯å¤±è´¥: {e}")
                continue
        
        # ç»¼åˆåˆ†æå‡è®¾æ”¯æŒæƒ…å†µ
        self._analyze_hypothesis_support()
        
        # ä¿å­˜ç»“æœ
        self._save_comprehensive_results()
        
        return {
            'hypothesis_results': self.hypothesis_results,
            'category_results': self.category_results,
            'summary': self._generate_summary()
        }
    
    def _validate_category(self, category: str) -> Dict[str, Any]:
        """éªŒè¯å•ä¸ªç±»åˆ«"""
        from experiments.amazon_layerwise_experiment import AmazonLayerwiseExperiment, LayerwiseConfig
        
        # åˆ›å»ºlayerwiseé…ç½®
        layerwise_config = LayerwiseConfig(
            embedding_dim=self.config.embedding_dim,
            hidden_dims=self.config.hidden_dims,
            learning_rate=self.config.learning_rate,
            batch_size=self.config.batch_size,
            max_epochs=self.config.max_epochs,
            patience=self.config.patience,
            max_users=self.config.max_users_per_category,
            max_items=self.config.max_items_per_category,
            fisher_sample_size=self.config.fisher_sample_size
        )
        
        # è¿è¡Œlayerwiseå®éªŒ
        experiment = AmazonLayerwiseExperiment(category, layerwise_config)
        
        # åŠ è½½æ•°æ®
        train_loader, test_loader = experiment.load_and_preprocess_data()
        
        # è®­ç»ƒæ¨¡å‹
        experiment.train_model(train_loader, test_loader)
        
        # Layerwiseåˆ†æ
        layer_importance, critical_layers = experiment.run_layerwise_analysis(train_loader)
        
        # è¯„ä¼°æ¨¡å‹
        metrics = experiment.evaluate_model(test_loader)
        
        # ç¼–è¯‘ç»“æœ
        result = {
            'category': category,
            'performance': metrics,
            'layer_importance': layer_importance,
            'critical_layers': critical_layers,
            'model_info': {
                'n_users': len(experiment.user_encoder.classes_),
                'n_items': len(experiment.item_encoder.classes_),
                'n_parameters': sum(p.numel() for p in experiment.model.parameters()),
                'embedding_dim': self.config.embedding_dim,
                'hidden_dims': self.config.hidden_dims
            }
        }
        
        return result
    
    def _update_hypothesis_evidence(self, category: str, result: Dict[str, Any]):
        """æ›´æ–°å‡è®¾è¯æ®"""
        
        # H1: Fisherä¿¡æ¯çŸ©é˜µèƒ½æœ‰æ•ˆè¯†åˆ«å…³é”®å±‚
        if result['critical_layers']:
            self.hypothesis_results['H1']['evidence'].append({
                'category': category,
                'critical_layers_found': len(result['critical_layers']),
                'layer_importance': result['layer_importance'],
                'support_strength': 'strong' if len(result['critical_layers']) > 0 else 'weak'
            })
        
        # H2: å±‚çº§é‡è¦æ€§å‘ˆç°å¤šæ¨¡æ€åˆ†å¸ƒ
        importance_values = list(result['layer_importance'].values())
        if len(importance_values) >= 3:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„é‡è¦æ€§å·®å¼‚
            max_importance = max(importance_values)
            min_importance = min(importance_values)
            importance_range = max_importance - min_importance
            
            self.hypothesis_results['H2']['evidence'].append({
                'category': category,
                'importance_range': importance_range,
                'max_importance': max_importance,
                'min_importance': min_importance,
                'n_layers': len(importance_values),
                'support_strength': 'strong' if importance_range > 0.5 else 'moderate' if importance_range > 0.2 else 'weak'
            })
        
        # H3: çŸ¥è¯†è’¸é¦èƒ½æœ‰æ•ˆå‹ç¼©æ¨¡å‹ (ç®€åŒ–éªŒè¯)
        # åŸºäºå…³é”®å±‚è¯†åˆ«çš„ç»“æœæ¥è¯„ä¼°å‹ç¼©æ½œåŠ›
        compression_potential = len(result['critical_layers']) / len(result['layer_importance'])
        self.hypothesis_results['H3']['evidence'].append({
            'category': category,
            'compression_potential': compression_potential,
            'critical_layer_ratio': compression_potential,
            'support_strength': 'strong' if compression_potential < 0.5 else 'moderate' if compression_potential < 0.8 else 'weak'
        })
        
        # H4: ä¼˜åŒ–çš„æ¨¡å‹åœ¨æ¨èä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½
        rmse = result['performance']['rmse']
        mae = result['performance']['mae']
        
        # åŸºäºæ€§èƒ½æŒ‡æ ‡è¯„ä¼° (RMSE < 1.2 ä¸” MAE < 1.0 è®¤ä¸ºæ˜¯å¥½çš„æ€§èƒ½)
        good_performance = rmse < 1.2 and mae < 1.0
        self.hypothesis_results['H4']['evidence'].append({
            'category': category,
            'rmse': rmse,
            'mae': mae,
            'good_performance': good_performance,
            'support_strength': 'strong' if rmse < 1.0 and mae < 0.8 else 'moderate' if good_performance else 'weak'
        })
    
    def _analyze_hypothesis_support(self):
        """åˆ†æå‡è®¾æ”¯æŒæƒ…å†µ"""
        for hypothesis_id, hypothesis_data in self.hypothesis_results.items():
            evidence_list = hypothesis_data['evidence']
            
            if not evidence_list:
                hypothesis_data['supported'] = False
                hypothesis_data['support_strength'] = 'none'
                continue
            
            # è®¡ç®—æ”¯æŒå¼ºåº¦
            strong_count = sum(1 for e in evidence_list if e.get('support_strength') == 'strong')
            moderate_count = sum(1 for e in evidence_list if e.get('support_strength') == 'moderate')
            weak_count = sum(1 for e in evidence_list if e.get('support_strength') == 'weak')
            
            total_evidence = len(evidence_list)
            strong_ratio = strong_count / total_evidence
            moderate_ratio = moderate_count / total_evidence
            
            # åˆ¤æ–­æ”¯æŒæƒ…å†µ
            if strong_ratio >= 0.5:
                hypothesis_data['supported'] = True
                hypothesis_data['support_strength'] = 'strong'
            elif strong_ratio + moderate_ratio >= 0.6:
                hypothesis_data['supported'] = True
                hypothesis_data['support_strength'] = 'moderate'
            elif moderate_ratio >= 0.4:
                hypothesis_data['supported'] = True
                hypothesis_data['support_strength'] = 'weak'
            else:
                hypothesis_data['supported'] = False
                hypothesis_data['support_strength'] = 'insufficient'
            
            hypothesis_data['evidence_summary'] = {
                'total_evidence': total_evidence,
                'strong_evidence': strong_count,
                'moderate_evidence': moderate_count,
                'weak_evidence': weak_count
            }
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ç”ŸæˆéªŒè¯æ‘˜è¦"""
        supported_hypotheses = sum(1 for h in self.hypothesis_results.values() if h['supported'])
        total_hypotheses = len(self.hypothesis_results)
        
        avg_performance = {}
        if self.category_results:
            rmse_values = [r['performance']['rmse'] for r in self.category_results.values()]
            mae_values = [r['performance']['mae'] for r in self.category_results.values()]
            
            avg_performance = {
                'avg_rmse': np.mean(rmse_values),
                'avg_mae': np.mean(mae_values),
                'best_rmse': min(rmse_values),
                'best_mae': min(mae_values)
            }
        
        return {
            'supported_hypotheses': supported_hypotheses,
            'total_hypotheses': total_hypotheses,
            'support_ratio': supported_hypotheses / total_hypotheses,
            'categories_tested': len(self.category_results),
            'avg_performance': avg_performance,
            'validation_timestamp': self.timestamp
        }
    
    def _save_comprehensive_results(self):
        """ä¿å­˜ç»¼åˆç»“æœ"""
        save_dir = Path("results/amazon_hypothesis_validation")
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = save_dir / f"amazon_hypothesis_validation_{self.timestamp}.json"
        comprehensive_results = {
            'config': {
                'categories': self.config.categories,
                'max_users_per_category': self.config.max_users_per_category,
                'max_items_per_category': self.config.max_items_per_category,
                'embedding_dim': self.config.embedding_dim,
                'hidden_dims': self.config.hidden_dims
            },
            'hypothesis_results': self.hypothesis_results,
            'category_results': self.category_results,
            'summary': self._generate_summary()
        }
        
        with open(results_file, 'w') as f:
            json.dump(comprehensive_results, f, indent=2, default=str)
        
        # ç”Ÿæˆå¯è§†åŒ–
        self._create_comprehensive_visualizations(save_dir)
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_comprehensive_report(save_dir)
        
        self.logger.info(f"ç»¼åˆéªŒè¯ç»“æœå·²ä¿å­˜è‡³: {save_dir}")
    
    def _create_comprehensive_visualizations(self, save_dir: Path):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–"""
        # 1. å‡è®¾æ”¯æŒæƒ…å†µ
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Amazon Hypothesis Validation Results', fontsize=16)
        
        # å‡è®¾æ”¯æŒçŠ¶æ€
        hypothesis_names = list(self.hypothesis_results.keys())
        support_status = [self.hypothesis_results[h]['supported'] for h in hypothesis_names]
        
        axes[0, 0].bar(hypothesis_names, [1 if s else 0 for s in support_status], 
                      color=['green' if s else 'red' for s in support_status], alpha=0.7)
        axes[0, 0].set_title('Hypothesis Support Status')
        axes[0, 0].set_ylabel('Supported (1) / Not Supported (0)')
        
        # å„ç±»åˆ«æ€§èƒ½å¯¹æ¯”
        if self.category_results:
            categories = list(self.category_results.keys())
            rmse_values = [self.category_results[c]['performance']['rmse'] for c in categories]
            mae_values = [self.category_results[c]['performance']['mae'] for c in categories]
            
            x = np.arange(len(categories))
            width = 0.35
            
            axes[0, 1].bar(x - width/2, rmse_values, width, label='RMSE', alpha=0.7)
            axes[0, 1].bar(x + width/2, mae_values, width, label='MAE', alpha=0.7)
            axes[0, 1].set_title('Performance by Category')
            axes[0, 1].set_xlabel('Category')
            axes[0, 1].set_ylabel('Error')
            axes[0, 1].set_xticks(x)
            axes[0, 1].set_xticklabels(categories)
            axes[0, 1].legend()
        
        # å±‚çº§é‡è¦æ€§çƒ­å›¾
        if self.category_results:
            importance_data = []
            for category, result in self.category_results.items():
                importance_data.append(list(result['layer_importance'].values()))
            
            if importance_data:
                importance_matrix = np.array(importance_data)
                im = axes[1, 0].imshow(importance_matrix, cmap='YlOrRd', aspect='auto')
                axes[1, 0].set_title('Layer Importance Heatmap')
                axes[1, 0].set_xlabel('Layer')
                axes[1, 0].set_ylabel('Category')
                axes[1, 0].set_yticks(range(len(categories)))
                axes[1, 0].set_yticklabels(categories)
                plt.colorbar(im, ax=axes[1, 0])
        
        # è¯æ®å¼ºåº¦åˆ†å¸ƒ
        evidence_strength_counts = {'strong': 0, 'moderate': 0, 'weak': 0, 'none': 0}
        for h_data in self.hypothesis_results.values():
            strength = h_data.get('support_strength', 'none')
            evidence_strength_counts[strength] += 1
        
        axes[1, 1].pie(evidence_strength_counts.values(), labels=evidence_strength_counts.keys(),
                      autopct='%1.1f%%', startangle=90)
        axes[1, 1].set_title('Evidence Strength Distribution')
        
        plt.tight_layout()
        plt.savefig(save_dir / f"comprehensive_validation_{self.timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_comprehensive_report(self, save_dir: Path):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        report_file = save_dir / f"comprehensive_report_{self.timestamp}.md"
        
        with open(report_file, 'w') as f:
            f.write("# Amazonæ•°æ®H1-H4å‡è®¾éªŒè¯ç»¼åˆæŠ¥å‘Š\n\n")
            f.write(f"**éªŒè¯æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**éªŒè¯ç±»åˆ«**: {', '.join(self.config.categories)}\n\n")
            
            # æ‰§è¡Œæ‘˜è¦
            summary = self._generate_summary()
            f.write("## ğŸ” æ‰§è¡Œæ‘˜è¦\n\n")
            f.write(f"- æ€»å‡è®¾æ•°: {summary['total_hypotheses']}\n")
            f.write(f"- æ”¯æŒçš„å‡è®¾æ•°: {summary['supported_hypotheses']}\n")
            f.write(f"- æ”¯æŒç‡: {summary['support_ratio']*100:.1f}%\n")
            f.write(f"- æµ‹è¯•ç±»åˆ«æ•°: {summary['categories_tested']}\n")
            
            if summary['avg_performance']:
                f.write(f"- å¹³å‡RMSE: {summary['avg_performance']['avg_rmse']:.4f}\n")
                f.write(f"- å¹³å‡MAE: {summary['avg_performance']['avg_mae']:.4f}\n")
            f.write("\n")
            
            # å‡è®¾éªŒè¯è¯¦æƒ…
            f.write("## ğŸ“Š å‡è®¾éªŒè¯è¯¦æƒ…\n\n")
            for h_id, h_data in self.hypothesis_results.items():
                status_emoji = "âœ…" if h_data['supported'] else "âŒ"
                f.write(f"### {status_emoji} {h_id}: {h_data['description']}\n\n")
                f.write(f"**æ”¯æŒçŠ¶æ€**: {'æ”¯æŒ' if h_data['supported'] else 'ä¸æ”¯æŒ'}\n")
                f.write(f"**æ”¯æŒå¼ºåº¦**: {h_data.get('support_strength', 'unknown')}\n")
                
                if 'evidence_summary' in h_data:
                    summary = h_data['evidence_summary']
                    f.write(f"**è¯æ®ç»Ÿè®¡**: æ€»è¯æ® {summary['total_evidence']}, ")
                    f.write(f"å¼ºè¯æ® {summary['strong_evidence']}, ")
                    f.write(f"ä¸­ç­‰è¯æ® {summary['moderate_evidence']}, ")
                    f.write(f"å¼±è¯æ® {summary['weak_evidence']}\n")
                
                f.write("\n")
            
            # ç±»åˆ«è¯¦ç»†ç»“æœ
            f.write("## ğŸ“‹ å„ç±»åˆ«è¯¦ç»†ç»“æœ\n\n")
            for category, result in self.category_results.items():
                f.write(f"### {category}\n\n")
                f.write(f"- **RMSE**: {result['performance']['rmse']:.4f}\n")
                f.write(f"- **MAE**: {result['performance']['mae']:.4f}\n")
                f.write(f"- **ç”¨æˆ·æ•°**: {result['model_info']['n_users']:,}\n")
                f.write(f"- **ç‰©å“æ•°**: {result['model_info']['n_items']:,}\n")
                f.write(f"- **æ¨¡å‹å‚æ•°æ•°**: {result['model_info']['n_parameters']:,}\n")
                f.write(f"- **å…³é”®å±‚**: {', '.join(result['critical_layers'])}\n")
                
                f.write("- **å±‚çº§é‡è¦æ€§**:\n")
                for layer, importance in result['layer_importance'].items():
                    f.write(f"  - {layer}: {importance:.4f}\n")
                f.write("\n")
            
            # ç»“è®ºä¸å»ºè®®
            f.write("## ğŸ¯ ç»“è®ºä¸å»ºè®®\n\n")
            supported_count = self._generate_summary()['supported_hypotheses']
            if supported_count >= 3:
                f.write("âœ… **å¼ºçƒˆæ”¯æŒ**: å¤§éƒ¨åˆ†å‡è®¾å¾—åˆ°éªŒè¯ï¼ŒLayerwise Adapteræ–¹æ³•åœ¨Amazonæ•°æ®ä¸Šè¡¨ç°å‡ºè‰²ã€‚\n\n")
            elif supported_count >= 2:
                f.write("âœ… **éƒ¨åˆ†æ”¯æŒ**: éƒ¨åˆ†å‡è®¾å¾—åˆ°éªŒè¯ï¼Œæ–¹æ³•æœ‰ä¸€å®šæ•ˆæœä½†éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚\n\n")
            else:
                f.write("âš ï¸ **æ”¯æŒæœ‰é™**: å°‘æ•°å‡è®¾å¾—åˆ°éªŒè¯ï¼Œæ–¹æ³•éœ€è¦é‡å¤§æ”¹è¿›ã€‚\n\n")
            
            f.write("### å»ºè®®åç»­å·¥ä½œ\n\n")
            f.write("1. æ‰©å¤§éªŒè¯è§„æ¨¡åˆ°æ›´å¤šAmazonç±»åˆ«\n")
            f.write("2. å®ç°å®Œæ•´çš„çŸ¥è¯†è’¸é¦å’ŒQLoRAé›†æˆ\n")
            f.write("3. ä¼˜åŒ–è¶…å‚æ•°ä»¥æå‡æ¨¡å‹æ€§èƒ½\n")
            f.write("4. å¯¹æ¯”æ›´å¤šåŸºçº¿æ–¹æ³•\n")

def run_amazon_hypothesis_validation():
    """è¿è¡ŒAmazonå‡è®¾éªŒè¯"""
    print("ğŸ”¬ å¼€å§‹Amazonæ•°æ®H1-H4å‡è®¾ç»¼åˆéªŒè¯...")
    
    # é…ç½®å®éªŒ
    config = AmazonHypothesisConfig(
        categories=['All_Beauty', 'Books'],  # é€‰æ‹©ä¸¤ä¸ªè¾ƒå°çš„ç±»åˆ«
        max_users_per_category=1500,
        max_items_per_category=1000,
        embedding_dim=32,
        hidden_dims=[64, 32, 16],
        max_epochs=25,
        batch_size=256
    )
    
    # è¿è¡ŒéªŒè¯
    validator = AmazonHypothesisValidator(config)
    results = validator.run_comprehensive_validation()
    
    # æ‰“å°ç»“æœæ‘˜è¦
    print("\nğŸ“Š éªŒè¯ç»“æœæ‘˜è¦:")
    print("-" * 50)
    
    summary = results['summary']
    print(f"æ”¯æŒçš„å‡è®¾: {summary['supported_hypotheses']}/{summary['total_hypotheses']} ({summary['support_ratio']*100:.1f}%)")
    print(f"æµ‹è¯•ç±»åˆ«æ•°: {summary['categories_tested']}")
    
    if summary['avg_performance']:
        print(f"å¹³å‡æ€§èƒ½: RMSE={summary['avg_performance']['avg_rmse']:.4f}, MAE={summary['avg_performance']['avg_mae']:.4f}")
    
    print("\nğŸ” å„å‡è®¾éªŒè¯ç»“æœ:")
    for h_id, h_data in results['hypothesis_results'].items():
        status = "âœ… æ”¯æŒ" if h_data['supported'] else "âŒ ä¸æ”¯æŒ"
        strength = h_data.get('support_strength', 'unknown')
        print(f"{h_id}: {status} (å¼ºåº¦: {strength})")
    
    print("\nâœ… Amazonå‡è®¾éªŒè¯å®Œæˆ!")
    return results

if __name__ == "__main__":
    results = run_amazon_hypothesis_validation()
