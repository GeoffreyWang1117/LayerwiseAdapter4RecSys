"""
Amazonæ¨èç³»ç»ŸåŸºçº¿å®ç°

åŸºäºçœŸå®Amazonæ•°æ®çš„ååŒè¿‡æ»¤æ¨èç³»ç»Ÿ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from scipy.sparse import csr_matrix, coo_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import time
import warnings

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class AmazonRecommenderBaseline:
    """Amazonæ¨èç³»ç»ŸåŸºçº¿"""
    
    def __init__(self, category: str = 'All_Beauty', max_users: int = 5000, max_items: int = 2000):
        """
        åˆå§‹åŒ–æ¨èç³»ç»Ÿ
        
        Args:
            category: Amazonäº§å“ç±»åˆ«
            max_users: æœ€å¤§ç”¨æˆ·æ•°
            max_items: æœ€å¤§ç‰©å“æ•°
        """
        self.category = category
        self.max_users = max_users
        self.max_items = max_items
        self.logger = logging.getLogger(self.__class__.__name__)
        
        self.user_encoder = LabelEncoder()
        self.item_encoder = LabelEncoder()
        
        # æ¨èæ¨¡å‹
        self.user_item_matrix = None
        self.item_similarity_matrix = None
        self.user_similarity_matrix = None
        
        # æ•°æ®
        self.train_data = None
        self.test_data = None
        self.user_means = None
        
    def load_and_preprocess_data(self, data_path: str = "dataset/amazon") -> pd.DataFrame:
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        self.logger.info(f"åŠ è½½ç±»åˆ« {self.category} çš„æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        reviews_file = Path(data_path) / f"{self.category}_reviews.parquet"
        if not reviews_file.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {reviews_file}")
        
        df = pd.read_parquet(reviews_file)
        self.logger.info(f"åŸå§‹æ•°æ®: {len(df):,} æ¡è®°å½•")
        
        # ç»Ÿä¸€åˆ—å
        df = df.rename(columns={'parent_asin': 'item_id'})
        
        # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
        required_cols = ['user_id', 'item_id', 'rating']
        if not all(col in df.columns for col in required_cols):
            raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {required_cols}")
        
        # æ•°æ®æ¸…ç†
        df = df.dropna(subset=required_cols)
        df = df[(df['rating'] >= 1) & (df['rating'] <= 5)]
        
        # è¿‡æ»¤ä½é¢‘ç”¨æˆ·å’Œç‰©å“
        min_interactions = 5
        
        user_counts = df['user_id'].value_counts()
        valid_users = user_counts[user_counts >= min_interactions].index
        df = df[df['user_id'].isin(valid_users)]
        
        item_counts = df['item_id'].value_counts()
        valid_items = item_counts[item_counts >= min_interactions].index
        df = df[df['item_id'].isin(valid_items)]
        
        # é™åˆ¶æ•°æ®è§„æ¨¡
        if self.max_users and len(df['user_id'].unique()) > self.max_users:
            top_users = df['user_id'].value_counts().head(self.max_users).index
            df = df[df['user_id'].isin(top_users)]
        
        if self.max_items and len(df['item_id'].unique()) > self.max_items:
            top_items = df['item_id'].value_counts().head(self.max_items).index
            df = df[df['item_id'].isin(top_items)]
        
        # ç¼–ç ç”¨æˆ·å’Œç‰©å“ID
        df['user_idx'] = self.user_encoder.fit_transform(df['user_id'])
        df['item_idx'] = self.item_encoder.fit_transform(df['item_id'])
        
        self.logger.info(
            f"é¢„å¤„ç†å: {len(df):,} æ¡è®°å½•, "
            f"{df['user_idx'].nunique():,} ç”¨æˆ·, "
            f"{df['item_idx'].nunique():,} ç‰©å“"
        )
        
        return df
    
    def split_data(self, df: pd.DataFrame, test_size: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """åˆ’åˆ†è®­ç»ƒå’Œæµ‹è¯•é›†"""
        self.logger.info("åˆ’åˆ†è®­ç»ƒå’Œæµ‹è¯•é›†...")
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·éšæœºåˆ’åˆ†æ•°æ®
        train_list, test_list = [], []
        
        for user_idx in df['user_idx'].unique():
            user_data = df[df['user_idx'] == user_idx]
            
            if len(user_data) < 2:
                train_list.append(user_data)
                continue
            
            # éšæœºåˆ’åˆ†
            user_train, user_test = train_test_split(
                user_data, test_size=test_size, random_state=42
            )
            
            train_list.append(user_train)
            test_list.append(user_test)
        
        train_df = pd.concat(train_list, ignore_index=True)
        test_df = pd.concat(test_list, ignore_index=True)
        
        self.logger.info(f"è®­ç»ƒé›†: {len(train_df):,}, æµ‹è¯•é›†: {len(test_df):,}")
        
        return train_df, test_df
    
    def build_user_item_matrix(self, df: pd.DataFrame) -> csr_matrix:
        """æ„å»ºç”¨æˆ·-ç‰©å“äº¤äº’çŸ©é˜µ"""
        self.logger.info("æ„å»ºç”¨æˆ·-ç‰©å“äº¤äº’çŸ©é˜µ...")
        
        n_users = df['user_idx'].max() + 1
        n_items = df['item_idx'].max() + 1
        
        # åˆ›å»ºç¨€ç–çŸ©é˜µ
        matrix = csr_matrix(
            (df['rating'].values, (df['user_idx'].values, df['item_idx'].values)),
            shape=(n_users, n_items)
        )
        
        density = matrix.nnz / (n_users * n_items) * 100
        self.logger.info(f"äº¤äº’çŸ©é˜µ: {n_users} x {n_items}, å¯†åº¦: {density:.4f}%")
        
        return matrix
    
    def compute_item_similarity(self, matrix: csr_matrix) -> np.ndarray:
        """è®¡ç®—ç‰©å“ç›¸ä¼¼åº¦çŸ©é˜µ"""
        self.logger.info("è®¡ç®—ç‰©å“ç›¸ä¼¼åº¦çŸ©é˜µ...")
        
        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        item_similarity = cosine_similarity(matrix.T)
        
        # å°†å¯¹è§’çº¿è®¾ä¸º0ï¼ˆç‰©å“ä¸è‡ªèº«çš„ç›¸ä¼¼åº¦ï¼‰
        np.fill_diagonal(item_similarity, 0)
        
        return item_similarity
    
    def compute_user_similarity(self, matrix: csr_matrix) -> np.ndarray:
        """è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µ"""
        self.logger.info("è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µ...")
        
        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        user_similarity = cosine_similarity(matrix)
        
        # å°†å¯¹è§’çº¿è®¾ä¸º0
        np.fill_diagonal(user_similarity, 0)
        
        return user_similarity
    
    def predict_item_based(self, user_idx: int, item_idx: int, k: int = 50) -> float:
        """åŸºäºç‰©å“çš„ååŒè¿‡æ»¤é¢„æµ‹"""
        if self.item_similarity_matrix is None:
            return self.user_means[user_idx] if user_idx < len(self.user_means) else 3.0
        
        # æ‰¾åˆ°ç”¨æˆ·è¯„ä»·è¿‡çš„ç‰©å“
        user_ratings = self.user_item_matrix[user_idx].toarray().flatten()
        rated_items = np.where(user_ratings > 0)[0]
        
        if len(rated_items) == 0:
            return 3.0  # é»˜è®¤è¯„åˆ†
        
        # è®¡ç®—ç›®æ ‡ç‰©å“ä¸å·²è¯„ä»·ç‰©å“çš„ç›¸ä¼¼åº¦
        if item_idx >= self.item_similarity_matrix.shape[0]:
            return self.user_means[user_idx] if user_idx < len(self.user_means) else 3.0
        
        similarities = self.item_similarity_matrix[item_idx][rated_items]
        
        # é€‰æ‹©top-kç›¸ä¼¼ç‰©å“
        top_k_indices = np.argsort(similarities)[-k:]
        top_k_items = rated_items[top_k_indices]
        top_k_similarities = similarities[top_k_indices]
        
        # è®¡ç®—åŠ æƒå¹³å‡è¯„åˆ†
        if np.sum(np.abs(top_k_similarities)) == 0:
            return self.user_means[user_idx] if user_idx < len(self.user_means) else 3.0
        
        weighted_ratings = user_ratings[top_k_items] * top_k_similarities
        prediction = np.sum(weighted_ratings) / np.sum(np.abs(top_k_similarities))
        
        return max(1.0, min(5.0, prediction))
    
    def predict_user_based(self, user_idx: int, item_idx: int, k: int = 50) -> float:
        """åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤é¢„æµ‹"""
        if self.user_similarity_matrix is None:
            return 3.0
        
        # æ‰¾åˆ°è¯„ä»·è¿‡è¯¥ç‰©å“çš„ç”¨æˆ·
        item_ratings = self.user_item_matrix[:, item_idx].toarray().flatten()
        rated_users = np.where(item_ratings > 0)[0]
        
        if len(rated_users) == 0:
            return 3.0
        
        # è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦
        if user_idx >= self.user_similarity_matrix.shape[0]:
            return 3.0
        
        similarities = self.user_similarity_matrix[user_idx][rated_users]
        
        # é€‰æ‹©top-kç›¸ä¼¼ç”¨æˆ·
        top_k_indices = np.argsort(similarities)[-k:]
        top_k_users = rated_users[top_k_indices]
        top_k_similarities = similarities[top_k_indices]
        
        # è®¡ç®—åŠ æƒå¹³å‡è¯„åˆ†
        if np.sum(np.abs(top_k_similarities)) == 0:
            return 3.0
        
        weighted_ratings = item_ratings[top_k_users] * top_k_similarities
        prediction = np.sum(weighted_ratings) / np.sum(np.abs(top_k_similarities))
        
        return max(1.0, min(5.0, prediction))
    
    def fit(self, df: pd.DataFrame):
        """è®­ç»ƒæ¨èæ¨¡å‹"""
        self.logger.info("å¼€å§‹è®­ç»ƒæ¨èæ¨¡å‹...")
        
        # åˆ’åˆ†æ•°æ®
        self.train_data, self.test_data = self.split_data(df)
        
        # æ„å»ºäº¤äº’çŸ©é˜µ
        self.user_item_matrix = self.build_user_item_matrix(self.train_data)
        
        # è®¡ç®—ç”¨æˆ·å¹³å‡è¯„åˆ†
        self.user_means = np.array([
            self.user_item_matrix[i].mean() if self.user_item_matrix[i].nnz > 0 else 3.0
            for i in range(self.user_item_matrix.shape[0])
        ])
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        self.item_similarity_matrix = self.compute_item_similarity(self.user_item_matrix)
        self.user_similarity_matrix = self.compute_user_similarity(self.user_item_matrix)
        
        self.logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def evaluate(self, method: str = 'item_based') -> Dict[str, float]:
        """è¯„ä¼°æ¨èæ¨¡å‹"""
        self.logger.info(f"è¯„ä¼° {method} æ¨èæ¨¡å‹...")
        
        predictions = []
        ground_truth = []
        
        for _, row in self.test_data.iterrows():
            user_idx = row['user_idx']
            item_idx = row['item_idx']
            true_rating = row['rating']
            
            if method == 'item_based':
                pred_rating = self.predict_item_based(user_idx, item_idx)
            else:
                pred_rating = self.predict_user_based(user_idx, item_idx)
            
            predictions.append(pred_rating)
            ground_truth.append(true_rating)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        mse = mean_squared_error(ground_truth, predictions)
        mae = mean_absolute_error(ground_truth, predictions)
        rmse = np.sqrt(mse)
        
        results = {
            'method': method,
            'mse': mse,
            'mae': mae,
            'rmse': rmse,
            'test_size': len(ground_truth)
        }
        
        self.logger.info(f"{method} - RMSE: {rmse:.4f}, MAE: {mae:.4f}")
        
        return results
    
    def get_recommendations(self, user_idx: int, n_recommendations: int = 10, method: str = 'item_based') -> List[Tuple[int, float]]:
        """ä¸ºç”¨æˆ·ç”Ÿæˆæ¨è"""
        if user_idx >= self.user_item_matrix.shape[0]:
            return []
        
        # è·å–ç”¨æˆ·å·²è¯„ä»·çš„ç‰©å“
        user_ratings = self.user_item_matrix[user_idx].toarray().flatten()
        rated_items = set(np.where(user_ratings > 0)[0])
        
        # é¢„æµ‹æœªè¯„ä»·ç‰©å“çš„è¯„åˆ†
        item_scores = []
        for item_idx in range(self.user_item_matrix.shape[1]):
            if item_idx not in rated_items:
                if method == 'item_based':
                    score = self.predict_item_based(user_idx, item_idx)
                else:
                    score = self.predict_user_based(user_idx, item_idx)
                item_scores.append((item_idx, score))
        
        # æŒ‰è¯„åˆ†æ’åºå¹¶è¿”å›top-N
        item_scores.sort(key=lambda x: x[1], reverse=True)
        return item_scores[:n_recommendations]

def run_amazon_baseline_experiment():
    """è¿è¡ŒAmazonåŸºçº¿å®éªŒ"""
    print("ğŸš€ å¼€å§‹Amazonæ¨èç³»ç»ŸåŸºçº¿å®éªŒ...")
    
    # åˆå§‹åŒ–æ¨èç³»ç»Ÿ
    recommender = AmazonRecommenderBaseline(
        category='All_Beauty',
        max_users=3000,
        max_items=1500
    )
    
    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    df = recommender.load_and_preprocess_data()
    
    # è®­ç»ƒæ¨¡å‹
    start_time = time.time()
    recommender.fit(df)
    training_time = time.time() - start_time
    
    print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    
    # è¯„ä¼°æ¨¡å‹
    results = {}
    
    # åŸºäºç‰©å“çš„ååŒè¿‡æ»¤
    results['item_based'] = recommender.evaluate('item_based')
    
    # åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤
    results['user_based'] = recommender.evaluate('user_based')
    
    # æ‰“å°ç»“æœ
    print("\nğŸ“Š åŸºçº¿æ¨¡å‹è¯„ä¼°ç»“æœ:")
    print("-" * 50)
    for method, metrics in results.items():
        print(f"{method.upper()}:")
        print(f"  RMSE: {metrics['rmse']:.4f}")
        print(f"  MAE: {metrics['mae']:.4f}")
        print(f"  æµ‹è¯•æ ·æœ¬æ•°: {metrics['test_size']:,}")
        print()
    
    # ç”Ÿæˆæ¨èç¤ºä¾‹
    print("ğŸ“‹ æ¨èç¤ºä¾‹:")
    user_idx = 0  # ç¬¬ä¸€ä¸ªç”¨æˆ·
    recommendations_item = recommender.get_recommendations(user_idx, n_recommendations=5, method='item_based')
    recommendations_user = recommender.get_recommendations(user_idx, n_recommendations=5, method='user_based')
    
    print(f"ç”¨æˆ· {user_idx} çš„æ¨è (åŸºäºç‰©å“):")
    for item_idx, score in recommendations_item:
        print(f"  ç‰©å“ {item_idx}: é¢„æµ‹è¯„åˆ† {score:.2f}")
    
    print(f"ç”¨æˆ· {user_idx} çš„æ¨è (åŸºäºç”¨æˆ·):")
    for item_idx, score in recommendations_user:
        print(f"  ç‰©å“ {item_idx}: é¢„æµ‹è¯„åˆ† {score:.2f}")
    
    return results, recommender

if __name__ == "__main__":
    results, model = run_amazon_baseline_experiment()
    print("\nâœ… AmazonåŸºçº¿å®éªŒå®Œæˆ!")
