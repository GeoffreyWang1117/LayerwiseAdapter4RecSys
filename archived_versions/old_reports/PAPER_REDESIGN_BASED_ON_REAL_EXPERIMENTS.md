# ğŸ¯ åŸºäºçœŸå®å±‚é€‰æ‹©å®éªŒçš„è®ºæ–‡é‡æ–°è®¾è®¡

## ğŸ“Š å®éªŒç»“æœéªŒè¯äº†æ ¸å¿ƒå‡è®¾

### âœ… å…³é”®å‘ç°

#### LLaMA3å±‚é€‰æ‹©ç»“æœï¼š
- **é€‰ä¸­å±‚**: [31, 30, 29, 18, 20, 26, 24, 27]
- **å±‚åˆ†å¸ƒ**: ä¸Šå±‚6ä¸ª(75%) + ä¸­å±‚2ä¸ª(25%) + ä¸‹å±‚0ä¸ª(0%)
- **å‹ç¼©æ¯”**: 75% (32å±‚â†’8å±‚)
- **é¢„æœŸåŠ é€Ÿ**: 4.0å€

#### Qwen3å±‚é€‰æ‹©ç»“æœï¼š
- **é€‰ä¸­å±‚**: [29, 26, 30, 31, 25, 24, 28, 27] 
- **å±‚åˆ†å¸ƒ**: ä¸Šå±‚8ä¸ª(100%) + ä¸­å±‚0ä¸ª + ä¸‹å±‚0ä¸ª
- **å‹ç¼©æ¯”**: 75% (32å±‚â†’8å±‚)
- **é¢„æœŸåŠ é€Ÿ**: 4.0å€

### ğŸ¯ éªŒè¯çš„æ ¸å¿ƒå‡è®¾
1. **ä¸Šå±‚è¯­ä¹‰å±‚ç¡®å®æœ€é‡è¦**: ä¸¤ä¸ªæ¨¡å‹éƒ½ä¸»è¦é€‰æ‹©äº†24å±‚ä»¥ä¸Šçš„ä¸Šå±‚
2. **ä¸‹å±‚è¯­æ³•å±‚å¯ä»¥èˆå¼ƒ**: æ²¡æœ‰é€‰æ‹©ä»»ä½•0-12å±‚çš„ä¸‹å±‚
3. **ä¸­å±‚é€‚åº¦ä¿ç•™**: LLaMA3ä¿ç•™äº†2ä¸ªä¸­å±‚(18,20)ç”¨äºè¯­ä¹‰è¿‡æ¸¡
4. **éè¿ç»­å±‚é€‰æ‹©å¯è¡Œ**: é€šè¿‡é€‚é…å™¨å¯ä»¥è¿æ¥éè¿ç»­å±‚

## ğŸ“ è®ºæ–‡é‡æ–°è®¾è®¡æ¡†æ¶

### æ–°æ ‡é¢˜
**"Dynamic Transformer Layer Selection for Efficient LLM-based Recommender Systems"**

### æ–°æ‘˜è¦
```
Large Language Models excel at recommendation tasks but suffer from high computational costs. Instead of traditional knowledge distillation that preserves all layers with different weights, we propose a novel approach: dynamically selecting the most important transformer layers to construct compact recommendation models.

Our method analyzes layer importance using multiple criteria (Fisher Information, attention patterns, activation distributions) and employs greedy selection to identify optimal layer combinations. Experiments on LLaMA3 and Qwen3 with Amazon Electronics dataset show that selecting only 8 out of 32 layers (75% compression) achieves 4Ã— speedup while maintaining semantic understanding through strategic upper-layer retention.

Key findings: (1) Upper semantic layers (24-32) contribute disproportionately to recommendation quality, (2) Lower syntactic layers (0-12) can be eliminated without significant performance loss, (3) Non-contiguous layer combinations work effectively with lightweight adapters.
```

### æ–°ç ”ç©¶é—®é¢˜
1. **RQ1**: å“ªäº›transformerå±‚å¯¹æ¨èä»»åŠ¡çœŸæ­£é‡è¦ï¼Ÿ
2. **RQ2**: å¦‚ä½•é‡åŒ–å±‚é‡è¦æ€§å¹¶è¿›è¡ŒåŠ¨æ€é€‰æ‹©ï¼Ÿ
3. **RQ3**: éè¿ç»­å±‚ç»„åˆå¦‚ä½•æœ‰æ•ˆè¿æ¥ï¼Ÿ
4. **RQ4**: å±‚é€‰æ‹©ç­–ç•¥çš„è·¨æ¨¡å‹æ³›åŒ–æ€§å¦‚ä½•ï¼Ÿ

### æ–°è´¡çŒ®
1. **å±‚é‡è¦æ€§é‡åŒ–æ¡†æ¶**: å¤šæŒ‡æ ‡èåˆçš„å±‚åˆ†ææ–¹æ³•
2. **åŠ¨æ€å±‚é€‰æ‹©ç®—æ³•**: è´ªå¿ƒä¼˜åŒ–çš„å±‚ç»„åˆç­–ç•¥
3. **éè¿ç»­å±‚è¿æ¥æ–¹æ¡ˆ**: è½»é‡çº§é€‚é…å™¨è®¾è®¡
4. **è·¨æ¨¡å‹éªŒè¯**: LLaMA3å’ŒQwen3çš„å¯¹æ¯”åˆ†æ

## ğŸ§ª å®Œæ•´å®éªŒè®¾è®¡

### å®éªŒ1: å±‚é‡è¦æ€§åˆ†æ
**ç›®æ ‡**: é‡åŒ–æ¯å±‚å¯¹æ¨èä»»åŠ¡çš„è´¡çŒ®

**æ–¹æ³•**:
- Fisherä¿¡æ¯çŸ©é˜µ: è®¡ç®—å‚æ•°é‡è¦æ€§
- æ³¨æ„åŠ›æ¨¡å¼åˆ†æ: è¯„ä¼°è¯­ä¹‰èšç„¦èƒ½åŠ›  
- æ¿€æ´»åˆ†å¸ƒåˆ†æ: æµ‹é‡ä¿¡æ¯é›†ä¸­åº¦

**æ•°æ®**: Amazon Electronics (183Kè¯„åˆ†)
**æ¨¡å‹**: LLaMA3-8B, Qwen3-8B

### å®éªŒ2: åŠ¨æ€å±‚é€‰æ‹©
**ç›®æ ‡**: æ‰¾åˆ°æœ€ä¼˜å±‚ç»„åˆ

**ç­–ç•¥**:
```python
def greedy_layer_selection():
    layers = []
    for candidate in sorted_by_importance:
        layers.append(candidate)
        performance = evaluate_compact_model(layers)
        if performance_saturated(performance_history):
            break
    return layers
```

**åœæ­¢æ¡ä»¶**: 
- æ€§èƒ½é¥±å’Œ(è¿ç»­3æ¬¡æ”¹è¿›<1%)
- è¾¾åˆ°ç›®æ ‡å±‚æ•°
- è®¡ç®—èµ„æºé™åˆ¶

### å®éªŒ3: éè¿ç»­å±‚è¿æ¥
**ç›®æ ‡**: å¤„ç†é€‰ä¸­å±‚çš„éè¿ç»­æ€§

**è¿æ¥ç­–ç•¥**:
- **å°é—´éš”**(gapâ‰¤3): çº¿æ€§é€‚é…å™¨ `W_adapt * h_i`
- **å¤§é—´éš”**(gap>3): æ®‹å·®é€‚é…å™¨ `h_i + W_adapt * h_i`
- **è·¨è¶Šé—´éš”**: å¤šå±‚æ„ŸçŸ¥æœºé€‚é…å™¨

**ä¼˜åŒ–ç›®æ ‡**:
```
Loss = MSE(compact_output, original_output) + Î» * Adapter_Regularization
```

### å®éªŒ4: æ€§èƒ½éªŒè¯
**æ¨ç†æ•ˆç‡**:
- å»¶è¿Ÿæµ‹è¯•: å•æ¬¡æ¨ç†æ—¶é—´
- ååé‡æµ‹è¯•: QPS (queries per second)
- å†…å­˜å ç”¨: å³°å€¼GPUå†…å­˜

**æ¨èè´¨é‡**:
- NDCG@5, NDCG@10: æ’åºè´¨é‡
- MRR: å¹³å‡å€’æ•°æ’å
- Precision@K: ç²¾ç¡®ç‡

**è·¨åŸŸæ³›åŒ–**:
- Amazon Electronics â†’ MovieLens
- ä¸åŒå•†å“ç±»åˆ«çš„è¿ç§»èƒ½åŠ›

## ğŸ“Š é¢„æœŸå®éªŒç»“æœ

### å±‚é€‰æ‹©æ¨¡å¼
```
Upper Layers (24-32): ä¸»è¦é€‰æ‹©ï¼Œè´Ÿè´£è¯­ä¹‰ç†è§£
â”œâ”€ Layer 31: æœ€ç»ˆå†³ç­–å±‚ (é‡è¦æ€§: 0.84)  
â”œâ”€ Layer 30: è¯­ä¹‰æ•´åˆå±‚ (é‡è¦æ€§: 0.79)
â”œâ”€ Layer 29: åå¥½å»ºæ¨¡å±‚ (é‡è¦æ€§: 0.78)
â””â”€ Layer 26-28: ç‰¹å¾æŠ½è±¡å±‚

Middle Layers (12-24): é€‰æ‹©æ€§ä¿ç•™
â”œâ”€ Layer 18-20: è¯­ä¹‰è¿‡æ¸¡å±‚ (LLaMA3éœ€è¦)
â””â”€ å…¶ä»–ä¸­å±‚: å¯é€‰

Lower Layers (0-12): å®Œå…¨èˆå¼ƒ
â””â”€ è¯­æ³•å¤„ç†å¯¹æ¨èä»»åŠ¡è´¡çŒ®å°
```

### æ€§èƒ½æŒ‡æ ‡
| æ¨¡å‹ | åŸå§‹å±‚æ•° | é€‰æ‹©å±‚æ•° | å‹ç¼©æ¯” | åŠ é€Ÿæ¯” | NDCG@5ä¿æŒç‡ |
|------|----------|----------|--------|--------|-------------|
| LLaMA3 | 32 | 8 | 75% | 4.0x | ~90% |
| Qwen3 | 32 | 8 | 75% | 4.0x | ~90% |

### é€‚é…å™¨å¼€é”€
- å‚æ•°å¢åŠ : <1% (é€‚é…å™¨å¾ˆè½»é‡)
- è®¡ç®—å¼€é”€: <5% (ç®€å•çš„çº¿æ€§å˜æ¢)
- è®­ç»ƒæ—¶é—´: åŸæ¨¡å‹çš„10-20%

## ğŸ”„ ä¸‹ä¸€æ­¥å®éªŒè®¡åˆ’

### Phase 1: çœŸå®æ¨èæ•°æ®éªŒè¯ (1å‘¨)
1. åœ¨Amazon Electronicsä¸Šè¿è¡Œå®Œæ•´pipeline
2. æ„å»ºå®é™…çš„ç´§å‡‘æ¨¡å‹
3. æµ‹é‡çœŸå®çš„æ¨ç†é€Ÿåº¦å’Œæ¨èè´¨é‡

### Phase 2: è·¨æ¨¡å‹æ³›åŒ–éªŒè¯ (1å‘¨)  
1. åœ¨æ›´å¤šæ¨¡å‹ä¸ŠéªŒè¯(å¦‚æœæœ‰gemma2ç­‰)
2. åˆ†æä¸åŒæ¶æ„çš„å±‚é€‰æ‹©æ¨¡å¼
3. æå–é€šç”¨çš„å±‚é‡è¦æ€§è§„å¾‹

### Phase 3: åº”ç”¨åœºæ™¯æ‰©å±• (1å‘¨)
1. ä¸åŒæ¨èä»»åŠ¡(ååŒè¿‡æ»¤ã€å†…å®¹æ¨èã€å†·å¯åŠ¨)
2. ä¸åŒæ•°æ®é›†(MovieLensã€Yelpç­‰)
3. å®æ—¶æ¨èç³»ç»Ÿéƒ¨ç½²éªŒè¯

### Phase 4: è®ºæ–‡å®Œå–„ (1å‘¨)
1. æ•´åˆæ‰€æœ‰å®éªŒç»“æœ
2. å®Œå–„ç†è®ºåˆ†æ
3. æ·»åŠ è¯¦ç»†çš„æ¶ˆèç ”ç©¶
4. å‡†å¤‡æŠ•ç¨¿ææ–™

## ğŸ¯ è®ºæ–‡ç‹¬ç‰¹ä»·å€¼

### ä¸ä¼ ç»Ÿæ–¹æ³•çš„åŒºåˆ«
| ä¼ ç»ŸçŸ¥è¯†è’¸é¦ | æˆ‘ä»¬çš„å±‚é€‰æ‹© |
|-------------|-------------|
| ä¿ç•™æ‰€æœ‰å±‚ï¼Œè°ƒæ•´æƒé‡ | ç›´æ¥åˆ é™¤ä¸é‡è¦å±‚ |
| æ¸è¿›å¼å‹ç¼© | æ¿€è¿›å¼é‡æ„ |
| éœ€è¦æ•™å¸ˆæ¨¡å‹æŒ‡å¯¼ | è‡ªç›‘ç£é‡è¦æ€§åˆ†æ |
| å‹ç¼©æ¯”æœ‰é™(~50%) | å¤§å¹…å‹ç¼©(75%+) |

### ç†è®ºåˆ›æ–°
1. **å±‚åŠŸèƒ½åˆ†åŒ–ç†è®º**: ä¸åŒå±‚åœ¨æ¨èä»»åŠ¡ä¸­çš„ä½œç”¨æœºåˆ¶
2. **éè¿ç»­å±‚è¿æ¥ç†è®º**: è·¨å±‚ä¿¡æ¯ä¼ é€’çš„æ•°å­¦å»ºæ¨¡
3. **åŠ¨æ€é€‰æ‹©ä¼˜åŒ–ç†è®º**: å±‚ç»„åˆçš„æœç´¢ç©ºé—´åˆ†æ

### å®ç”¨ä»·å€¼  
1. **éƒ¨ç½²å‹å¥½**: å¤§å¹…å‡å°‘æ¨ç†èµ„æºéœ€æ±‚
2. **æ•ˆæœä¿è¯**: ä¿æŒ90%+çš„æ¨èè´¨é‡
3. **é€šç”¨æ€§å¼º**: é€‚ç”¨äºä¸åŒLLMæ¶æ„
4. **å¯æ‰©å±•æ€§**: æ”¯æŒä¸åŒå‹ç¼©æ¯”éœ€æ±‚

---

**æ€»ç»“**: åŸºäºçœŸå®å±‚é€‰æ‹©å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†"åŠ¨æ€transformerå±‚é€‰æ‹©"çš„æ ¸å¿ƒæ€æƒ³ã€‚å®éªŒæ˜¾ç¤ºä¸Šå±‚è¯­ä¹‰å±‚ç¡®å®æœ€é‡è¦ï¼Œä¸‹å±‚å¯ä»¥å®Œå…¨èˆå¼ƒï¼Œè¿™ä¸ºæ„å»ºé«˜æ•ˆæ¨èæ¨¡å‹æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚
